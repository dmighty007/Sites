<!DOCTYPE html>
<html lang="en">

<!-- Basic -->
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<!-- Mobile Metas -->
<meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">

<!-- Site Metas -->
<title>OPS | Blog1 | DMCODERS'</title>
<meta name="keywords" content="">
<meta name="description" content="">
<meta name="author" content="">

<!-- Site Icons -->
<link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon" />
<link rel="apple-touch-icon" href="images/apple-touch-icon.png">

<!-- Bootstrap CSS -->
<link rel="stylesheet" href="css/bootstrap.min.css">
<!-- Site CSS -->
<link rel="stylesheet" href="css/animate.css">
<link rel="stylesheet" href="css/camera.css">
<link rel="stylesheet" href="css/flaticon.css">
<link rel="stylesheet" href="css/prettyPhoto.css">
<link rel="stylesheet" href="css/owl.carousel.css">
<link rel="stylesheet" href="css/font-awesome.min.css">
<link rel="stylesheet" href="style_blog.css">
<link rel="stylesheet" href="style.css">

<!-- Responsive CSS -->
<link rel="stylesheet" href="css/responsive.css">
<!-- Custom CSS -->
<link rel="stylesheet" href="css/custom.css">
<script src="js/modernizr.js"></script> <!-- Modernizr -->
<script src="https://3Dmol.csb.pitt.edu/build/3Dmol-min.js"></script>

<!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->



    <style>
      a {
      font-family: sohne, "Helvetica Neue", Helvetica, Arial, sans-serif;
      font-style: normal;
      color: cornflowerblue;
      text-decoration: underline;
      }
      ul li {
        margin: 5px 10px;
        float: left;
        clear: both;
        width: 100%;
      }
    
      .wp-icon {
        width: 30px;
        height: 30px;
        border-radius: 30%;
        text-align: center;
        line-height: 35px;
        vertical-align: middle;
        color: #fff;
        margin-left: 10px;
        margin-bottom: 0px;
      }
    
      .fa-facebook-f {
        background: #3B5998;
      }
    
      .fa-linkedin {
        background: #0077B5;
      }
    
      .fa-twitter {
        background: #1DA1F2;
      }
    
      .fa-instagram {
        background: #d6249f;
        background: radial-gradient(circle at 30% 107%, #fdf497 0%, #fdf497 5%, #fd5949 45%, #d6249f 60%, #285AEB 90%);
        box-shadow: 0px 3px 10px rgba(0, 0, 0, .25);
      }
    
      .fa-google-plus {
        background: #D04338;
      }
    
      .fa-youtube {
        background: #FF0000;
      }
    
      .fa-pinterest {
        background: #BD081C;
      }
    
      .image {
        text-align: center;
      }
    
      .figure_caption {
        text-align: center;
        font-size: 15px;
        font-weight: 300;
      }
    
      .block {
        text-align: center;
        font-size: 25px;
        font-weight: 300;
        text-shadow: #1DA1F2;
        color: #333333;
      }
    
      .section-title, h3 {
        font-family: Georgia, 'Times New Roman', Times, serif;
        color: darkblue;
        text-decoration: none;
      }
      h1 {
        font-family: Georgia, 'Times New Roman', Times, serif;
        color: rgb(0, 0, 0);
        font-size: 30px;
        font-weight: 700;
      }
      h2 {
        font-family: Georgia, 'Times New Roman', Times, serif;
        color: rgb(0, 0, 0);
        font-size: 25px;
        font-weight: 700;
      }
      .iframe {
        width: 90%
      }
    
      pre {
        margin: 20px;
        padding: 20px;
        color: rgb(24, 21, 21);
        background-color: rgb(235, 232, 232);
        white-space: pre;
        text-shadow: 0 1px 0 #000;
        border-radius: 15px;
        border-bottom: 1px solid #555;
        box-shadow: 0 1px 5px rgba(0, 0, 0, 0.4) inset, 0 0 20px rgba(0, 0, 0, 0.2) inset;
        font: 16px/24px 'Courier New', Courier, 'Lucida Sans Typewriter', 'Lucida Typewriter', monospace;
      }
    
    
      * {
        box-sizing: border-box;
      }
    
      /* lists reset */
      ul,ol {
        clear: both;
        padding-left: 75px;
        /*display: grid;*/
        /*grid-gap: 5px;*/
      }
    
      li {
        /*display: grid;*/
        color:#5c5e63;
        list-style:circle;
        align-items: start;
        font-size: 20px;
        /*line-height: 1.25;*/
      }
    
      ul ol li::before {
        content: attr(data-icon);
        font-size: 1.25em;
      }
    
    body {
    background-color: mintcream;
    font-weight: normal;
  }
  p{
    font-weight: normal;
    font-style: normal;
  }
  .article{
    font-weight: normal;
    font-style: normal;
  }
    </style>

</head>

<body id="page-top" class="politics_version">

  <!-- LOADER -->
  <div id="preloader">
    <div id="main-ld">
      <div id="loader"></div>
    </div>
  </div><!-- end loader -->
  <!-- END LOADER -->

  <!-- Navigation -->
  <nav class="navbar navbar-dark navbar-expand-lg relative" id="mainNav" style="background-color:#333333">
    <div class="container">
      <!--
      <a class="navbar-brand js-scroll-trigger" href="#page-top">
        <img class="img-fluid" src="images/dmlogo1.png" alt="" />
      </a>
      <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse"
        data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false"
        aria-label="Toggle navigation">
        Menu
        <i class="fa fa-bars"></i>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav text-uppercase ml-auto">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger active" href="index.html">Home</a>
          </li>
        </ul>
      </div>-->
    </div>
  </nav>

  <div class="section wb">
    <div class="container"><h4> This article is reformatted from originally published at TDS(Towards Data Science)</h4></br><h4> <a href="/@ladysign?source=post_page-----cd2d6ea71c6a--------------------------------">Author : Lee Boonstra</a> </h4><div class="section-title text-center"><h3><a href="https://medium.com/google-cloud/getting-audio-data-from-text-text-to-speech-and-play-it-in-your-browser-part-iv-cd2d6ea71c6a">Getting Audio Data from Text (Text to Speech) and play it in your browser. (Part IV)</a></h3></div><div class="post"><h1 >A best practice for streaming audio from a browser microphone to Dialogflow & Google Cloud Speech To Text.</h1><p class="article">This is the fourth blog in the series:</p><p class="article"><strong >A best practice for streaming audio from a browser microphone to Dialogflow & Google Cloud Speech To Text.</strong></p><p class="article">In case you haven’t read the other blogs, I recommend to browse back to these blogs:</p><ul><li><a href="https://medium.com/@ladysign/building-your-own-conversational-voice-ai-with-dialogflow-speech-to-text-in-web-apps-part-i-b92770bd8b47">Blog 1: Introduction to the GCP conversational AI components, and integrating your own voice AI in a web app</a></li><li><a href="https://medium.com/@ladysign/building-a-client-side-web-app-which-streams-audio-from-a-browser-microphone-to-a-server-part-ii-df20ddb47d4e">Blog 2: Building a client-side web application which streams audio from a browser microphone to a server.</a></li><li><a href="https://medium.com/google-cloud/building-a-web-server-which-receives-a-browser-microphone-stream-and-uses-dialogflow-or-the-speech-62b47499fc71">Blog 3: Building a web server which receives a browser microphone stream and uses Dialogflow or the Speech to Text API for retrieving text results.</a></li></ul></br><p class="article">In the next blog of this series, I will take text (or Dialogflow QueryResult text data) that’s currently available on the server-side, pass it to the Text to Speech API (to synthesize the text) and return the audio bytes back to the client app, to play it in the browser. It has to play the audio bytes automatically.</p><p class="article">These blogs <a href="https://github.com/dialogflow/selfservicekiosk-audio-streaming/tree/master/examples">contain simple code snippets</a>, and a demo application; <a href="https://github.com/dialogflow/selfservicekiosk-audio-streaming">the Airport Self Service Kiosk</a>, which will be used as a reference architecture.</p><h1 >Architecture</h1><p class="article">When you make a Text to Speech call, either with Text to Speech or by using the built-in speech return from Dialogflow, it will return audio byte data. Both TTS and Dialogflow can be called from server-side code. In order to stream and play this in a browser, you could make use of websockets. Once the AudioBuffer (ArrayBuffer in browser JavaScript code) is returned to the client, it can be played by using WebRTC methods.</p><p class="article">Here’s an example of a browser flow when using the Text to Speech API. In this example a user types text of which the synthesized speech will be played in the browser:</p></br><div class="image"> <img src ="https://miro.medium.com/max/640/0*Lx53NgcM87rjoFkp"></div><p class="article">Here’s an example of a browser flow by using Dialogflow. In this example a user speaks in the microphone (similar as the examples above), but Dialogflow returns an AudioBuffer as the result.</p></br><div class="image"> <img src ="https://miro.medium.com/max/640/0*Qe3oEu8FeFdDgjN8"></div><h1 >Client-side Code to play the audio</h1><p class="article">The JavaScript code which runs in your browser will look like this:</p><ul><li><a href="https://github.com/dialogflow/selfservicekiosk-audio-streaming/blob/master/examples/example6.html">Client-Side Code: Play TTS output in the browser </a></li></ul></br><p class="article">I’m loading Socket.io and Socket.io with stream support (for bidirectional binary data transfer), from the CDN:</p><p class="article">Socket.IO is a real time, bidirectional event-based communication library. One of the transports that it uses are websockets, but it also provides other transports (XHR/JSONP), not just as a fallback but also for situations where websockets aren’t supported/required/wanted.</p></br><pre class="ty tz ua ub fc ui uj uk ul aw qg bi"><span>&lt;script src=”https://cdnjs.cloudflare.com/ajax/libs/socket.io/2.3.0/socket.io.js"&gt;&lt;/script&gt;</span><span>&lt;script src=”https://cdnjs.cloudflare.com/ajax/libs/socket.io-stream/0.9.1/socket.io-stream.js"&gt;&lt;/script&gt;</span></pre></br><p class="article">I created the Socket.IO object, and made sure it connects.</p><p class="article">I’ve created an on<strong > ‘results’</strong> listener, which will run once the data from the server-side is retrieved in the browser. This will call my <strong >playOutput</strong> method, which I will show later:</p></br><pre class="ty tz ua ub fc ui uj uk ul aw qg bi"><span>const socketio = io();</span><span>const socket = socketio.on(‘connect’, function() {});</span><span>    socketio.on(‘results’, function (data) {</span><span>    console.log(data);</span><span>    playOutput(data);</span><span>});</span></pre></br><p class="article">In my simple demo, I’ve create a textarea field, and a JavaScript method, which will be called on a button click , that takes the value from the field, and emits this via Socket.IO to the back-end:</p></br><pre class="ty tz ua ub fc ui uj uk ul aw qg bi"><span>const inputTextEl = document.getElementById(‘inputText’);</span><span>function submitTTSCall(){</span><span>    var input = inputTextEl.value;</span><span>    if (input) ss(socket).emit(‘tts’, input, {});</span><span>}</span></pre></br><p class="article">Now this is just a simple demo. But in a real-world application, a text prompt, or an incoming chatbot answer could trigger TTS to read it out loud in the browser. I’m doing this in the <a href="https://github.com/dialogflow/selfservicekiosk-audio-streaming/blob/master/client/src/app/dialogflow/dialogflow.component.ts">Airport Self Service Kiosk application</a>, which you can try out on this URL: <a href="http://selfservicedesk.appspot.com/">http://selfservicedesk.appspot.com/</a></p><p class="article">Here’s the code for playing the output in your browser from your device speakers:</p></br><div class="iframe"><pre> // 1)
function playOutput(arrayBuffer){
 let audioContext = new AudioContext();
 let outputSource;
 try {
     if(arrayBuffer.byteLength > 0){
         // 2)
         audioContext.decodeAudioData(arrayBuffer,
         function(buffer){
             // 3)
             audioContext.resume();
             outputSource = audioContext.createBufferSource();
             outputSource.connect(audioContext.destination);
             outputSource.buffer = buffer;
             outputSource.start(0);
         },
         function(){
             console.log(arguments);
         });
     }
 } catch(e) {
     console.log(e);
 }
}
</pre></div></br><ol><li>Here’s the <strong>playOutput</strong> function, which takes the <strong>arrayBuffer</strong> that I retrieved from the back-end code that calls the Text to Speech API. Here, I can create a new <strong>AudioContext</strong> object. The AudioContext interface represents an audio-processing graph built from audio modules linked together, each represented by an AudioNode. An audio context controls both the creation of audio nodes it contains and the execution of the audio processing, or decoding.</li><li>Now, let’s create an audio source for Web Audio API from an ArrayBuffer. The decoded AudioBuffer is resampled to the AudioContext’s sampling rate, then passed to a callback.</li><li>A user agent could block autoplay, hence why I run <strong>audioContext.resume</strong> as a trick first. Afterwards, create a new AudioBufferSourceNode to connect to the audioContext destination, which are in our case the device speakers. The <strong>buffer</strong> property of the AudioBufferSourceNode interface provides the ability to play back audio using an AudioBuffer as the source of the sound data. Finally, let’s play the audio.</li></ol></br><h1 >Server-side code to convert text to an AudioBuffer</h1><p class="article">Since I’m writing JavaScript code on the back-end for Node.js, I can make use of a Google Cloud client-side SDK for <a href="https://www.npmjs.com/package/@google-cloud/text-to-speech">TTS</a>.</p><p class="article">Run <strong >npm install @google-cloud/text-to-speech</strong> to install the latest package in your project. Once you downloaded the package, you can require the package in the top of your code:</p></br><pre class="ty tz ua ub fc ui uj uk ul aw qg bi"><span>const textToSpeech = require(‘@google-cloud/text-to-speech’);</span></pre></br><p class="article">First, I instantiate the <strong >TextToSpeechClient()</strong> from the textToSpeech npm package. Then create a request object, which contains settings such as the voice language, voice gender and the audioEncoding. <a href="https://cloud.google.com/text-to-speech/docs/reference/rest/v1/text/synthesize">Here’s an overview of all the settings</a>.</p></br><pre class="ty tz ua ub fc ui uj uk ul aw qg bi"><span>let ttsClient, requestTTS;</span><span>ttsClient = new textToSpeech.TextToSpeechClient();</span><span>requestTTS = {</span><span>    voice: {</span><span>        languageCode: ‘en-US’, //https://www.rfc-editor.org/rfc/bcp/bcp47.txt</span><span>        ssmlGender: ‘NEUTRAL’ // ‘MALE|FEMALE|NEUTRAL’</span><span>    },</span><span>    audioConfig: {</span><span>    audioEncoding: encoding,    //’LINEAR16|MP3|AUDIO_ENCODING_UNSPECIFIED/OGG_OPUS’</span><span>    }</span><span>};</span></pre></br><p class="article">This part finally makes the <strong >synthesizeSpeech</strong> call, which is asynchronous, the await operator is used to wait for a Promise, from the response, I return the audioContent that contains the audio buffer:</p></br><pre class="ty tz ua ub fc ui uj uk ul aw qg bi"><span>async function textToAudioBuffer(text) {</span><span>    requestTTS.input = { text: text }; // text or SSML</span><span>    const response = await ttsClient.synthesizeSpeech(requestTTS);</span><span>    return response[0].audioContent;</span><span>}</span></pre></br><h1 >TTS in Dialogflow</h1><p class="article">Dialogflow, the tool to create chat agents, can also return AudioBuffers once it detected the intent. You would only need to specify an <strong >outputAudioConfig</strong> in the Dialogflow <a href="https://cloud.google.com/dialogflow/docs/reference/rpc/google.cloud.dialogflow.v2#detectintentrequest">DetectIntentRequest</a>, in order to also get an AudioBuffer as part of the response:</p></br><pre class="ty tz ua ub fc ui uj uk ul aw qg bi"><span>outputAudioConfig: {</span><span>    audioEncoding: `OUTPUT_AUDIO_ENCODING_LINEAR_16`,</span><span>},</span></pre></br><p class="article"><a href="https://cloud.google.com/dialogflow/docs/detect-intent-tts">You can follow this guide, for the full code.</a> To play it in the browser, you can use the same instructions as I showed, when working with the Text to Speech API directly.</p><p class="article">The back-end listens to the ‘tts’ event, which was fired from the client-side.</p><p class="article">You can find the full creation of the <a href="https://github.com/dialogflow/selfservicekiosk-audio-streaming/blob/master/examples/simpleserver.js">Express server code here</a>. In case you want to run this yourself. Call the method: <strong >textToAudioBuffer</strong>() it will pass the string text as a parameter, and it returns a Promise to chain a function that passes the response (which eventually will be the AudioBuffer), to the client-side via Socket.IO emit:</p></br><pre class="ty tz ua ub fc ui uj uk ul aw qg bi"><span>ss(client).on(‘tts’, function(text) {</span><span>    textToAudioBuffer(text).then(function(results){</span><span>        console.log(results);</span><span>        client.emit(‘results’, results);</span><span>    }).catch(function(e){</span><span>        console.log(e);</span><span>    });</span><span>});</span></pre></br><p class="article"><strong >Caution</strong>: Be aware of using Dialogflow detect intent on streaming audio. When you use simple detectIntent calls without streaming, you stop the microphone and you will play the TTS audio buffer. However, when you do streaming, you keep your microphone open. You don’t want to end-up in an endless loop, where the speech synthesizer records new streams based on the TTS response, through your microphone. :-)</p><p class="article">The <a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioBufferSourceNode">AudioBufferSourceNode</a> has an onended event handler. Which will run once the AudioBufferSourceNode stopped playing the audio. In case you want to solve the above problem, you could set a <strong >boolean flag; isPlaying</strong>, which should block the recorder from sending the stream to the back-end when it’s set to true.</p><p class="article">Congratulations! By reading this blog series, you now know how to build an end-to-end solution for streaming audio from a microphone to a server, and stream & play the audio results back in the browser!</p><p class="article">Do you want to play around with these examples? I am <a href="http://selfservicedesk.appspot.com/">hosting a web demo online</a>. Also I’ve <a href="https://youtu.be/6JD8WC1LV7g">a video recording of one of my conference talks</a>!</p></br><div class="image"> <img src ="https://miro.medium.com/max/640/0*pfIRKUC8neSiP4iL.webp"></div></div></div>

</div>



<div class="copyrights">
  <div class="container">
    <div class="footer-distributed">
      <div class="footer-left">

        <a href="#"><i class="fa wp-icon fa-facebook-f fa-lg"></i></a>
        <a href="#"><i class="fa wp-icon fa-linkedin fa-lg"></i></a>
        <a href="#"><i class="fa wp-icon fa-twitter fa-lg"></i></a>
        <a href="#"><i class="fa wp-icon fa-instagram fa-lg"></i></a>
        <a href="#"><i class="fa wp-icon fa-google-plus fa-lg"></i></a>
        <a href="#"><i class="fa wp-icon fa-youtube fa-lg"></i></a>

        <p class="footer-company-name">All Rights Reserved. &copy; 2018 <a href="#">Dominic</a> Design By :
          <a href="https://html.design/">html design</a>
        </p>
      </div>
    </div>
  </div><!-- end container -->
</div><!-- end copyrights -->

<a href="#" id="scroll-to-top" class="dmtop global-radius"><i class="fa fa-angle-up"></i></a>

<!-- ALL JS FILES -->
<script src="js/all.js"></script>
<!-- Camera Slider -->
<script src="js/jquery.mobile.customized.min.js"></script>
<script src="js/jquery.easing.1.3.js"></script>
<script src="js/parallaxie.js"></script>
<script src="js/headline.js"></script>
<!-- Contact form JavaScript -->
<script src="js/jqBootstrapValidation.js"></script>
<script src="js/contact_me.js"></script>
<!-- ALL PLUGINS -->
<script src="js/custom.js"></script>
<script src="js/jquery.vide.js"></script>

</body>

</html>