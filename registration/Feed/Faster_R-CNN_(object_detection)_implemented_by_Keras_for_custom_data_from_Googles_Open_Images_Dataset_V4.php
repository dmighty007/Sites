<!DOCTYPE html>
                <html>
                <head>
                    <title>Faster R-CNN (object detection) implemented by Keras for custom data from Google’s Open Images Dataset V4</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://towardsdatascience.com/faster-r-cnn-object-detection-implemented-by-keras-for-custom-data-from-googles-open-images-125f62b9141a"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://medium.com/@rockyxu399?source=post_page-----125f62b9141a--------------------------------">Author : Yinghan Xu</a> </h5></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*YubY2mm5hAELjsxno4ToqQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*YubY2mm5hAELjsxno4ToqQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*YubY2mm5hAELjsxno4ToqQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*YubY2mm5hAELjsxno4ToqQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*YubY2mm5hAELjsxno4ToqQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*YubY2mm5hAELjsxno4ToqQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YubY2mm5hAELjsxno4ToqQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*YubY2mm5hAELjsxno4ToqQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*YubY2mm5hAELjsxno4ToqQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*YubY2mm5hAELjsxno4ToqQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*YubY2mm5hAELjsxno4ToqQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*YubY2mm5hAELjsxno4ToqQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*YubY2mm5hAELjsxno4ToqQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*YubY2mm5hAELjsxno4ToqQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*YubY2mm5hAELjsxno4ToqQ.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>Faster R-CNN (object detection) implemented by Keras for custom data from Google’s Open Images Dataset V4</h3></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Introduction</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After exploring CNN for a while, I decided to try another crucial area in Computer Vision, <strong>object detection</strong>. There are several methods popular in this area, including <strong>Faster R-CNN, RetinaNet, YOLOv3, SSD and etc</strong>. I tried Faster R-CNN in this article. Here, I want to summarise what I have learned and maybe give you a little inspiration if you are interested in this topic.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The original code of Keras version of Faster R-CNN I used was written by <a href="https://github.com/yhenon" target="_self">yhenon</a> (resource link: <a href="https://github.com/yhenon/keras-frcnn" target="_self">GitHub</a> .) He used the PASCAL VOC 2007, 2012, and MS COCO datasets. For me, I just extracted three classes, “Person”, “Car” and “Mobile phone”, from Google’s <a href="https://storage.googleapis.com/openimages/web/index.html" target="_self">Open Images Dataset V4</a>. I applied configs different from his work to fit my dataset and I removed unuseful code. Btw, to run this on <a href="https://colab.research.google.com/notebooks/welcome.ipynb" target="_self">Google Colab</a> (for free GPU computing up to 12hrs), I compressed all the code into three .ipynb notebooks. Sorry for the messy structure.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To start with, I assume you know the basic knowledge of CNN and what is object detection. This is the link for <a href="https://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf" target="_self">original paper</a>, named “<strong>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</strong>”. For someone who wants to implement custom data from Google’s Open Images Dataset V4 on Faster R-CNN, you should keep read the content below.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I read many articles explaining topics relative to Faster R-CNN. They have a good understanding and better explanation around this. Btw, if you already know the details about Faster R-CNN and are more curious about the code, you can skip the part below and directly jump to the code explanation part. This is my <a href="https://github.com/RockyXu66/Faster_RCNN_for_Open_Images_Dataset_Keras" target="_self">GitHub</a> link for this project.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Recommendation for reading:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><a href="https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/" target="_self">Faster R-CNN: Down the rabbit hole of modern object detection</a></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><a href="https://medium.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9" target="_self">Deep Learning for Object Detection: A Comprehensive Review</a></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><a href="https://medium.com/comet-app/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852" target="_self">Review of Deep Learning Algorithms for Object Detection</a></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Faster R-CNN (Brief explanation)</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>R-CNN</strong> <a href="http://islab.ulsan.ac.kr/files/announcement/513/rcnn_pami.pdf" target="_self">(R. Girshick et al., 2014)</a> is the first step for Faster R-CNN. It uses <strong>search selective</strong> (<a href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf" target="_self">J.R.R. Uijlings and al. (2012)</a>) to find out the regions of interests and passes them to a ConvNet. It tries to find out the areas that might be an object by combining similar pixels and textures into several rectangular boxes. The R-CNN paper uses 2,000 proposed areas (rectangular boxes) from search selective. Then, these 2,000 areas are passed to a pre-trained CNN model. Finally, the outputs (feature maps) are passed to a SVM for classification. The regression between predicted bounding boxes (bboxes) and ground-truth bboxes are computed.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*cyA4h6DWI5o1hhBFJSUhbQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*cyA4h6DWI5o1hhBFJSUhbQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*cyA4h6DWI5o1hhBFJSUhbQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*cyA4h6DWI5o1hhBFJSUhbQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*cyA4h6DWI5o1hhBFJSUhbQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*cyA4h6DWI5o1hhBFJSUhbQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cyA4h6DWI5o1hhBFJSUhbQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*cyA4h6DWI5o1hhBFJSUhbQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*cyA4h6DWI5o1hhBFJSUhbQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*cyA4h6DWI5o1hhBFJSUhbQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*cyA4h6DWI5o1hhBFJSUhbQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*cyA4h6DWI5o1hhBFJSUhbQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*cyA4h6DWI5o1hhBFJSUhbQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*cyA4h6DWI5o1hhBFJSUhbQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*cyA4h6DWI5o1hhBFJSUhbQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Example of search selective. Source: <a href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf" target="_self">J.R.R. Uijlings and al. (2012)</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Fast R-CNN</strong> (<a href="https://arxiv.org/pdf/1504.08083.pdf" target="_self">R. Girshick (2015)</a>) moves one step forward. Instead of applying 2,000 times CNN to proposed areas, it only passes the original image to a pre-trained CNN model once. <mark>Search selective algorithm is computed base on the output feature map of the previous step</mark>. Then, ROI pooling layer is used to ensure the standard and pre-defined output size. These valid outputs are passed to a fully connected layer as inputs. Finally, two output vectors are used to predict the observed object with a softmax classifier and adapt bounding box localisations with a linear regressor.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Faster R-CNN </strong>(frcnn for short) makes further progress than Fast R-CNN. Search selective process is replaced by <strong>Region Proposal Network</strong> (RPN). As the name revealed, RPN is a network to propose regions. For instance, after getting the output feature map from a pre-trained model (VGG-16), if the input image has 600x800x3 dimensions, the output feature map would be 37x50x256 dimensions.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*ACLCb8IGEVOsdISoUfen8Q.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*ACLCb8IGEVOsdISoUfen8Q.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*ACLCb8IGEVOsdISoUfen8Q.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*ACLCb8IGEVOsdISoUfen8Q.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*ACLCb8IGEVOsdISoUfen8Q.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ACLCb8IGEVOsdISoUfen8Q.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ACLCb8IGEVOsdISoUfen8Q.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*ACLCb8IGEVOsdISoUfen8Q.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*ACLCb8IGEVOsdISoUfen8Q.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*ACLCb8IGEVOsdISoUfen8Q.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*ACLCb8IGEVOsdISoUfen8Q.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*ACLCb8IGEVOsdISoUfen8Q.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*ACLCb8IGEVOsdISoUfen8Q.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*ACLCb8IGEVOsdISoUfen8Q.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*ACLCb8IGEVOsdISoUfen8Q.jpeg"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">First step of frcnn</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Each point in 37x50 is considered as an anchor. We need to define specific ratios and sizes for each anchor (1:1, 1:2, 2:1 for three ratios and 128², 256², 512² for three sizes in the original image).</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*9wCnE_PZDAogWB3nAYB95w.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*9wCnE_PZDAogWB3nAYB95w.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*9wCnE_PZDAogWB3nAYB95w.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*9wCnE_PZDAogWB3nAYB95w.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*9wCnE_PZDAogWB3nAYB95w.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*9wCnE_PZDAogWB3nAYB95w.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9wCnE_PZDAogWB3nAYB95w.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*9wCnE_PZDAogWB3nAYB95w.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*9wCnE_PZDAogWB3nAYB95w.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*9wCnE_PZDAogWB3nAYB95w.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*9wCnE_PZDAogWB3nAYB95w.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*9wCnE_PZDAogWB3nAYB95w.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*9wCnE_PZDAogWB3nAYB95w.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*9wCnE_PZDAogWB3nAYB95w.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*9wCnE_PZDAogWB3nAYB95w.jpeg"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">One anchor projected to the original image.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*fxy--Qcd4Ad1mRvsBZfwfg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*fxy--Qcd4Ad1mRvsBZfwfg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*fxy--Qcd4Ad1mRvsBZfwfg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*fxy--Qcd4Ad1mRvsBZfwfg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*fxy--Qcd4Ad1mRvsBZfwfg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*fxy--Qcd4Ad1mRvsBZfwfg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fxy--Qcd4Ad1mRvsBZfwfg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*fxy--Qcd4Ad1mRvsBZfwfg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*fxy--Qcd4Ad1mRvsBZfwfg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*fxy--Qcd4Ad1mRvsBZfwfg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*fxy--Qcd4Ad1mRvsBZfwfg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*fxy--Qcd4Ad1mRvsBZfwfg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*fxy--Qcd4Ad1mRvsBZfwfg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*fxy--Qcd4Ad1mRvsBZfwfg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*fxy--Qcd4Ad1mRvsBZfwfg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Anchor centers throught the original image. Source: <a href="https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/" target="_self">https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Next, RPN is connected to a Conv layer with 3x3 filters, 1 padding, 512 output channels. The output is connected to two 1x1 convolutional layer for classification and box-regression (Note that the classification here is to determine if the box is an object or not).</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*mgcg1mWwOb2S_qSuT_KDRw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*mgcg1mWwOb2S_qSuT_KDRw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*mgcg1mWwOb2S_qSuT_KDRw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*mgcg1mWwOb2S_qSuT_KDRw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*mgcg1mWwOb2S_qSuT_KDRw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*mgcg1mWwOb2S_qSuT_KDRw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mgcg1mWwOb2S_qSuT_KDRw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*mgcg1mWwOb2S_qSuT_KDRw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*mgcg1mWwOb2S_qSuT_KDRw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*mgcg1mWwOb2S_qSuT_KDRw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*mgcg1mWwOb2S_qSuT_KDRw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*mgcg1mWwOb2S_qSuT_KDRw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*mgcg1mWwOb2S_qSuT_KDRw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*mgcg1mWwOb2S_qSuT_KDRw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*mgcg1mWwOb2S_qSuT_KDRw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Convolutional implementation of an RPN architecture, where k is the number of anchors. (k=9 in here) Source: <a href="https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/" target="_self">https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4"><a href="https://tryolabs.com/blog/authors/javier-rey/" target="_self">Javier</a><em>: </em>For training, we take all the anchors and put them into two different categories. Those that overlap a ground-truth object with an <a href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/" target="_self">Intersection over Union</a> (IoU) bigger than 0.5 are considered “foreground” and those that don’t overlap any ground truth object or have less than 0.1 IoU with ground-truth objects are considered “background”.</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In this case, every anchor has 3x3 = 9 corresponding boxes in the original image, which means there are 37x50x9 = 16650 boxes in the original image. We just choose 256 of these 16650 boxes as a mini batch which contains 128 foregrounds (pos) and 128 backgrounds (neg). At the same time, <code>non-maximum suppression</code> is applied to make sure there is no overlapping for the proposed regions.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">RPN is finished after going through the above steps. Then we go to the second stage of frcnn. Similar to Fast R-CNN, ROI pooling is used for these proposed regions (ROIs). The output is 7x7x512. Then, we flatten this layer with some fully connected layers. The final step is a softmax function for classification and linear regression to fix the boxes’ location.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*UY0oc0GqK_g-z2Vsotp4TA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*UY0oc0GqK_g-z2Vsotp4TA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*UY0oc0GqK_g-z2Vsotp4TA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*UY0oc0GqK_g-z2Vsotp4TA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*UY0oc0GqK_g-z2Vsotp4TA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*UY0oc0GqK_g-z2Vsotp4TA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UY0oc0GqK_g-z2Vsotp4TA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*UY0oc0GqK_g-z2Vsotp4TA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*UY0oc0GqK_g-z2Vsotp4TA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*UY0oc0GqK_g-z2Vsotp4TA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*UY0oc0GqK_g-z2Vsotp4TA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*UY0oc0GqK_g-z2Vsotp4TA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*UY0oc0GqK_g-z2Vsotp4TA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*UY0oc0GqK_g-z2Vsotp4TA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*UY0oc0GqK_g-z2Vsotp4TA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">R-CNN architecture. Source: <a href="https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/" target="_self">https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Code explanation</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Part 1: Extract annotation for custom classes from Google’s Open Images Dataset v4 (Bounding Boxes)</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Download and load three .csv files</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In the official <a href="https://storage.googleapis.com/openimages/web/download.html" target="_self">website</a>, you can download <code>class-descriptions-boxable.csv</code> by clicking the red box in the bottom of below image named <code>Class Names</code>. Then go to the <code>Download from Figure Eight</code> and download other two files.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*XoJNr8IImy5cTeP9vcPAag.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*XoJNr8IImy5cTeP9vcPAag.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*XoJNr8IImy5cTeP9vcPAag.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*XoJNr8IImy5cTeP9vcPAag.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*XoJNr8IImy5cTeP9vcPAag.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*XoJNr8IImy5cTeP9vcPAag.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XoJNr8IImy5cTeP9vcPAag.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*XoJNr8IImy5cTeP9vcPAag.png 640w, https://miro.medium.com/v2/resize:fit:720/1*XoJNr8IImy5cTeP9vcPAag.png 720w, https://miro.medium.com/v2/resize:fit:750/1*XoJNr8IImy5cTeP9vcPAag.png 750w, https://miro.medium.com/v2/resize:fit:786/1*XoJNr8IImy5cTeP9vcPAag.png 786w, https://miro.medium.com/v2/resize:fit:828/1*XoJNr8IImy5cTeP9vcPAag.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*XoJNr8IImy5cTeP9vcPAag.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*XoJNr8IImy5cTeP9vcPAag.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*XoJNr8IImy5cTeP9vcPAag.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Screen shot of Open Images website</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In the Figure Eight website, I downloaded the <code>train-annotaion-bbox.csv</code> and <code>train-images-boxable.csv</code> like the image below.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*nqM6xqhHkByjgbQNF97xXg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*nqM6xqhHkByjgbQNF97xXg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*nqM6xqhHkByjgbQNF97xXg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*nqM6xqhHkByjgbQNF97xXg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*nqM6xqhHkByjgbQNF97xXg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*nqM6xqhHkByjgbQNF97xXg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nqM6xqhHkByjgbQNF97xXg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*nqM6xqhHkByjgbQNF97xXg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*nqM6xqhHkByjgbQNF97xXg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*nqM6xqhHkByjgbQNF97xXg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*nqM6xqhHkByjgbQNF97xXg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*nqM6xqhHkByjgbQNF97xXg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*nqM6xqhHkByjgbQNF97xXg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*nqM6xqhHkByjgbQNF97xXg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*nqM6xqhHkByjgbQNF97xXg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Screen shot of Figure Eight</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After downloading them, let’s look at what’s inside these files now. <code>train-images-boxable.csv</code> contains the boxable image name and their URL link. <code>class-descriptions-boxable.csv</code> contains the class name corresponding to their class LabelName. <code>train-annotations-bbox.csv</code> has more information. Each row in the <code>train-annotations-bbox.csv</code> contains one bounding box (bbox for short) coordinates for one image, and it also has this bbox’s LabelName and current image’s ID (ImageID+’.jpg’=Image_name). <code>XMin, YMin</code> is the top left point of this bbox and <code>XMax, YMax</code> is the bottom right point of this bbox. Please note that these coordinates values are normalised and should be computed for the real coordinates if needed.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*ZwPDKeVO07GoBskBlC-WvA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*ZwPDKeVO07GoBskBlC-WvA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*ZwPDKeVO07GoBskBlC-WvA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*ZwPDKeVO07GoBskBlC-WvA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*ZwPDKeVO07GoBskBlC-WvA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ZwPDKeVO07GoBskBlC-WvA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZwPDKeVO07GoBskBlC-WvA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*ZwPDKeVO07GoBskBlC-WvA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*ZwPDKeVO07GoBskBlC-WvA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*ZwPDKeVO07GoBskBlC-WvA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*ZwPDKeVO07GoBskBlC-WvA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*ZwPDKeVO07GoBskBlC-WvA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*ZwPDKeVO07GoBskBlC-WvA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*ZwPDKeVO07GoBskBlC-WvA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*ZwPDKeVO07GoBskBlC-WvA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">First 5 rows of three .csv file</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Get the subset of the whole dataset</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The whole dataset of <a href="https://storage.googleapis.com/openimages/web/download.html" target="_self">Open Images Dataset V4</a> which contains 600 classes is too large for me. So I extract 1,000 images for three classes, ‘Person’, ‘Mobile phone’ and ‘Car’ respectively.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After downloading these 3,000 images, I saved the useful annotation info in a .txt file. Each row has the format like this: file_path,x1,y1,x2,y2,class_name (no space just comma between two values) where file_path is the absolute file path for this image, (x1,y1) and (x2,y2) represent the top left and bottom right real coordinates of the original image, class_name is the class name of the current bounding box. I used 80% images for training and 20% images for testing. The expected number of training images and testing images should be 3x800 -&gt; 2400 and 3x200 -&gt; 600. However, there might be some overlapped images which appear in two or three classes simultaneously. For instance, an image might be a person walking on the street, and there are several cars in the street. So the number of bboxes for training images is 7236, and the number of bboxes for testing images is 1931.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*VuHw_4_YxjZ9q9Zd-xzUpA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*VuHw_4_YxjZ9q9Zd-xzUpA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*VuHw_4_YxjZ9q9Zd-xzUpA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*VuHw_4_YxjZ9q9Zd-xzUpA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*VuHw_4_YxjZ9q9Zd-xzUpA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*VuHw_4_YxjZ9q9Zd-xzUpA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VuHw_4_YxjZ9q9Zd-xzUpA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*VuHw_4_YxjZ9q9Zd-xzUpA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*VuHw_4_YxjZ9q9Zd-xzUpA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*VuHw_4_YxjZ9q9Zd-xzUpA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*VuHw_4_YxjZ9q9Zd-xzUpA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*VuHw_4_YxjZ9q9Zd-xzUpA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*VuHw_4_YxjZ9q9Zd-xzUpA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*VuHw_4_YxjZ9q9Zd-xzUpA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*VuHw_4_YxjZ9q9Zd-xzUpA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">One sample from my extracting data. The left is original image downloaded from given url and the right is drawn by adding bounding boxes parsered from train-annotations-bbox.csv.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Part 2: Faster R-CNN code</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I will explain some main functions in the codes. The complete comments for each function are written in the .jpynb notebooks. Note that I keep the resized image to 300 for faster training instead of 600 that I explained in the Part 1.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Rebuild the structure of VGG-16 and load pre-trained model (<code>nn_base</code>)</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:82%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 602px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*g8fe6eiwOcPKJ8jdkcrLtA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*g8fe6eiwOcPKJ8jdkcrLtA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*g8fe6eiwOcPKJ8jdkcrLtA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*g8fe6eiwOcPKJ8jdkcrLtA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*g8fe6eiwOcPKJ8jdkcrLtA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*g8fe6eiwOcPKJ8jdkcrLtA.png 1100w, https://miro.medium.com/v2/resize:fit:1204/format:webp/1*g8fe6eiwOcPKJ8jdkcrLtA.png 1204w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 602px" srcset="https://miro.medium.com/v2/resize:fit:640/1*g8fe6eiwOcPKJ8jdkcrLtA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*g8fe6eiwOcPKJ8jdkcrLtA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*g8fe6eiwOcPKJ8jdkcrLtA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*g8fe6eiwOcPKJ8jdkcrLtA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*g8fe6eiwOcPKJ8jdkcrLtA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*g8fe6eiwOcPKJ8jdkcrLtA.png 1100w, https://miro.medium.com/v2/resize:fit:1204/1*g8fe6eiwOcPKJ8jdkcrLtA.png 1204w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/602/1*g8fe6eiwOcPKJ8jdkcrLtA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">VGG-16 structure. Source: <a href="https://www.quora.com/What-is-the-VGG-neural-network" target="_self">https://www.quora.com/What-is-the-VGG-neural-network</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Prepare training data and training labels (<code>get_anchor_gt</code>)</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The input data is from annotation.txt file which contains a bunch of images with their bounding boxes information. We need to use RPN method to create proposed bboxes.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Arguments in this function<br></br><strong>all_img_data</strong>: list(filepath, width, height, list(bboxes))<br></br><strong>C</strong>: config<br></br><strong>img_length_calc_function</strong>: function to calculate final layer’s feature map (of base model) size according to input image size<br></br><strong>mode</strong>: ‘train’ or ‘test’; ‘train’ mode need augmentation</li><li class="ff3" style="font-size:22px;">Returns value in this function<br></br> <strong>x_img</strong>: image data after resized and scaling (smallest size = 300px)<br></br> <strong>Y</strong>: [y_rpn_cls, y_rpn_regr]<br></br> <strong>img_data_aug</strong>: augmented image data (original image with augmentation)<br></br> <strong>debug_img</strong>: show image for debug<br></br> <strong>num_pos</strong>: show number of positive anchors for debug</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><strong>Calculate rpn for each image (calc_rpn)</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If feature map has shape 18x25=450 and anchor sizes=9, there are 450x9=4050 potential anchors. The initial status for each anchor is ‘negative’. Then, we set the anchor to positive if the IOU is &gt;0.7. If the IOU is &gt;0.3 and &lt;0.7, it is ambiguous and not included in the objective. One issue is that the RPN has many more negative than positive regions, so we turn off some of the negative regions. We also limit the total number of positive regions and negative regions to 256. <code>y_is_box_valid</code> represents if this anchor has an object. <code>y_rpn_overlap</code> represents if this anchor overlaps with the ground-truth bounding box.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For ‘positive’ anchor, <code>y_is_box_valid</code> =1, <code>y_rpn_overlap</code> =1. <br></br>For ‘neutral’ anchor, <code>y_is_box_valid</code> =0, <code>y_rpn_overlap</code> =0. <br></br>For ‘negative’ anchor, <code>y_is_box_valid</code> =1, <code>y_rpn_overlap</code> =0.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Arguments in this function<br></br><strong>C</strong>: config<br></br><strong>img_data</strong>: augmented image data<br></br><strong>width</strong>: original image width (e.g. 600)<br></br><strong>height</strong>: original image height (e.g. 800)<br></br><strong>resized_width</strong>: resized image width according to C.im_size (e.g. 300)<br></br><strong>resized_height</strong>: resized image height according to C.im_size (e.g. 400)<br></br><strong>img_length_calc_function</strong>: function to calculate final layer’s feature map (of base model) size according to input image size</li><li class="ff3" style="font-size:22px;">Returns value in this function<br></br><strong>y_rpn_cls</strong>: list(num_bboxes, y_is_box_valid + y_rpn_overlap)<br></br><strong>y_is_box_valid</strong>: 0 or 1 (0 means the box is invalid, 1 means the box is valid)<br></br><strong>y_rpn_overlap</strong>: 0 or 1 (0 means the box is not an object, 1 means the box is an object)<br></br><strong>y_rpn_regr</strong>: list(num_bboxes, 4*y_rpn_overlap + y_rpn_regr)<br></br><strong>y_rpn_regr</strong>: x1,y1,x2,y2 bunding boxes coordinates</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The shape of <code>y_rpn_cls</code> is (1, 18, 25, 18). 18x25 is feature map size. Each point in feature map has 9 anchors, and each anchor has 2 values for <code>y_is_box_valid</code> and <code>y_rpn_overlap</code> respectively. So the fourth shape 18 is from 9x2.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The shape of <code>y_rpn_regr</code> is (1, 18, 25, 72). 18x25 is feature map size. Each point in feature map has 9 anchors and each anchor has 4 values for <code>tx</code>, <code>ty</code>, <code>tw</code> and <code>th</code> respectively. Note that these 4 value has their own <code>y_is_box_valid</code> and <code>y_rpn_overlap</code>. So the fourth shape 72 is from 9x4x2.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Calculate region of interest from RPN (rpn_to_roi)</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Arguments in this function (num_anchors = 9)<br></br> <strong>rpn_layer</strong>: output layer for rpn classification <br></br> shape (1, feature_map.height, feature_map.width, num_anchors)<br></br> Might be (1, 18, 25, 9) if resized image is 400 width and 300<br></br> <strong>regr_layer</strong>: output layer for rpn regression<br></br>shape (1, feature_map.height, feature_map.width, num_anchors*4)<br></br> Might be (1, 18, 25, 36) if resized image is 400 width and 300<br></br><strong> C</strong>: config<br></br> <strong>use_regr</strong>: Wether to use bboxes regression in rpn<br></br> <strong>max_boxes</strong>: max bboxes number for non-max-suppression (NMS)<br></br> <strong>overlap_thresh</strong>: If iou in NMS is larger than this threshold, drop the box</li><li class="ff3" style="font-size:22px;">Returns value in this function<br></br> <strong>result</strong>: boxes from non-max-suppression (shape=(300, 4))<br></br> <strong>boxes</strong>: coordinates for bboxes (on the feature map)</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For 4050 anchors from above step, we need to extract <code>max_boxes</code> (300 in the code) number of boxes as the region of interests and pass them to the classifier layer (second stage of frcnn). In the function, we first delete the boxes that overstep the original image. Then, we use non-max-suppression with 0.7 threshold value.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:65%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 484px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*odvgdafEShWGcJ0YrYnTSA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*odvgdafEShWGcJ0YrYnTSA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*odvgdafEShWGcJ0YrYnTSA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*odvgdafEShWGcJ0YrYnTSA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*odvgdafEShWGcJ0YrYnTSA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*odvgdafEShWGcJ0YrYnTSA.png 1100w, https://miro.medium.com/v2/resize:fit:968/format:webp/1*odvgdafEShWGcJ0YrYnTSA.png 968w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 484px" srcset="https://miro.medium.com/v2/resize:fit:640/1*odvgdafEShWGcJ0YrYnTSA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*odvgdafEShWGcJ0YrYnTSA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*odvgdafEShWGcJ0YrYnTSA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*odvgdafEShWGcJ0YrYnTSA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*odvgdafEShWGcJ0YrYnTSA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*odvgdafEShWGcJ0YrYnTSA.png 1100w, https://miro.medium.com/v2/resize:fit:968/1*odvgdafEShWGcJ0YrYnTSA.png 968w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/484/1*odvgdafEShWGcJ0YrYnTSA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">The green box is ground-truth bounding box. The purple box is the anchor (label) calculated by RPN layer.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">RoIPooling layer and Classifier layer (RoiPoolingConv, classifier_layer)</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">RoIPooling layer is the function to process the roi to a specific size output by max pooling. Every input roi is divided into some sub-cells, and we applied max pooling to each sub-cell. The number of sub-cells should be the dimension of the output shape.</p></div></div></div></section><section data-bs-version="5.1" class="gallery3 cid-ttaWMCQCls" id="gallery3-7" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container" style="width:60%;"><div class="row mt-4">
            <div class="item features-image сol-12 col-md-6 col-lg-6">
                <div class="item-wrapper" >
                    <div class="item-img"><img src ="https://miro.medium.com/v2/resize:fit:764/format:webp/1*NnOQVtbE_JsWoklZJu0g3w.png" style="height:auto; max-width: 100%;"></div></div></div><br>
            <div class="item features-image сol-12 col-md-6 col-lg-6">
                <div class="item-wrapper" >
                    <div class="item-img"><img src ="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*wi5oBU1jgqqSwOwhA8Fx3g.png" style="height:auto; max-width: 100%;"></div></div></div><br><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">If the large red rectangle in the left image is RoI and it’s supposed to perform a RoI Pooling layer with 2x2 output, it would be divided to 4 sub-cells and be applied a max pooling process. The right image is the 2x2 output result. Source: http://wavelab.uwaterloo.ca/wp-content/uploads/2017/04/Lecture_6.pdf</p><br></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Classifier layer is the final layer of the whole model and just behind the RoIPooling layer. It’s used to predict the class name for each input anchor and the regression of their bounding box.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Arguments in this function<br></br> <code>base_layers</code>: vgg<br></br> <code>input_rois</code>: `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)<br></br> <code>num_rois</code>: number of rois to be processed in one time (4 in here)</li><li class="ff3" style="font-size:22px;">Returns value in this function<br></br> list(out_class, out_regr)<br></br> <code>out_class</code>: classifier layer output<br></br> <code>out_regr</code>: regression layer output</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">First, the pooling layer is flattened. <br></br>Then, it’s followed with two fully connected layer and 0.5 dropout. <br></br>Finally, there are two output layers.<br></br> # out_class: softmax activation function for classifying the class name of the object<br></br> # out_regr: linear activation function for bboxes coordinates regression</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Dataset</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Again, my dataset is extracted from Google’s Open Images Dataset V4. Three classes for ‘Car’, ‘Person’ and ‘Mobile Phone’ are chosen. Every class contains around 1000 images. The number of bounding boxes for ‘Car’, ‘Mobile Phone’ and ‘Person’ is 2383, 1108 and 3745 respectively.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Parameters</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Resized (im_size) value is 300.</li><li class="ff3" style="font-size:22px;">The number of anchors is 9.</li><li class="ff3" style="font-size:22px;">Max number of non-max-suppression is 300.</li><li class="ff3" style="font-size:22px;">Number of RoI to process in the model is 4 (I haven’t tried larger size which might speed up the calculation but more memory needed)</li><li class="ff3" style="font-size:22px;">Adam is used for optimisation and the learning rate is 1e-5. It might works different if we applied the original paper’s solution. They used a learning rate of 0.001 for 60k mini-batches, and 0.0001 for the next 20k mini-batches on the PASCAL VOC dataset.</li><li class="ff3" style="font-size:22px;">For images augmentation, I turn on the horizontal_flips, vertical_flips and 90-degree rotations.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Environment</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Google’s Colab with Tesla K80 GPU acceleration for training.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><strong>Training time</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The length of each epoch that I choose is 1000. Note that every batch only processes one image in here. The total number of epochs I trained is 114. Every epoch spends around 700 seconds under this environment which means that the total time for training is around 22 hours. If you are using Colab’s GPU like me, you need to reconnect the server and load the weights when it disconnects automatically for continuing training because it has a time limitation for every session.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Result</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">There are two loss functions we applied to both the RPN model and Classifier model. As we mentioned before, RPN model has two output. One is for classifying whether it’s an object and the other one is for bounding boxes’ coordinates regression. From the figure below, we can see that it learned very fast at the first 20 epochs. Then, it became slower for classifier layer while the regression layer still keeps going down. The reason for this might be that the accuracy for objectness is already high for the early stage of our training, but at the same time, the accuracy of bounding boxes’ coordinates is still low and needs more time to learn.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*9hPil17YYbZuGx8jEZVt3Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*9hPil17YYbZuGx8jEZVt3Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*9hPil17YYbZuGx8jEZVt3Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*9hPil17YYbZuGx8jEZVt3Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*9hPil17YYbZuGx8jEZVt3Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*9hPil17YYbZuGx8jEZVt3Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9hPil17YYbZuGx8jEZVt3Q.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*9hPil17YYbZuGx8jEZVt3Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*9hPil17YYbZuGx8jEZVt3Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*9hPil17YYbZuGx8jEZVt3Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*9hPil17YYbZuGx8jEZVt3Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*9hPil17YYbZuGx8jEZVt3Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*9hPil17YYbZuGx8jEZVt3Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*9hPil17YYbZuGx8jEZVt3Q.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*9hPil17YYbZuGx8jEZVt3Q.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Epochs vs. Loss value for RPN model’s classifying output and bboxes regression output</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The similar learning process is shown in Classifier model. Compared with the two plots for bboxes’ regression, they show a similar tendency and even similar loss value. I think it’s because they are predicting the quite similar value with a little difference of their layer structure. Compared with two plots for classifying, we can see that predicting objectness is easier than predicting the class name of a bbox.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*xV-u-BMjjIgNjIzywM8BaA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*xV-u-BMjjIgNjIzywM8BaA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*xV-u-BMjjIgNjIzywM8BaA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*xV-u-BMjjIgNjIzywM8BaA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*xV-u-BMjjIgNjIzywM8BaA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*xV-u-BMjjIgNjIzywM8BaA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xV-u-BMjjIgNjIzywM8BaA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*xV-u-BMjjIgNjIzywM8BaA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*xV-u-BMjjIgNjIzywM8BaA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*xV-u-BMjjIgNjIzywM8BaA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*xV-u-BMjjIgNjIzywM8BaA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*xV-u-BMjjIgNjIzywM8BaA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*xV-u-BMjjIgNjIzywM8BaA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*xV-u-BMjjIgNjIzywM8BaA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*xV-u-BMjjIgNjIzywM8BaA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Epochs vs. Loss value for Classifier model’s classifying output and bboxes regression output</p><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:63%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 473px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*IhQuRCKKJwHU7ywkTI58LA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*IhQuRCKKJwHU7ywkTI58LA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*IhQuRCKKJwHU7ywkTI58LA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*IhQuRCKKJwHU7ywkTI58LA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*IhQuRCKKJwHU7ywkTI58LA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*IhQuRCKKJwHU7ywkTI58LA.png 1100w, https://miro.medium.com/v2/resize:fit:946/format:webp/1*IhQuRCKKJwHU7ywkTI58LA.png 946w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 473px" srcset="https://miro.medium.com/v2/resize:fit:640/1*IhQuRCKKJwHU7ywkTI58LA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*IhQuRCKKJwHU7ywkTI58LA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*IhQuRCKKJwHU7ywkTI58LA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*IhQuRCKKJwHU7ywkTI58LA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*IhQuRCKKJwHU7ywkTI58LA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*IhQuRCKKJwHU7ywkTI58LA.png 1100w, https://miro.medium.com/v2/resize:fit:946/1*IhQuRCKKJwHU7ywkTI58LA.png 946w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/473/1*IhQuRCKKJwHU7ywkTI58LA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Epochs vs. Total loss for two models</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This total loss is the sum of four losses above. It has a decreasing tendency. However, the mAP (mean average precision) doesn’t increase as the loss decreases. The mAP is 0.15 when the number of epochs is 60. The mAP is 0.19 when the number of epochs is 87. The mAP is 0.13 when the number of epochs is 114. I think this is because of the small number of training images which leads to overfitting of the model.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Other things we could tune</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;">For a shorter training process. I choose 300 as <code>im_size</code> for images resized instead of 600 in the original code (and original paper). So I choose a smaller <code>anchor_size</code> [64, 128, 256] instead of [128, 256, 512].</li><li class="ff3" style="font-size:22px;">I choose VGG-16 as my base model because it has a simpler structure. However, the model like ResNet-50 might have a better result for its better performance on image classification.</li><li class="ff3" style="font-size:22px;">There are many thresholds in the model. I used most of them as original code did. <code>rpn_max_overlap=0.7</code> and <code>rpn_min_overla=0.3</code> is the range to differentiate ‘positive’, ‘neutral’ and ‘negative’ for each anchor. <code>overlap_thresh=0.7</code> is the threshold for non-max-suppression.</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Test on images</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In the notebook, I splitted the training process and the testing process into two parts. <strong>Please reset all runtimes as below before running the test .ipynb notebook.</strong> And maybe you need to close the training notebook when running test notebook, because the memory usage is almost out of limitation.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*zYOK4d6aMmtLI8dbNOTdtQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*zYOK4d6aMmtLI8dbNOTdtQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*zYOK4d6aMmtLI8dbNOTdtQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*zYOK4d6aMmtLI8dbNOTdtQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*zYOK4d6aMmtLI8dbNOTdtQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*zYOK4d6aMmtLI8dbNOTdtQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zYOK4d6aMmtLI8dbNOTdtQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*zYOK4d6aMmtLI8dbNOTdtQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*zYOK4d6aMmtLI8dbNOTdtQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*zYOK4d6aMmtLI8dbNOTdtQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*zYOK4d6aMmtLI8dbNOTdtQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*zYOK4d6aMmtLI8dbNOTdtQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*zYOK4d6aMmtLI8dbNOTdtQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*zYOK4d6aMmtLI8dbNOTdtQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*zYOK4d6aMmtLI8dbNOTdtQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Screen shot for ‘Reset all runtimes’</p><br></div></div></div></div></section><section data-bs-version="5.1" class="gallery3 cid-ttaWMCQCls" id="gallery3-7" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container" style="width:60%;"><div class="row mt-4">
            <div class="item features-image сol-12 col-md-6 col-lg-6">
                <div class="item-wrapper" >
                    <div class="item-img"><img src ="https://miro.medium.com/v2/resize:fit:762/format:webp/1*oI-6U5FEDIARs6_AkIipKQ.jpeg" style="height:auto; max-width: 100%;"></div></div></div><br>
            <div class="item features-image сol-12 col-md-6 col-lg-6">
                <div class="item-wrapper" >
                    <div class="item-img"><img src ="https://miro.medium.com/v2/resize:fit:860/format:webp/1*uv2HUpM-y20_-Iqz-BylkQ.jpeg" style="height:auto; max-width: 100%;"></div></div></div><br>
            <div class="item features-image сol-12 col-md-6 col-lg-6">
                <div class="item-wrapper" >
                    <div class="item-img"><img src ="https://miro.medium.com/v2/resize:fit:382/format:webp/1*Sx56T2xngHCHD-_ep006BA.jpeg" style="height:auto; max-width: 100%;"></div></div></div><br></div></div></section><section data-bs-version="5.1" class="gallery3 cid-ttaWMCQCls" id="gallery3-7" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container" style="width:60%;"><div class="row mt-4">
            <div class="item features-image сol-12 col-md-6 col-lg-6">
                <div class="item-wrapper" >
                    <div class="item-img"><img src ="https://miro.medium.com/v2/resize:fit:600/format:webp/1*ukopHbI7VabWKh55JZXbwg.jpeg" style="height:auto; max-width: 100%;"></div></div></div><br>
            <div class="item features-image сol-12 col-md-6 col-lg-6">
                <div class="item-wrapper" >
                    <div class="item-img"><img src ="https://miro.medium.com/v2/resize:fit:710/format:webp/1*1uVs-g8v9ULAgOnLYeNbkQ.jpeg" style="height:auto; max-width: 100%;"></div></div></div><br>
            <div class="item features-image сol-12 col-md-6 col-lg-6">
                <div class="item-wrapper" >
                    <div class="item-img"><img src ="https://miro.medium.com/v2/resize:fit:694/format:webp/1*Q4S6iDMPgiKxKQV2qfFhcA.jpeg" style="height:auto; max-width: 100%;"></div></div></div><br><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Some result for the test data</p><br></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">At Last</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To have fun, you can create your own dataset that is not included in Google’s Open Images Dataset V4 and train them. For the cover image I use in this article, they are three porcoelainous monks made by China. I just named them according to their face look (not sure about the sleepy one). They are not included in the Open Images Dataset V4. So I use <a href="https://rectlabel.com" target="_self">RectLabel</a> to annotate by myself. I spent around 3 hours to dragged the ground-truth boxes for 6 classes with 465 images (including ‘<em>Apple Pen</em>’, ‘<em>Lipbalm</em>’, ‘<em>Scissor</em>’, ‘<em>Sleepy Monk</em>’, ‘<em>Upset Monk</em>’ and ‘<em>Happy Monk</em>’). For the <code>anchor_scaling_size</code>, I choose [32, 64, 128, 256] because the Lipbalm is usually small in the image. To find these small square lip balms. I added a smaller anchor size for a stronger model. Considering the Apple Pen is long and thin, the anchor_ratio could use 1:3 and 3:1 or even 1:4 and 4:1 but I haven’t tried. The training time was not long, and the performance was not bad. I guess it’s because of the relatively simple background and plain scene. Actually, I find out that the harder part is not to annotate this dataset but to think about how to photograph them to make the dataset more robust.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Alright, that’s all for this article. Thanks for your watching. This is my <a href="https://github.com/RockyXu66/Faster_RCNN_for_Open_Images_Dataset_Keras" target="_self">GitHub</a> link for this project. Go ahead and train your own object detector. If you have any problem, please leave your review. I’m glad to hear from you :)</p></div></div></div></section><section data-bs-version="5.1" class="gallery3 cid-ttaWMCQCls" id="gallery3-7" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container" style="width:60%;"><div class="row mt-4">
            <div class="item features-image сol-12 col-md-6 col-lg-6">
                <div class="item-wrapper" >
                    <div class="item-img"><img src ="https://miro.medium.com/v2/resize:fit:722/format:webp/1*wZgz5mebM-OKfjAO9S-tfw.jpeg" style="height:auto; max-width: 100%;"></div></div></div><br>
            <div class="item features-image сol-12 col-md-6 col-lg-6">
                <div class="item-wrapper" >
                    <div class="item-img"><img src ="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*v9bUZQ4yhgJ4nxx7dAWHPA.jpeg" style="height:auto; max-width: 100%;"></div></div></div><br><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Some result for my custom dataset</p><br></div></div></section><?php include_once 'Elemental/footer.php'; ?>