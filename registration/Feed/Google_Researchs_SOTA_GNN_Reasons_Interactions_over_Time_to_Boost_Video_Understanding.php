<!DOCTYPE html>
                <html>
                <head>
                    <title>Google Research’s SOTA GNN ‘Reasons’ Interactions over Time to Boost Video Understanding</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://medium.com/syncedreview/google-researchs-sota-gnn-reasons-interactions-over-time-to-boost-video-understanding-b121001ab3b3"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://medium.com/@synced?source=post_page-----b121001ab3b3--------------------------------">Author : Synced</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>Google Research’s SOTA GNN ‘Reasons’ Interactions over Time to Boost Video Understanding</h3></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*y6p_9pSgScf1vLoFkf-zcg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*y6p_9pSgScf1vLoFkf-zcg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*y6p_9pSgScf1vLoFkf-zcg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*y6p_9pSgScf1vLoFkf-zcg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*y6p_9pSgScf1vLoFkf-zcg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*y6p_9pSgScf1vLoFkf-zcg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y6p_9pSgScf1vLoFkf-zcg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*y6p_9pSgScf1vLoFkf-zcg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*y6p_9pSgScf1vLoFkf-zcg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*y6p_9pSgScf1vLoFkf-zcg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*y6p_9pSgScf1vLoFkf-zcg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*y6p_9pSgScf1vLoFkf-zcg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*y6p_9pSgScf1vLoFkf-zcg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*y6p_9pSgScf1vLoFkf-zcg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*y6p_9pSgScf1vLoFkf-zcg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In the last few years, deep learning driven computer vision (CV) research has achieved impressive progress on classifying video clips taken from the Internet and analyzing the human actions therein. Such video-based tasks are challenging, as they require an understanding of the interactions between humans, objects and other content and context within a given scene, as well as reasoning over long temporal intervals. A successful CV model in this area needs to capture both spatial and long-range temporal interactions while also being “intelligent” enough to reason based on its observations.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In the paper <em>Unified Graph Structured Models for Video Understanding</em>, a Google Research team proposes a message-passing graph neural network (MPNN) that can explicitly model these spatio-temporal relations, use either implicitly (with supervision) or explicitly (without supervision) captured representations of objects, and generalize previous structured models for video understanding.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*oSAttep0LVtkvB-g.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*oSAttep0LVtkvB-g.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*oSAttep0LVtkvB-g.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*oSAttep0LVtkvB-g.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*oSAttep0LVtkvB-g.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*oSAttep0LVtkvB-g.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oSAttep0LVtkvB-g.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*oSAttep0LVtkvB-g.png 640w, https://miro.medium.com/v2/resize:fit:720/0*oSAttep0LVtkvB-g.png 720w, https://miro.medium.com/v2/resize:fit:750/0*oSAttep0LVtkvB-g.png 750w, https://miro.medium.com/v2/resize:fit:786/0*oSAttep0LVtkvB-g.png 786w, https://miro.medium.com/v2/resize:fit:828/0*oSAttep0LVtkvB-g.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*oSAttep0LVtkvB-g.png 1100w, https://miro.medium.com/v2/resize:fit:1400/0*oSAttep0LVtkvB-g.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*oSAttep0LVtkvB-g.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The Google Research paper <em>Unified Graph Structured Models for Video Understanding </em>focuses on spatio-temporal action recognition and video scene graph parsing, which require reasoning about interactions between actors, objects and their environment in both space and time. Because video is a high-dimensional signal, it is not feasible to train large convolutional networks to learn from video datasets. Instead, previous work has proposed graph-structured models to tackle this issue. Some of these studies only modelled the spatial relations in videos, ignoring the interactions that can evolve over time, while other studies took long-range temporal interactions into consideration but failed to capture spatial relations. Although some studies modelled spatio-temporal interactions within a keyframe, these approaches require additional supervision for explicit representations of objects.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The proposed MPNN method aims to build structured representations of videos by representing them as a graph of actors, objects and contextual elements in a scene. MPNN performs coherent modelling of both spatial and temporal interactions, and uses action recognition and scene graph prediction to understand the interactions between elements in the graph.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*bXi75sPOBqlsflCc 640w, https://miro.medium.com/v2/resize:fit:720/0*bXi75sPOBqlsflCc 720w, https://miro.medium.com/v2/resize:fit:750/0*bXi75sPOBqlsflCc 750w, https://miro.medium.com/v2/resize:fit:786/0*bXi75sPOBqlsflCc 786w, https://miro.medium.com/v2/resize:fit:828/0*bXi75sPOBqlsflCc 828w, https://miro.medium.com/v2/resize:fit:1100/0*bXi75sPOBqlsflCc 1100w, https://miro.medium.com/v2/resize:fit:1400/0*bXi75sPOBqlsflCc 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*bXi75sPOBqlsflCc 640w, https://miro.medium.com/v2/resize:fit:720/0*bXi75sPOBqlsflCc 720w, https://miro.medium.com/v2/resize:fit:750/0*bXi75sPOBqlsflCc 750w, https://miro.medium.com/v2/resize:fit:786/0*bXi75sPOBqlsflCc 786w, https://miro.medium.com/v2/resize:fit:828/0*bXi75sPOBqlsflCc 828w, https://miro.medium.com/v2/resize:fit:1100/0*bXi75sPOBqlsflCc 1100w, https://miro.medium.com/v2/resize:fit:1400/0*bXi75sPOBqlsflCc 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*bXi75sPOBqlsflCc"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">MPNN is a flexible model that can operate on a directed or undirected graph. Its inference consists of a message-passing phase and a final readout phase. In the message-passing phase, messages are first computed by applying spatial and temporal message-passing functions. An update function then aggregates the received messages to update the latent state. Intuitively, the update function is updated by aggregating the messages passed from its neighbours. Finally, a readout function uses the updated node features to classify tasks of interest.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Spatial connections include the relationships between actors, objects and scene context. MPNN models scene context by considering the features from each spatial position in the feature map. The researchers also add an implicit object model to enable the network to encode information about the scene and relevant objects without any extra supervision. It is also possible to augment contextual nodes with an explicit object representation by computing class agnostic object proposals with a Region Proposal Network (RPN).</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:83%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 612px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*y_fJoLgCLqxzVcie.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*y_fJoLgCLqxzVcie.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*y_fJoLgCLqxzVcie.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*y_fJoLgCLqxzVcie.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*y_fJoLgCLqxzVcie.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*y_fJoLgCLqxzVcie.png 1100w, https://miro.medium.com/v2/resize:fit:1224/format:webp/0*y_fJoLgCLqxzVcie.png 1224w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 612px" srcset="https://miro.medium.com/v2/resize:fit:640/0*y_fJoLgCLqxzVcie.png 640w, https://miro.medium.com/v2/resize:fit:720/0*y_fJoLgCLqxzVcie.png 720w, https://miro.medium.com/v2/resize:fit:750/0*y_fJoLgCLqxzVcie.png 750w, https://miro.medium.com/v2/resize:fit:786/0*y_fJoLgCLqxzVcie.png 786w, https://miro.medium.com/v2/resize:fit:828/0*y_fJoLgCLqxzVcie.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*y_fJoLgCLqxzVcie.png 1100w, https://miro.medium.com/v2/resize:fit:1224/0*y_fJoLgCLqxzVcie.png 1224w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/612/0*y_fJoLgCLqxzVcie.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The team notes that understanding actions often requires reasoning about actors who are no longer visible in the current frame, thus requiring large temporal contexts. MPNN models temporal interactions by connecting foreground nodes in a keyframe with all other foreground nodes in neighbouring keyframes. By setting the sampling rate no less than one, it is possible to consider a wider temporal interval in a more computationally efficient manner to train the entire model end-to-end. The researchers explain that because each foreground feature node is a spatio-temporal feature computed by a 3D CNN, selecting adjacent keyframes could result in the capturing of redundant information via temporal connections.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The researchers evaluated MPNN on scene graph classification (SGCls), predicate classification (PredCls), and spatio-temporal action detection tasks. They used the Action Genome dataset for video scene graph classification and prediction, and the AVA and UCF101- 24 datasets for spatio-temporal action recognition.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:83%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 612px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*i04_aRDTQu6p0RYA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*i04_aRDTQu6p0RYA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*i04_aRDTQu6p0RYA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*i04_aRDTQu6p0RYA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*i04_aRDTQu6p0RYA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*i04_aRDTQu6p0RYA.png 1100w, https://miro.medium.com/v2/resize:fit:1224/format:webp/0*i04_aRDTQu6p0RYA.png 1224w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 612px" srcset="https://miro.medium.com/v2/resize:fit:640/0*i04_aRDTQu6p0RYA.png 640w, https://miro.medium.com/v2/resize:fit:720/0*i04_aRDTQu6p0RYA.png 720w, https://miro.medium.com/v2/resize:fit:750/0*i04_aRDTQu6p0RYA.png 750w, https://miro.medium.com/v2/resize:fit:786/0*i04_aRDTQu6p0RYA.png 786w, https://miro.medium.com/v2/resize:fit:828/0*i04_aRDTQu6p0RYA.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*i04_aRDTQu6p0RYA.png 1100w, https://miro.medium.com/v2/resize:fit:1224/0*i04_aRDTQu6p0RYA.png 1224w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/612/0*i04_aRDTQu6p0RYA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In video scene graph classification, the proposed spatio-temporal graph structured model improved substantially improved on the SlowFast-ResNet 50 3D baseline by 4.9 and 4.7 points for the R@20 and R@50 for SGCls respectively. The improvements over PreCIs were less pronounced, as the task is easier, leaving less room for improvement.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:82%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 603px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*rzY2BhU2qRzhyQp3.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*rzY2BhU2qRzhyQp3.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*rzY2BhU2qRzhyQp3.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*rzY2BhU2qRzhyQp3.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*rzY2BhU2qRzhyQp3.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*rzY2BhU2qRzhyQp3.png 1100w, https://miro.medium.com/v2/resize:fit:1206/format:webp/0*rzY2BhU2qRzhyQp3.png 1206w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 603px" srcset="https://miro.medium.com/v2/resize:fit:640/0*rzY2BhU2qRzhyQp3.png 640w, https://miro.medium.com/v2/resize:fit:720/0*rzY2BhU2qRzhyQp3.png 720w, https://miro.medium.com/v2/resize:fit:750/0*rzY2BhU2qRzhyQp3.png 750w, https://miro.medium.com/v2/resize:fit:786/0*rzY2BhU2qRzhyQp3.png 786w, https://miro.medium.com/v2/resize:fit:828/0*rzY2BhU2qRzhyQp3.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*rzY2BhU2qRzhyQp3.png 1100w, https://miro.medium.com/v2/resize:fit:1206/0*rzY2BhU2qRzhyQp3.png 1206w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/603/0*rzY2BhU2qRzhyQp3.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:82%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 603px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*fDZh2fYC8D6Fj1B7.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*fDZh2fYC8D6Fj1B7.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*fDZh2fYC8D6Fj1B7.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*fDZh2fYC8D6Fj1B7.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*fDZh2fYC8D6Fj1B7.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*fDZh2fYC8D6Fj1B7.png 1100w, https://miro.medium.com/v2/resize:fit:1206/format:webp/0*fDZh2fYC8D6Fj1B7.png 1206w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 603px" srcset="https://miro.medium.com/v2/resize:fit:640/0*fDZh2fYC8D6Fj1B7.png 640w, https://miro.medium.com/v2/resize:fit:720/0*fDZh2fYC8D6Fj1B7.png 720w, https://miro.medium.com/v2/resize:fit:750/0*fDZh2fYC8D6Fj1B7.png 750w, https://miro.medium.com/v2/resize:fit:786/0*fDZh2fYC8D6Fj1B7.png 786w, https://miro.medium.com/v2/resize:fit:828/0*fDZh2fYC8D6Fj1B7.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*fDZh2fYC8D6Fj1B7.png 1100w, https://miro.medium.com/v2/resize:fit:1206/0*fDZh2fYC8D6Fj1B7.png 1206w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/603/0*fDZh2fYC8D6Fj1B7.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In spatio-temporal action detection on AVA datasets, the proposed model showed substantial improvements with either a 3D ResNet 50 or ResNet 101 backbone baseline. On UCF101–24, the model also outperformed all other approaches.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*X6gzDRcnSGmV9lxZ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*X6gzDRcnSGmV9lxZ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*X6gzDRcnSGmV9lxZ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*X6gzDRcnSGmV9lxZ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*X6gzDRcnSGmV9lxZ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*X6gzDRcnSGmV9lxZ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*X6gzDRcnSGmV9lxZ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*X6gzDRcnSGmV9lxZ.png 640w, https://miro.medium.com/v2/resize:fit:720/0*X6gzDRcnSGmV9lxZ.png 720w, https://miro.medium.com/v2/resize:fit:750/0*X6gzDRcnSGmV9lxZ.png 750w, https://miro.medium.com/v2/resize:fit:786/0*X6gzDRcnSGmV9lxZ.png 786w, https://miro.medium.com/v2/resize:fit:828/0*X6gzDRcnSGmV9lxZ.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*X6gzDRcnSGmV9lxZ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/0*X6gzDRcnSGmV9lxZ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*X6gzDRcnSGmV9lxZ.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Overall, the researchers validated their novel spatio-temporal graph neural network framework’s ability to explicitly model both spatial and temporal interactions, achieving state-of-the-art results on two diverse tasks across three datasets.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The paper <em>Unified Graph Structured Models for Video Understanding </em>is on <a href="https://arxiv.org/pdf/2103.15662.pdf" target="_self">arXiv</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Author</strong>: Hecate He | <strong>Editor</strong>: Michael Sarazen</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*bFqWKaqtMhgGzpBe.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*bFqWKaqtMhgGzpBe.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*bFqWKaqtMhgGzpBe.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*bFqWKaqtMhgGzpBe.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*bFqWKaqtMhgGzpBe.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*bFqWKaqtMhgGzpBe.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bFqWKaqtMhgGzpBe.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*bFqWKaqtMhgGzpBe.png 640w, https://miro.medium.com/v2/resize:fit:720/0*bFqWKaqtMhgGzpBe.png 720w, https://miro.medium.com/v2/resize:fit:750/0*bFqWKaqtMhgGzpBe.png 750w, https://miro.medium.com/v2/resize:fit:786/0*bFqWKaqtMhgGzpBe.png 786w, https://miro.medium.com/v2/resize:fit:828/0*bFqWKaqtMhgGzpBe.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*bFqWKaqtMhgGzpBe.png 1100w, https://miro.medium.com/v2/resize:fit:1400/0*bFqWKaqtMhgGzpBe.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*bFqWKaqtMhgGzpBe.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We know you don’t want to miss any news or research breakthroughs. <strong>Subscribe to our popular newsletter </strong><a href="https://mailchi.mp/2fb3aa308ad3/welcome-to-synced-global-ai-weekly-newsletter" target="_self">Synced Global AI Weekly</a><strong> to get weekly AI updates.</strong></p></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>