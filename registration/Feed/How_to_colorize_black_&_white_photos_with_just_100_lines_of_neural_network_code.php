<!DOCTYPE html>
                <html>
                <head>
                    <title>How to colorize black & white photos with just 100 lines of neural network code</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://emilwallner.medium.com/colorize-b-w-photos-with-a-100-line-neural-network-53d9b4449f8d"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://medium.com/?source=post_page-----53d9b4449f8d--------------------------------">Author : Emil Wallner</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>How to colorize black & white photos with just 100 lines of neural network code</h3></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>👉 Check out the latest advancement in AI colorization, </strong><a href="https://palette.fm/" target="_self">my desktop browser app for colorization</a><strong>.</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><mark>Earlier this year, Amir Avni used neural networks to </mark><mark>troll the subreddit</mark><mark>/r/Colorization</mark><mark> — a community where people colorize historical black and white images manually using Photoshop.</mark></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">They were astonished with Amir’s deep learning bot. What could take up to a month of manual labour could now be done in just a few seconds.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I was fascinated by Amir’s neural network, so I reproduced it and documented the process. First off, let’s look at some of the results/failures from my experiments (scroll to the bottom for the final result).</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Wk43GNwKLB4kYjQt5i8ntQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Wk43GNwKLB4kYjQt5i8ntQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Wk43GNwKLB4kYjQt5i8ntQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Wk43GNwKLB4kYjQt5i8ntQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Wk43GNwKLB4kYjQt5i8ntQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Wk43GNwKLB4kYjQt5i8ntQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wk43GNwKLB4kYjQt5i8ntQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Wk43GNwKLB4kYjQt5i8ntQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*Wk43GNwKLB4kYjQt5i8ntQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*Wk43GNwKLB4kYjQt5i8ntQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*Wk43GNwKLB4kYjQt5i8ntQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*Wk43GNwKLB4kYjQt5i8ntQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*Wk43GNwKLB4kYjQt5i8ntQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*Wk43GNwKLB4kYjQt5i8ntQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*Wk43GNwKLB4kYjQt5i8ntQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">The original b&w images are from Unsplash</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Today, colorization is usually done by hand in Photoshop. To appreciate all the hard work behind this process, take a peek at this gorgeous colorization memory lane video:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:80%;"><iframe  scrolling="auto" width="854.0" height="480.0" frameborder="0" src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FvubuBrcAwtY%3Ffeature%3Doembed&display_name=YouTube&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DvubuBrcAwtY&image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FvubuBrcAwtY%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=youtube" allowfullscreen=""></iframe></div><br><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In short, a picture can take up to one month to colorize. It requires extensive research. A face alone needs up to 20 layers of pink, green and blue shades to get it just right.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This article is for beginners. Yet, if you’re new to deep learning terminology, you can read my previous two posts <a href="https://blog.floydhub.com/my-first-weekend-of-deep-learning/" target="_self">here</a> and <a href="https://blog.floydhub.com/coding-the-history-of-deep-learning/" target="_self">here</a>, and watch Andrej Karpathy’s <a href="https://www.youtube.com/watch?v=LxfUGhug-iQ" target="_self">lecture</a> for more background.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I’ll show you how to build your own colorization neural net in three steps.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The first section breaks down the core logic. We’ll build a bare-bones 40-line neural network as an “alpha” colorization bot. There’s not a lot of magic in this code snippet. This well help us become familiar with the syntax.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The next step is to create a neural network that can generalize — our “beta” version. We’ll be able to color images the bot has not seen before.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For our “final” version, we’ll combine our neural network with a classifier. We’ll use an <a href="https://research.googleblog.com/2016/08/improving-inception-and-image.html" target="_self">Inception Resnet V2</a> that has been trained on 1.2 million images. To make the coloring pop, we’ll train our neural network on portraits from <a href="https://unsplash.com/" target="_self">Unsplash</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you want to look ahead, here’s a <a href="https://www.floydhub.com/emilwallner/projects/color/43/code/Alpha-version/alpha_version.ipynb" target="_self">Jupyter Notebook</a> with the Alpha version of our bot. You can also check out the three versions on <a href="https://www.floydhub.com/emilwallner/projects/color/43/code" target="_self">FloydHub</a> and <a href="https://github.com/emilwallner/Coloring-greyscale-images-in-Keras" target="_self">GitHub</a>, along with code for <a href="https://www.floydhub.com/emilwallner/projects/color/jobs" target="_self">all the experiments</a> I ran on FloydHub’s cloud GPUs.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Core logic</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In this section, I’ll outline how to render an image, the basics of digital colors, and the main logic for our neural network.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Black and white images can be represented in grids of pixels. Each pixel has a value that corresponds to its brightness. The values span from 0–255, from black to white.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:68%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 507px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*QVT2380CUU_h7ZCSvR7T0Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*QVT2380CUU_h7ZCSvR7T0Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*QVT2380CUU_h7ZCSvR7T0Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*QVT2380CUU_h7ZCSvR7T0Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*QVT2380CUU_h7ZCSvR7T0Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*QVT2380CUU_h7ZCSvR7T0Q.png 1100w, https://miro.medium.com/v2/resize:fit:1014/format:webp/1*QVT2380CUU_h7ZCSvR7T0Q.png 1014w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 507px" srcset="https://miro.medium.com/v2/resize:fit:640/1*QVT2380CUU_h7ZCSvR7T0Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*QVT2380CUU_h7ZCSvR7T0Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*QVT2380CUU_h7ZCSvR7T0Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*QVT2380CUU_h7ZCSvR7T0Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*QVT2380CUU_h7ZCSvR7T0Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*QVT2380CUU_h7ZCSvR7T0Q.png 1100w, https://miro.medium.com/v2/resize:fit:1014/1*QVT2380CUU_h7ZCSvR7T0Q.png 1014w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/507/1*QVT2380CUU_h7ZCSvR7T0Q.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Color images consist of three layers: a red layer, a green layer, and a blue layer. This might be counter-intuitive to you. Imagine splitting a green leaf on a white background into the three channels. Intuitively, you might think that the plant is only present in the green layer.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">But, as you see below, the leaf is present in all three channels. The layers not only determine color, but also brightness.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*eLVP_ujVoM9pXRNpAT3bOA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*eLVP_ujVoM9pXRNpAT3bOA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*eLVP_ujVoM9pXRNpAT3bOA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*eLVP_ujVoM9pXRNpAT3bOA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*eLVP_ujVoM9pXRNpAT3bOA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*eLVP_ujVoM9pXRNpAT3bOA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eLVP_ujVoM9pXRNpAT3bOA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*eLVP_ujVoM9pXRNpAT3bOA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*eLVP_ujVoM9pXRNpAT3bOA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*eLVP_ujVoM9pXRNpAT3bOA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*eLVP_ujVoM9pXRNpAT3bOA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*eLVP_ujVoM9pXRNpAT3bOA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*eLVP_ujVoM9pXRNpAT3bOA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*eLVP_ujVoM9pXRNpAT3bOA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*eLVP_ujVoM9pXRNpAT3bOA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To achieve the color white, for example, you need an equal distribution of all colors. By adding an equal amount of red and blue, it makes the green brighter. Thus, a color image encodes the color and the contrast using three layers:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*n8Dux1lL5P3y8Njv1anoAA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*n8Dux1lL5P3y8Njv1anoAA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*n8Dux1lL5P3y8Njv1anoAA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*n8Dux1lL5P3y8Njv1anoAA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*n8Dux1lL5P3y8Njv1anoAA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*n8Dux1lL5P3y8Njv1anoAA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n8Dux1lL5P3y8Njv1anoAA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*n8Dux1lL5P3y8Njv1anoAA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*n8Dux1lL5P3y8Njv1anoAA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*n8Dux1lL5P3y8Njv1anoAA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*n8Dux1lL5P3y8Njv1anoAA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*n8Dux1lL5P3y8Njv1anoAA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*n8Dux1lL5P3y8Njv1anoAA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*n8Dux1lL5P3y8Njv1anoAA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*n8Dux1lL5P3y8Njv1anoAA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Just like black and white images, each layer in a color image has a value from 0–255. The value 0 means that it has no color in this layer. If the value is 0 for all color channels, then the image pixel is black.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">As you may know, a neural network creates a relationship between an input value and output value. To be more precise with our colorization task, the network needs to find the traits that link grayscale images with colored ones.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In sum, we are searching for the features that link a grid of grayscale values to the three color grids.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*W23SQ2oEdE_PsK-HmP4cow.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*W23SQ2oEdE_PsK-HmP4cow.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*W23SQ2oEdE_PsK-HmP4cow.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*W23SQ2oEdE_PsK-HmP4cow.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*W23SQ2oEdE_PsK-HmP4cow.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*W23SQ2oEdE_PsK-HmP4cow.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W23SQ2oEdE_PsK-HmP4cow.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*W23SQ2oEdE_PsK-HmP4cow.png 640w, https://miro.medium.com/v2/resize:fit:720/1*W23SQ2oEdE_PsK-HmP4cow.png 720w, https://miro.medium.com/v2/resize:fit:750/1*W23SQ2oEdE_PsK-HmP4cow.png 750w, https://miro.medium.com/v2/resize:fit:786/1*W23SQ2oEdE_PsK-HmP4cow.png 786w, https://miro.medium.com/v2/resize:fit:828/1*W23SQ2oEdE_PsK-HmP4cow.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*W23SQ2oEdE_PsK-HmP4cow.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*W23SQ2oEdE_PsK-HmP4cow.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*W23SQ2oEdE_PsK-HmP4cow.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">f() is the neural network, [B&W] is our input, and [R],[G],[B] is our output.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Alpha version</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We’ll start by making a simple version of our neural network to color an image of a woman’s face. This way, you can get familiar with the core syntax of our model as we add features to it.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">With just 40 lines of code, we can make the following transition. The middle picture is done with our neural network and the picture to the right is the original color photo. The network is trained and tested on the same image — we’ll get back to this during the beta-version.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Vd4u8Q34bZ4hAFpyJYlKPw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Vd4u8Q34bZ4hAFpyJYlKPw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Vd4u8Q34bZ4hAFpyJYlKPw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Vd4u8Q34bZ4hAFpyJYlKPw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Vd4u8Q34bZ4hAFpyJYlKPw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Vd4u8Q34bZ4hAFpyJYlKPw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vd4u8Q34bZ4hAFpyJYlKPw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Vd4u8Q34bZ4hAFpyJYlKPw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*Vd4u8Q34bZ4hAFpyJYlKPw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*Vd4u8Q34bZ4hAFpyJYlKPw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*Vd4u8Q34bZ4hAFpyJYlKPw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*Vd4u8Q34bZ4hAFpyJYlKPw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*Vd4u8Q34bZ4hAFpyJYlKPw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*Vd4u8Q34bZ4hAFpyJYlKPw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*Vd4u8Q34bZ4hAFpyJYlKPw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Photo by Camila Cordeiro</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Color space</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">First, we’ll use an algorithm to change the color channels, from RGB to Lab. <strong>L </strong>stands for lightness, and <strong>a</strong> and <strong>b</strong> for the color spectra green–red and blue–yellow.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">As you can see below, a Lab encoded image has one layer for grayscale, and has packed three color layers into two. This means that we can use the original grayscale image in our final prediction. Also, we only have two channels to predict.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*OX9DWIK6bOHKwTAp4Q92pQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*OX9DWIK6bOHKwTAp4Q92pQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*OX9DWIK6bOHKwTAp4Q92pQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*OX9DWIK6bOHKwTAp4Q92pQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*OX9DWIK6bOHKwTAp4Q92pQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*OX9DWIK6bOHKwTAp4Q92pQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OX9DWIK6bOHKwTAp4Q92pQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*OX9DWIK6bOHKwTAp4Q92pQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*OX9DWIK6bOHKwTAp4Q92pQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*OX9DWIK6bOHKwTAp4Q92pQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*OX9DWIK6bOHKwTAp4Q92pQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*OX9DWIK6bOHKwTAp4Q92pQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*OX9DWIK6bOHKwTAp4Q92pQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*OX9DWIK6bOHKwTAp4Q92pQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*OX9DWIK6bOHKwTAp4Q92pQ.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Science fact — 94% of the cells in our eyes determine brightness. That leaves only 6% of our receptors to act as sensors for colors. As you can see in the above image, the grayscale image is a lot sharper than the color layers. This is another reason to keep the grayscale image in our final prediction.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">From B&W to color</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Our final prediction looks like this. We have a grayscale layer for input, and we want to predict two color layers, the <strong>ab</strong> in <strong>Lab</strong>. To create the final color image we’ll include the <strong>L</strong>/grayscale image we used for the input. The result will be creating a <strong>Lab</strong> image.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*pZcnvgEuiTUywDgeMF_R3Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*pZcnvgEuiTUywDgeMF_R3Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*pZcnvgEuiTUywDgeMF_R3Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*pZcnvgEuiTUywDgeMF_R3Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*pZcnvgEuiTUywDgeMF_R3Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*pZcnvgEuiTUywDgeMF_R3Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pZcnvgEuiTUywDgeMF_R3Q.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*pZcnvgEuiTUywDgeMF_R3Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*pZcnvgEuiTUywDgeMF_R3Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*pZcnvgEuiTUywDgeMF_R3Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*pZcnvgEuiTUywDgeMF_R3Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*pZcnvgEuiTUywDgeMF_R3Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*pZcnvgEuiTUywDgeMF_R3Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*pZcnvgEuiTUywDgeMF_R3Q.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*pZcnvgEuiTUywDgeMF_R3Q.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To turn one layer into two layers, we use convolutional filters. Think of them as the blue/red filters in 3D glasses. Each filter determines what we see in a picture. They can highlight or remove something to extract information out of the picture. The network can either create a new image from a filter or combine several filters into one image.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For a convolutional neural network, each filter is automatically adjusted to help with the intended outcome. We’ll start by stacking hundreds of filters and narrow them down into two layers, the <strong>a </strong>and <strong>b</strong> layers.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Before we get into detail into how it works, let’s run the code.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Run the code on FloydHub</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><a href="https://floydhub.com/run?template=https://github.com/floydhub/colornet-template" target="_self">Click here</a> to open a <a href="https://blog.floydhub.com/workspaces/" target="_self">Workspace</a> on <a href="https://www.floydhub.com/?utm_medium=readme&utm_source=colornet&utm_campaign=aug_2018" target="_self">FloydHub</a> where you will find the same environment and dataset used for the <em>Full version</em>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">You can also make a local FloydHub installation with their <a href="https://www.floydhub.com/" target="_self">2-min installation</a>, watch my <a href="https://www.youtube.com/watch?v=byLQ9kgjTdQ&t=6s" target="_self">5-min video tutorial</a> or check out my <a href="https://blog.floydhub.com/my-first-weekend-of-deep-learning/" target="_self">step-to-step guide</a>. It’s the best (and easiest) way to train deep learning models on cloud GPUs.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Alpha version</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Once FloydHub is installed, use the following commands:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>git clone <a href="https://github.com/emilwallner/Coloring-greyscale-images-in-Keras" rel="noopener ugc nofollow" target="_blank">https://github.com/emilwallner/Coloring-greyscale-images-in-Keras</a></span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Open the folder and initiate FloydHub.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>cd Coloring-greyscale-images-in-Keras/floydhub<br/>floyd init colornet</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The FloydHub web dashboard will open in your browser. You will be prompted to create a new FloydHub project called <code>colornet</code>. Once that's done, go back to your terminal and run the same <code>init</code>command.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>floyd init colornet</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Okay, let’s run our job:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>floyd run --data emilwallner/datasets/colornet/2:data --mode jupyter --tensorboard</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Some quick notes about our job:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">We mounted a public dataset on FloydHub (which I’ve already uploaded) at the <code>data</code>directory with the below line:</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>--dataemilwallner/datasets/colornet/2:data</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">You can explore and use this dataset (and many other public datasets) by viewing it on <a href="https://www.floydhub.com/emilwallner/datasets/cifar-10/1" target="_self">FloydHub</a></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">We enabled Tensorboard with <code>--tensorboard</code></li><li class="ff3" style="font-size:22px;">We ran the job in Jupyter Notebook mode with <code>--mode jupyter</code></li><li class="ff3" style="font-size:22px;">If you have GPU credit, you can also add the GPU flag <code>--gpu</code>to your command. This will make it approximately 50x faster</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Go to the Jupyter notebook. Under the Jobs tab on the FloydHub website, click on the Jupyter Notebook link and navigate to this file:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>floydhub/Alpha version/working_floyd_pink_light_full.ipynb</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Open it and click Shift+Enter on all the cells.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Gradually increase the epoch value to get a feel for how the neural network learns.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>model.fit(x=X, y=Y, batch_size=1, epochs=1)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Start with an epoch value of 1 and the increase it to 10, 100, 500, 1000 and 3000. The epoch value indicates how many times the neural network learns from the image. You will find the image <code>img_result.png</code>in the main folder once you’ve trained your neural network.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span># Get images<br/>image = img_to_array(load_img('woman.png'))<br/>image = np.array(image, dtype=float)</span><span># Import map images into the lab colorspace<br/>X = rgb2lab(1.0/255*image)[:,:,0]<br/>Y = rgb2lab(1.0/255*image)[:,:,1:]<br/>Y = Y / 128<br/>X = X.reshape(1, 400, 400, 1)<br/>Y = Y.reshape(1, 400, 400, 2)</span><span># Building the neural network<br/>model = Sequential()<br/>model.add(InputLayer(input_shape=(None, None, 1)))<br/>model.add(Conv2D(8, (3, 3), activation='relu', padding='same', strides=2))<br/>model.add(Conv2D(8, (3, 3), activation='relu', padding='same'))<br/>model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))<br/>model.add(Conv2D(16, (3, 3), activation='relu', padding='same', strides=2))<br/>model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))<br/>model.add(Conv2D(32, (3, 3), activation='relu', padding='same', strides=2))<br/>model.add(UpSampling2D((2, 2)))<br/>model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))<br/>model.add(UpSampling2D((2, 2)))<br/>model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))<br/>model.add(UpSampling2D((2, 2)))<br/>model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))</span><span># Finish model<br/>model.compile(optimizer='rmsprop',loss='mse')</span><span>#Train the neural network<br/>model.fit(x=X, y=Y, batch_size=1, epochs=3000)<br/>print(model.evaluate(X, Y, batch_size=1))</span><span># Output colorizations<br/>output = model.predict(X)<br/>output = output * 128<br/>canvas = np.zeros((400, 400, 3))<br/>canvas[:,:,0] = X[0][:,:,0]<br/>canvas[:,:,1:] = output[0]<br/>imsave("img_result.png", lab2rgb(canvas))<br/>imsave("img_gray_scale.png", rgb2gray(lab2rgb(canvas)))</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">FloydHub command to run this network:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>floyd run --data emilwallner/datasets/colornet/2:data --mode jupyter --tensorboard</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Technical explanation</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To recap, the input is a grid representing a black and white image. It outputs two grids with color values. Between the input and output values, we create filters to link them together. This is a convolutional neural network.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">When we train the network, we use colored images. We convert RGB colors to the Lab color space. The black and white layer is our input and the two colored layers are the output.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:88%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 650px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*arOhHOAQLE_SFv3D.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*arOhHOAQLE_SFv3D.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*arOhHOAQLE_SFv3D.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*arOhHOAQLE_SFv3D.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*arOhHOAQLE_SFv3D.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*arOhHOAQLE_SFv3D.png 1100w, https://miro.medium.com/v2/resize:fit:1300/format:webp/0*arOhHOAQLE_SFv3D.png 1300w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 650px" srcset="https://miro.medium.com/v2/resize:fit:640/0*arOhHOAQLE_SFv3D.png 640w, https://miro.medium.com/v2/resize:fit:720/0*arOhHOAQLE_SFv3D.png 720w, https://miro.medium.com/v2/resize:fit:750/0*arOhHOAQLE_SFv3D.png 750w, https://miro.medium.com/v2/resize:fit:786/0*arOhHOAQLE_SFv3D.png 786w, https://miro.medium.com/v2/resize:fit:828/0*arOhHOAQLE_SFv3D.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*arOhHOAQLE_SFv3D.png 1100w, https://miro.medium.com/v2/resize:fit:1300/0*arOhHOAQLE_SFv3D.png 1300w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/650/0*arOhHOAQLE_SFv3D.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To the left side, we have the B&W input, our filters, and the prediction from our neural network.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We map the predicted values and the real values within the same interval. This way, we can compare the values. The interval ranges from -1 to 1. To map the predicted values, we use a tanh activation function. For any value you give the <a href="http://mathworld.wolfram.com/HyperbolicTangent.html" target="_self">tanh function</a>, it will return -1 to 1.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The true color values range between -128 and 128. This is the default interval in the Lab color space. By dividing them by 128, they too fall within the -1 to 1 interval. This “normalization” enables us to compare the error from our prediction.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After calculating the final error, the network updates the filters to reduce the total error. The network continues in this loop until the error is as low as possible.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Let’s clarify some syntax in the code snippet.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>X = rgb2lab(1.0/255*image)[:,:,0]<br/>Y = rgb2lab(1.0/255*image)[:,:,1:]</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>1.0/255</strong> indicates that we are using a 24-bit RGB color space. It means that we are using numbers between 0–255 for each color channel. This results in 16.7 million color combinations.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Since humans can only perceive 2–10 million colors, it does not make much sense to use a larger color space.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>Y = Y / 128</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The Lab color space has a different range in comparison to RGB. The color spectrum <strong>ab</strong> in Lab ranges from -128 to 128. By dividing all values in the output layer by 128, we bound the range between -1 and 1.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We match it with our neural network, which also returns values between -1 and 1.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After converting the color space using the function <code>rgb2lab()</code> we select the grayscale layer with: <code>[ : , : , 0].</code> This is our input for the neural network. <code>[ : , : , 1: ]</code> selects the two color layers, green–red and blue–yellow.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After training the neural network, we make a final prediction which we convert into a picture.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>output = model.predict(X)<br/>output = output * 128</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here, we use a grayscale image as input and run it through our trained neural network. We take all the output values between -1 and 1 and multiply it by 128. This gives us the correct color in the Lab color spectrum.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>canvas = np.zeros((400, 400, 3))<br/>canvas[:,:,0] = X[0][:,:,0]<br/>canvas[:,:,1:] = output[0]</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Lastly, we create a black RGB canvas by filling it with three layers of 0s. Then we copy the grayscale layer from our test image. Then we add our two color layers to the RGB canvas. This array of pixel values is then converted into a picture.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Takeaways from the Alpha version</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><strong>Reading research papers is challenging</strong>. Once I summarized the core characteristics of each paper, it became easier to skim papers. It also allowed me to put the details into a context.</li><li class="ff3" style="font-size:22px;"><strong>Starting simple is key</strong>. Most of the implementations I could find online were 2–10K lines long. That made it hard to get an overview of the core logic of the problem. Once I had a barebones version, it became easier to read both the code implementation, and also the research papers.</li><li class="ff3" style="font-size:22px;"><strong>Explore public projects.</strong> To get a rough idea for what to code, I skimmed 50–100 projects on colorization on Github.</li><li class="ff3" style="font-size:22px;"><strong>Things won’t always work as expected.</strong> In the beginning, it could only create red and yellow colors. At first, I had a Relu activation function for the final activation. Since it only maps numbers into positive digits, it could not create negative values, the blue and green spectrums. Adding a tanh activation function and mapping the Y values fixed this.</li><li class="ff3" style="font-size:22px;"><strong>Understanding &gt; Speed.</strong> Many of the implementations I saw were fast but hard to work with. I chose to optimize for innovation speed instead of code speed.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Beta version</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To understand the weakness of the alpha version, try coloring an image it has not been trained on. If you try it, you’ll see that it makes a poor attempt. It’s because the network has memorized the information. It has not learned how to color an image it hasn’t seen before. But this is what we’ll do in the beta version. We’ll teach our network to generalize.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Below is the result of coloring the validation images with our beta version.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Instead of using Imagenet, I created <a href="https://www.floydhub.com/emilwallner/datasets/colornet" target="_self">a public dataset on FloydHub</a> with higher quality images. The images are from <a href="https://unsplash.com/" target="_self">Unsplash</a> — creative commons pictures by professional photographers. It includes 9,500 training images and 500 validation images.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*M-R9SWSz1UHlbOoZUpSVIA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*M-R9SWSz1UHlbOoZUpSVIA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*M-R9SWSz1UHlbOoZUpSVIA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*M-R9SWSz1UHlbOoZUpSVIA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*M-R9SWSz1UHlbOoZUpSVIA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*M-R9SWSz1UHlbOoZUpSVIA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M-R9SWSz1UHlbOoZUpSVIA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*M-R9SWSz1UHlbOoZUpSVIA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*M-R9SWSz1UHlbOoZUpSVIA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*M-R9SWSz1UHlbOoZUpSVIA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*M-R9SWSz1UHlbOoZUpSVIA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*M-R9SWSz1UHlbOoZUpSVIA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*M-R9SWSz1UHlbOoZUpSVIA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*M-R9SWSz1UHlbOoZUpSVIA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*M-R9SWSz1UHlbOoZUpSVIA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">The feature extractor</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Our neural network finds characteristics that link grayscale images with their colored versions.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Imagine you had to color black and white images — but with restriction that you can only see nine pixels at a time. You could scan each image from the top left to bottom right and try to predict which color each pixel should be.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*l6cKjgG6wPG5FfX4xJSv0w.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*l6cKjgG6wPG5FfX4xJSv0w.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*l6cKjgG6wPG5FfX4xJSv0w.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*l6cKjgG6wPG5FfX4xJSv0w.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*l6cKjgG6wPG5FfX4xJSv0w.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*l6cKjgG6wPG5FfX4xJSv0w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l6cKjgG6wPG5FfX4xJSv0w.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*l6cKjgG6wPG5FfX4xJSv0w.png 640w, https://miro.medium.com/v2/resize:fit:720/1*l6cKjgG6wPG5FfX4xJSv0w.png 720w, https://miro.medium.com/v2/resize:fit:750/1*l6cKjgG6wPG5FfX4xJSv0w.png 750w, https://miro.medium.com/v2/resize:fit:786/1*l6cKjgG6wPG5FfX4xJSv0w.png 786w, https://miro.medium.com/v2/resize:fit:828/1*l6cKjgG6wPG5FfX4xJSv0w.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*l6cKjgG6wPG5FfX4xJSv0w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*l6cKjgG6wPG5FfX4xJSv0w.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*l6cKjgG6wPG5FfX4xJSv0w.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For example, these nine pixels are the edge of the nostril from the woman just above. As you can imagine, it’d be next to impossible to make a good colorization, so you break it down into steps.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">First, you look for simple patterns: a diagonal line, all black pixels, and so on. You look for the same exact pattern in each square and remove the pixels that don’t match. You generate 64 new images from your 64 mini filters.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*M0Rk1C0ziWhkhNzONGZfYg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*M0Rk1C0ziWhkhNzONGZfYg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*M0Rk1C0ziWhkhNzONGZfYg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*M0Rk1C0ziWhkhNzONGZfYg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*M0Rk1C0ziWhkhNzONGZfYg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*M0Rk1C0ziWhkhNzONGZfYg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M0Rk1C0ziWhkhNzONGZfYg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*M0Rk1C0ziWhkhNzONGZfYg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*M0Rk1C0ziWhkhNzONGZfYg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*M0Rk1C0ziWhkhNzONGZfYg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*M0Rk1C0ziWhkhNzONGZfYg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*M0Rk1C0ziWhkhNzONGZfYg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*M0Rk1C0ziWhkhNzONGZfYg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*M0Rk1C0ziWhkhNzONGZfYg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*M0Rk1C0ziWhkhNzONGZfYg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">The number of filtered images for each step</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you scan the images again, you’d see the same small patterns you’ve already detected. To gain a higher level understanding of the image, you decrease the image size in half.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*xEj4c-CGzXe2Zh3BM1IdZQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*xEj4c-CGzXe2Zh3BM1IdZQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*xEj4c-CGzXe2Zh3BM1IdZQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*xEj4c-CGzXe2Zh3BM1IdZQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*xEj4c-CGzXe2Zh3BM1IdZQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*xEj4c-CGzXe2Zh3BM1IdZQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xEj4c-CGzXe2Zh3BM1IdZQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*xEj4c-CGzXe2Zh3BM1IdZQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*xEj4c-CGzXe2Zh3BM1IdZQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*xEj4c-CGzXe2Zh3BM1IdZQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*xEj4c-CGzXe2Zh3BM1IdZQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*xEj4c-CGzXe2Zh3BM1IdZQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*xEj4c-CGzXe2Zh3BM1IdZQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*xEj4c-CGzXe2Zh3BM1IdZQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*xEj4c-CGzXe2Zh3BM1IdZQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">We decrease the size in three steps</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">You still only have a 3x3 filter to scan each image. But by combining your new nine pixels with your lower level filters, you can detect more complex patterns. One pixel combination might form a half circle, a small dot, or a line. Again, you repeatedly extract the same pattern from the image. This time, you generate 128 new filtered images.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After a couple of steps the filtered images you produce might look something like these:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*h2A5tZSPK6HUSfXK2BNK9Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*h2A5tZSPK6HUSfXK2BNK9Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*h2A5tZSPK6HUSfXK2BNK9Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*h2A5tZSPK6HUSfXK2BNK9Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*h2A5tZSPK6HUSfXK2BNK9Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*h2A5tZSPK6HUSfXK2BNK9Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h2A5tZSPK6HUSfXK2BNK9Q.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*h2A5tZSPK6HUSfXK2BNK9Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*h2A5tZSPK6HUSfXK2BNK9Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*h2A5tZSPK6HUSfXK2BNK9Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*h2A5tZSPK6HUSfXK2BNK9Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*h2A5tZSPK6HUSfXK2BNK9Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*h2A5tZSPK6HUSfXK2BNK9Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*h2A5tZSPK6HUSfXK2BNK9Q.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*h2A5tZSPK6HUSfXK2BNK9Q.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">From Keras layer tutorial</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">As mentioned, you start with low-level features, such as an edge. Layers closer to the output are combined into patterns. Then, they are combined into details, and eventually transformed into a face. This <a href="https://www.youtube.com/watch?v=AgkfIQ4IGaM" target="_self">video tutorial</a> provides a further explanation.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The process is similar to that of most neural networks that deal with vision. The type of network here is known as a convolutional neural network. In these networks, you combine several filtered images to understand the context in the image.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">From feature extraction to color</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The neural network operates in a trial and error manner. It first makes a random prediction for each pixel. Based on the error for each pixel, it works backward through the network to improve the feature extraction.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">It starts adjusting for the situations that generate the largest errors. In this case, the adjustments are: whether to color or not, and how to locate different objects.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The network starts by coloring all the objects brown. It’s the color that is most similar to all other colors, thus producing the smallest error.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Because most of the training data is quite similar, the network struggles to differentiate between different objects. It will fail to generate more nuanced colors. That’s what we’ll explore in the full version.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Below is the code for the beta version, followed by a technical explanation of the code.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span># Get images<br/>X = []<br/>for filename in os.listdir('../Train/'):<br/>    X.append(img_to_array(load_img('../Train/'+filename)))<br/>X = np.array(X, dtype=float)</span><span># Set up training and test data<br/>split = int(0.95*len(X))<br/>Xtrain = X[:split]<br/>Xtrain = 1.0/255*Xtrain</span><span>#Design the neural network<br/>model = Sequential()<br/>model.add(InputLayer(input_shape=(256, 256, 1)))<br/>model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))<br/>model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2))<br/>model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))<br/>model.add(Conv2D(128, (3, 3), activation='relu', padding='same', strides=2))<br/>model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))<br/>model.add(Conv2D(256, (3, 3), activation='relu', padding='same', strides=2))<br/>model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))<br/>model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))<br/>model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))<br/>model.add(UpSampling2D((2, 2)))<br/>model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))<br/>model.add(UpSampling2D((2, 2)))<br/>model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))<br/>model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))<br/>model.add(UpSampling2D((2, 2)))</span><span># Finish model<br/>model.compile(optimizer='rmsprop', loss='mse')</span><span># Image transformer<br/>datagen = ImageDataGenerator(<br/>        shear_range=0.2,<br/>        zoom_range=0.2,<br/>        rotation_range=20,<br/>        horizontal_flip=True)</span><span># Generate training data<br/>batch_size = 50<br/>def image_a_b_gen(batch_size):<br/>    for batch in datagen.flow(Xtrain, batch_size=batch_size):<br/>        lab_batch = rgb2lab(batch)<br/>        X_batch = lab_batch[:,:,:,0]<br/>        Y_batch = lab_batch[:,:,:,1:] / 128<br/>        yield (X_batch.reshape(X_batch.shape+(1,)), Y_batch)</span><span># Train model<br/>TensorBoard(log_dir='/output')<br/>model.fit_generator(image_a_b_gen(batch_size), steps_per_epoch=10000, epochs=1)<br/># Test images<br/>Xtest = rgb2lab(1.0/255*X[split:])[:,:,:,0]<br/>Xtest = Xtest.reshape(Xtest.shape+(1,))<br/>Ytest = rgb2lab(1.0/255*X[split:])[:,:,:,1:]<br/>Ytest = Ytest / 128<br/>print model.evaluate(Xtest, Ytest, batch_size=batch_size)</span><span># Load black and white images<br/>color_me = []<br/>for filename in os.listdir('../Test/'):<br/>        color_me.append(img_to_array(load_img('../Test/'+filename)))<br/>color_me = np.array(color_me, dtype=float)<br/>color_me = rgb2lab(1.0/255*color_me)[:,:,:,0]<br/>color_me = color_me.reshape(color_me.shape+(1,))</span><span># Test model<br/>output = model.predict(color_me)<br/>output = output * 128</span><span># Output colorizations<br/>for i in range(len(output)):<br/>        cur = np.zeros((256, 256, 3))<br/>        cur[:,:,0] = color_me[i][:,:,0]<br/>        cur[:,:,1:] = output[i]<br/>        imsave("result/img_"+str(i)+".png", lab2rgb(cur))</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here’s the FloydHub command to run the Beta neural network:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>floyd run --data emilwallner/datasets/colornet/2:data --mode jupyter --tensorboard</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Technical explanation</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The main difference from other visual neural networks is the importance of pixel location. In coloring networks, the image size or ratio stays the same throughout the network. In other types of network, the image gets distorted the closer it gets to the final layer.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The max-pooling layers in classification networks increase the information density, but also distort the image. It only values the information, but not the layout of an image. In coloring networks we instead use a stride of 2, to decrease the width and height by half. This also increases information density but does not distort the image.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*b_lKunpTECvQBPMUxi5hWA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*b_lKunpTECvQBPMUxi5hWA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*b_lKunpTECvQBPMUxi5hWA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*b_lKunpTECvQBPMUxi5hWA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*b_lKunpTECvQBPMUxi5hWA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*b_lKunpTECvQBPMUxi5hWA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b_lKunpTECvQBPMUxi5hWA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*b_lKunpTECvQBPMUxi5hWA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*b_lKunpTECvQBPMUxi5hWA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*b_lKunpTECvQBPMUxi5hWA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*b_lKunpTECvQBPMUxi5hWA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*b_lKunpTECvQBPMUxi5hWA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*b_lKunpTECvQBPMUxi5hWA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*b_lKunpTECvQBPMUxi5hWA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*b_lKunpTECvQBPMUxi5hWA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Two further differences are: upsampling layers and maintaining the image ratio. Classification networks only care about the final classification. Therefore, they keep decreasing the image size and quality as it moves through the network.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Coloring networks keep the image ratio constant. This is done by adding white padding like the visualization above. Otherwise, each convolutional layer cuts the images. It’s done with the <code>*padding='same'*</code>parameter.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To double the size of the image, the coloring network uses <a href="https://keras.io/layers/convolutional/#upsampling2d" target="_self">an upsampling layer</a>.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>for filename in os.listdir('/Color_300/Train/'):<br/>    X.append(img_to_array(load_img('/Color_300/Test'+filename)))</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This for-loop first counts all the file names in the directory. Then, it iterates through the image directory and converts the images into an array of pixels. Finally, it combines them into a giant vector.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>datagen = ImageDataGenerator(<br/>        shear_range=0.2,<br/>        zoom_range=0.2,<br/>        rotation_range=20,<br/>        horizontal_flip=True)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">With <code>ImageDataGenerator</code>, we adjust the setting for our image generator. This way, each image will never be the same, thus improving the learning rate. The <code>shear_range</code>tilts the image to the left or right, and the other settings are zoom, rotation and horizontal-flip.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>batch_size = 50<br/>def image_a_b_gen(batch_size):<br/>    for batch in datagen.flow(Xtrain, batch_size=batch_size):<br/>        lab_batch = rgb2lab(batch)<br/>        X_batch = lab_batch[:,:,:,0]<br/>        Y_batch = lab_batch[:,:,:,1:] / 128<br/>        yield (X_batch.reshape(X_batch.shape+(1,)), Y_batch)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We use the images from our folder, Xtrain, to generate images based on the settings above. Then, we extract the black and white layer for the <code>X_batch</code> and the two colors for the two color layers.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>model.fit_generator(image_a_b_gen(batch_size), steps_per_epoch=1, epochs=1000)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The stronger the GPU you have, the more images you can fit into it. With this setup, you can use 50–100 images. <code>steps_per_epoch</code> is calculated by dividing the number of training images with your batch size.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For example: 100 images with a batch size of 50 gives 2 steps per epoch. The number of epochs determines how many times you want to train all images. 10K images with 21 epochs will take about 11 hours on a Tesla K80 GPU.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Takeaways</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><strong>Run a lot of experiments in smaller batches before you make larger runs.</strong> Even after 20–30 experiments, I still found mistakes. Just because it’s running doesn’t mean it’s working. Bugs in a neural network are often more nuanced than traditional programming errors. One of the more bizarre ones was <a href="https://twitter.com/EmilWallner/status/916309564966006784" target="_self">my Adam hiccup</a>.</li><li class="ff3" style="font-size:22px;"><strong>A more diverse dataset makes the pictures brownish.</strong> If you have <a href="https://github.com/2014mchidamb/DeepColorization/tree/master/face_images" target="_self">very similar images</a>, you can get a decent result without needing a more complex architecture. The trade-off is the network becomes worse at generalizing.</li><li class="ff3" style="font-size:22px;"><strong>Shapes, shapes, and shapes.</strong> The size of each image has to be exact and remain proportional throughout the network. In the beginning, I used an image size of 300. Halving this three times gives sizes of 150, 75, and 35.5. The result is losing half a pixel! This led to many “hacks” until I realized it’s better to use a power of two: 2, 8, 16, 32, 64, 256 and so on.</li><li class="ff3" style="font-size:22px;"><strong>Creating datasets: </strong>a)<strong> </strong><a href="http://osxdaily.com/2010/02/03/how-to-prevent-ds_store-file-creation/" target="_self">Disable</a> the .DS_Store file, it drove me crazy. b) Be creative. I ended up with a Chrome <a href="https://github.com/emilwallner/useful-scripts/blob/master/auto_scroll_browser_window_console" target="_self">console script</a> and <a href="https://chrome.google.com/webstore/detail/imagespark-ultimate-image/hooaoionkjogngfhjjniefmenehnopag" target="_self">an extension</a> to download the files. c) Make a copy of the raw files you scrape and structure your <a href="https://github.com/emilwallner/useful-scripts" target="_self">cleaning scripts</a>.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Full-version</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Our final version of the colorization neural network has four components. We split the network we had before into an encoder and a decoder. Between them, we’ll use a fusion layer. If you are new to classification networks, I’d recommend having a look at <a href="http://cs231n.github.io/classification/" target="_self">this tutorial</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In parallel to the encoder, the input images also run through one of today’s most powerful classifiers — the <a href="https://research.googleblog.com/2016/08/improving-inception-and-image.html" target="_self">Inception ResNet v2 </a>. This is a neural network trained on 1.2M images. We extract the classification layer and merge it with the output from the encoder.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*KRXxAAxlBz1psRvB1ak04Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*KRXxAAxlBz1psRvB1ak04Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*KRXxAAxlBz1psRvB1ak04Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*KRXxAAxlBz1psRvB1ak04Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*KRXxAAxlBz1psRvB1ak04Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*KRXxAAxlBz1psRvB1ak04Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KRXxAAxlBz1psRvB1ak04Q.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*KRXxAAxlBz1psRvB1ak04Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*KRXxAAxlBz1psRvB1ak04Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*KRXxAAxlBz1psRvB1ak04Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*KRXxAAxlBz1psRvB1ak04Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*KRXxAAxlBz1psRvB1ak04Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*KRXxAAxlBz1psRvB1ak04Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*KRXxAAxlBz1psRvB1ak04Q.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*KRXxAAxlBz1psRvB1ak04Q.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here is a more <a href="https://github.com/baldassarreFe/deep-koalarization" target="_self">detailed visual</a> from the original paper.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">By transferring the learning from the classifier to the coloring network, the network can get a sense of what’s in the picture. Thus, enabling the network to match an object representation with a coloring scheme.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here are some of the validation images, using only 20 images to train the network on.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*frA53gIp67fljqJ_yQy7Yg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*frA53gIp67fljqJ_yQy7Yg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*frA53gIp67fljqJ_yQy7Yg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*frA53gIp67fljqJ_yQy7Yg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*frA53gIp67fljqJ_yQy7Yg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*frA53gIp67fljqJ_yQy7Yg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*frA53gIp67fljqJ_yQy7Yg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*frA53gIp67fljqJ_yQy7Yg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*frA53gIp67fljqJ_yQy7Yg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*frA53gIp67fljqJ_yQy7Yg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*frA53gIp67fljqJ_yQy7Yg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*frA53gIp67fljqJ_yQy7Yg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*frA53gIp67fljqJ_yQy7Yg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*frA53gIp67fljqJ_yQy7Yg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*frA53gIp67fljqJ_yQy7Yg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Most of the images turned out poor. But I was able to find a few decent ones because of a large validation set (2,500 images). Training it on more images gave a more consistent result, but most of them turned out brownish. Here is a full list of the <a href="https://www.floydhub.com/emilwallner/projects/color" target="_self">experiments I ran</a> including the validation images.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here are the most common architectures from previous research, with links:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Manually adding small dots of color in a picture to guide the neural network (<a href="http://www.cs.huji.ac.il/~yweiss/Colorization/" target="_self">link</a>)</li><li class="ff3" style="font-size:22px;">Find a matching image and transfer the coloring (learn more <a href="https://dl.acm.org/citation.cfm?id=2393402" target="_self">here</a> and <a href="https://arxiv.org/abs/1505.05192" target="_self">here</a>)</li><li class="ff3" style="font-size:22px;">Residual encoder and merging classification layers (<a href="http://tinyclouds.org/colorize/" target="_self">link</a>)</li><li class="ff3" style="font-size:22px;">Merging hypercolumns from a classifying network (more detail <a href="https://arxiv.org/pdf/1603.08511.pdf" target="_self">here</a> and <a href="https://arxiv.org/pdf/1603.06668.pdf" target="_self">here</a>)</li><li class="ff3" style="font-size:22px;">Merging the final classification between the encoder and decoder (details <a href="http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/data/colorization_sig2016.pdf" target="_self">here </a>and <a href="https://arxiv.org/abs/1712.03400" target="_self">here</a>)</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Colorspaces: </strong>Lab, YUV, HSV, and LUV (more detail <a href="http://cs231n.stanford.edu/reports/2016/pdfs/219_Report.pdf" target="_self">here</a> and <a href="https://arxiv.org/abs/1605.00075" target="_self">here</a>)</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Loss:</strong> Mean square error, classification, weighted classification (<a href="https://arxiv.org/pdf/1603.06668.pdf" target="_self">link</a>)</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I chose the ‘fusion layer’ architecture (the fifth one in the list above).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This was because it produces some of the best results. It is also easier to understand and reproduce in <a href="https://keras.io/" target="_self">Keras</a>. Although it’s not the strongest colorization network design, it is a good place to start. It’s a great architecture to understand the dynamics of the coloring problem.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I used the neural network design from <a href="https://arxiv.org/abs/1712.03400" target="_self">this paper</a> by Federico Baldassarre and collaborators. I proceeded with my own interpretation in Keras.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Note: in the below code I switch from Keras’ sequential model to their functional API. [<a href="https://keras.io/getting-started/functional-api-guide/" target="_self">Documentation</a>]</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span># Get images<br/>X = []<br/>for filename in os.listdir('/data/images/Train/'):<br/>    X.append(img_to_array(load_img('/data/images/Train/'+filename)))<br/>X = np.array(X, dtype=float)<br/>Xtrain = 1.0/255*X</span><span>#Load weights<br/>inception = InceptionResNetV2(weights=None, include_top=True)<br/>inception.load_weights('/data/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5')<br/>inception.graph = tf.get_default_graph()</span><span>embed_input = Input(shape=(1000,))</span><span>#Encoder<br/>encoder_input = Input(shape=(256, 256, 1,))<br/>encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)<br/>encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)<br/>encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output)<br/>encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)<br/>encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_output)<br/>encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)<br/>encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)<br/>encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)</span><span>#Fusion<br/>fusion_output = RepeatVector(32 * 32)(embed_input) <br/>fusion_output = Reshape(([32, 32, 1000]))(fusion_output)<br/>fusion_output = concatenate([encoder_output, fusion_output], axis=3) <br/>fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output)</span><span>#Decoder<br/>decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)<br/>decoder_output = UpSampling2D((2, 2))(decoder_output)<br/>decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)<br/>decoder_output = UpSampling2D((2, 2))(decoder_output)<br/>decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)<br/>decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)<br/>decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)<br/>decoder_output = UpSampling2D((2, 2))(decoder_output)</span><span>model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)</span><span>#Create embedding<br/>def create_inception_embedding(grayscaled_rgb):<br/>    grayscaled_rgb_resized = []<br/>    for i in grayscaled_rgb:<br/>        i = resize(i, (299, 299, 3), mode='constant')<br/>        grayscaled_rgb_resized.append(i)<br/>    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)<br/>    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)<br/>    with inception.graph.as_default():<br/>        embed = inception.predict(grayscaled_rgb_resized)<br/>    return embed</span><span># Image transformer<br/>datagen = ImageDataGenerator(<br/>        shear_range=0.4,<br/>        zoom_range=0.4,<br/>        rotation_range=40,<br/>        horizontal_flip=True)</span><span>#Generate training data<br/>batch_size = 20</span><span>def image_a_b_gen(batch_size):<br/>    for batch in datagen.flow(Xtrain, batch_size=batch_size):<br/>        grayscaled_rgb = gray2rgb(rgb2gray(batch))<br/>        embed = create_inception_embedding(grayscaled_rgb)<br/>        lab_batch = rgb2lab(batch)<br/>        X_batch = lab_batch[:,:,:,0]<br/>        X_batch = X_batch.reshape(X_batch.shape+(1,))<br/>        Y_batch = lab_batch[:,:,:,1:] / 128<br/>        yield ([X_batch, create_inception_embedding(grayscaled_rgb)], Y_batch)</span><span>#Train model      <br/>tensorboard = TensorBoard(log_dir="/output")<br/>model.compile(optimizer='adam', loss='mse')<br/>model.fit_generator(image_a_b_gen(batch_size), callbacks=[tensorboard], epochs=1000, steps_per_epoch=20)</span><span>#Make a prediction on the unseen images<br/>color_me = []<br/>for filename in os.listdir('../Test/'):<br/>    color_me.append(img_to_array(load_img('../Test/'+filename)))<br/>color_me = np.array(color_me, dtype=float)<br/>color_me = 1.0/255*color_me<br/>color_me = gray2rgb(rgb2gray(color_me))<br/>color_me_embed = create_inception_embedding(color_me)<br/>color_me = rgb2lab(color_me)[:,:,:,0]<br/>color_me = color_me.reshape(color_me.shape+(1,))</span><span># Test model<br/>output = model.predict([color_me, color_me_embed])<br/>output = output * 128</span><span># Output colorizations<br/>for i in range(len(output)):<br/>    cur = np.zeros((256, 256, 3))<br/>    cur[:,:,0] = color_me[i][:,:,0]<br/>    cur[:,:,1:] = output[i]<br/>    imsave("result/img_"+str(i)+".png", lab2rgb(cur))</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here’s the FloydHub command to run the full neural network:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>floyd run --data emilwallner/datasets/colornet/2:data --mode jupyter --tensorboard</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Technical Explanation</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><a href="https://keras.io/getting-started/functional-api-guide/" target="_self">Keras’ functional API</a> is ideal when we are concatenating or merging several models.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:88%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 650px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*naXHqRyIdBtCE7Ty.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*naXHqRyIdBtCE7Ty.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*naXHqRyIdBtCE7Ty.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*naXHqRyIdBtCE7Ty.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*naXHqRyIdBtCE7Ty.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*naXHqRyIdBtCE7Ty.png 1100w, https://miro.medium.com/v2/resize:fit:1300/format:webp/0*naXHqRyIdBtCE7Ty.png 1300w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 650px" srcset="https://miro.medium.com/v2/resize:fit:640/0*naXHqRyIdBtCE7Ty.png 640w, https://miro.medium.com/v2/resize:fit:720/0*naXHqRyIdBtCE7Ty.png 720w, https://miro.medium.com/v2/resize:fit:750/0*naXHqRyIdBtCE7Ty.png 750w, https://miro.medium.com/v2/resize:fit:786/0*naXHqRyIdBtCE7Ty.png 786w, https://miro.medium.com/v2/resize:fit:828/0*naXHqRyIdBtCE7Ty.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*naXHqRyIdBtCE7Ty.png 1100w, https://miro.medium.com/v2/resize:fit:1300/0*naXHqRyIdBtCE7Ty.png 1300w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/650/0*naXHqRyIdBtCE7Ty.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">First, we download the <a href="https://research.googleblog.com/2016/08/improving-inception-and-image.html" target="_self">Inception ResNet v2</a> neural network and load the weights. Since we will be using two models in parallel, we need to specify which model we are using. This is done in <a href="https://www.tensorflow.org/" target="_self">Tensorflow</a>, the backend for Keras.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>inception = InceptionResNetV2(weights=None, include_top=True)<br/>inception.load_weights('/data/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5')<br/>inception.graph = tf.get_default_graph()</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To create our batch, we use the tweaked images. We conver them to black and white and run them through the Inception ResNet model.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>grayscaled_rgb = gray2rgb(rgb2gray(batch))<br/>embed = create_inception_embedding(grayscaled_rgb)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">First, we have to resize the image to fit into the Inception model. Then we use the preprocessor to format the pixel and color values according to the model. In the final step, we run it through the Inception network and extract the final layer of the model.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>def create_inception_embedding(grayscaled_rgb):<br/>    grayscaled_rgb_resized = []<br/>    for i in grayscaled_rgb:<br/>        i = resize(i, (299, 299, 3), mode='constant')<br/>        grayscaled_rgb_resized.append(i)<br/>    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)<br/>    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)<br/>    with inception.graph.as_default():<br/>        embed = inception.predict(grayscaled_rgb_resized)<br/>    return embed</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Let’s go back to the generator. For each batch, we generate 20 images in the below format. It takes about an hour on a Tesla K80 GPU. It can do up to 50 images at a time with this model without having memory problems.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>yield ([X_batch, create_inception_embedding(grayscaled_rgb)], Y_batch)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This matches with our colornet model format.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><code>encoder_input</code>is fed into our Encoder model, the output of the Encoder model is then fused with the <code>embed_input</code>in the fusion layer; the output of the fusion is then used as input in our Decoder model, which then returns the final output, <code>decoder_output</code>.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>fusion_output = RepeatVector(32 * 32)(embed_input) <br/>fusion_output = Reshape(([32, 32, 1000]))(fusion_output)<br/>fusion_output = concatenate([fusion_output, encoder_output], axis=3) <br/>fusion_output = Conv2D(256, (1, 1), activation='relu')(fusion_output)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In the fusion layer, we first multiply the 1000 category layer by 1024 (32 * 32). This way, we get 1024 rows with the final layer from the Inception model.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This is then reshaped from 2D to 3D, a 32 x 32 grid with the 1000 category pillars. These are then linked together with the output from the encoder model. We apply a 254 filtered convolutional network with a 1X1 kernel, the final output of the fusion layer.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Takeaways</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><strong>The research terminology was daunting.</strong> I spent three days googling for ways to implement the “fusion model” in Keras. Because it sounded complex, I didn’t want to face the problem. Instead, I tricked myself into searching for short cuts.</li><li class="ff3" style="font-size:22px;"><strong>I asked questions online.</strong> I didn’t have a single comment in the Keras slack channel and Stack Overflow deleted my questions. But, by publicly breaking down the problem to make it simple to answer, it forced me to isolate the error, taking me closer to a solution.</li><li class="ff3" style="font-size:22px;"><strong>Email people.</strong> Although forums can be cold, people care if you connect with them directly. Discussing color spaces over Skype with a researcher is inspiring!</li><li class="ff3" style="font-size:22px;"><strong>After delaying on the fusion problem, I decided to build all the components before I stitched them together.</strong> Here are a <a href="https://www.floydhub.com/emilwallner/projects/color/24/code/Experiments/transfer-learning-examples" target="_self">few experiments</a> I used to break down the fusion layer.</li><li class="ff3" style="font-size:22px;"><strong>Once I had something I thought would work, I was hesitant to run it.</strong> Although I knew the core logic was okay, I didn’t believe it would work. After a cup of lemon tea and a long walk — I ran it. It produced an error after the first line in my model. But after four days, several hundred bugs and several thousand Google searches, “Epoch 1/22” appeared under my model.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Next steps</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Colorizing images is a deeply fascinating problem. It is as much as a scientific problem as artistic one. I wrote this article so you can get up to speed in coloring and continue where I left off. Here are some suggestions to get started:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Implement it with another pre-trained model</li><li class="ff3" style="font-size:22px;">Try a different dataset</li><li class="ff3" style="font-size:22px;">Increase the network’s accuracy by using more pictures</li><li class="ff3" style="font-size:22px;">Build an amplifier within the RGB color space. Create a similar model to the coloring network, that takes a saturated colored image as input and the correct colored image as output.</li><li class="ff3" style="font-size:22px;">Implement a weighted classification</li><li class="ff3" style="font-size:22px;">Apply it to video. Don’t worry too much about the colorization, but make the switch between images consistent. You could also do something similar for larger images, by tiling smaller ones.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>You can also easily colorize your own black and white images with my three versions of the colorization neural network using FloydHub.</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">For the alpha version, simply replace the <code>woman.jpg</code>file with your file with the same name (image size 400x400 pixels).</li><li class="ff3" style="font-size:22px;">For the beta and the full version, add your images to the <code>Test</code>folder before you run the FloydHub command. You can also upload them directly in the Notebook to the Test folder while the notebook is running. Note that these images need to be exactly 256x256 pixels. Also, you can upload all test images in color because it will automatically convert them into B&W.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you build something or get stuck, ping me on twitter: <a href="https://twitter.com/EmilWallner" target="_self">emilwallner</a>. I’d love to see what you are building.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Huge thanks to</strong> Federico Baldassarre, for answering my questions and their previous work on colorization. Also thanks to Muthu Chidambaram, who influenced the core implementation in Keras, and the Unsplash community for providing the pictures. Thanks also to Marine Haziza, Valdemaras Repsys, Qingping Hou, Charlie Harrington, Sai Soundararaj, Jannes Klaas, Claudio Cabral, Alain Demenet, and Ignacio Tonoli for reading drafts of this.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:80%;"><iframe  scrolling="auto" width="854.0" height="480.0" frameborder="0" src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FxKPk7tG2upc%3Ffeature%3Doembed&url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DxKPk7tG2upc&image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FxKPk7tG2upc%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=youtube" allowfullscreen=""></iframe></div><br><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">About Emil Wallner</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This the third part in a multi-part blog series from Emil as he learns deep learning. Emil has spent a decade exploring human learning. He’s worked for Oxford’s business school, invested in education startups, and built an education technology business. Last year, he enrolled at <a href="https://twitter.com/paulg/status/847844863727087616" target="_self">Ecole 42</a> to apply his knowledge of human learning to machine learning.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">You can follow along with Emil on <a href="https://twitter.com/EmilWallner" target="_self">Twitter</a> and <a href="https://medium.com/@emilwallner" target="_self">Medium</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This was first published as a community post on <a href="https://blog.floydhub.com/colorizing-b&w-photos-with-neural-networks/" target="_self">Floydhub’s</a> blog.</p></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>