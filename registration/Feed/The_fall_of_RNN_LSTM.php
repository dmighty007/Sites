<!DOCTYPE html>
                <html>
                <head>
                    <title>The fall of RNN / LSTM</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://culurciello.medium.com/?source=post_page-----2d1594c74ce0--------------------------------">Author : Eugenio Culurciello</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>The fall of RNN / LSTM</h3></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Fr-q6mQNYjJr6hTglTic7g.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Fr-q6mQNYjJr6hTglTic7g.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Fr-q6mQNYjJr6hTglTic7g.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Fr-q6mQNYjJr6hTglTic7g.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Fr-q6mQNYjJr6hTglTic7g.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Fr-q6mQNYjJr6hTglTic7g.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fr-q6mQNYjJr6hTglTic7g.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Fr-q6mQNYjJr6hTglTic7g.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*Fr-q6mQNYjJr6hTglTic7g.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*Fr-q6mQNYjJr6hTglTic7g.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*Fr-q6mQNYjJr6hTglTic7g.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*Fr-q6mQNYjJr6hTglTic7g.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*Fr-q6mQNYjJr6hTglTic7g.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*Fr-q6mQNYjJr6hTglTic7g.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*Fr-q6mQNYjJr6hTglTic7g.jpeg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We fell for Recurrent neural networks (RNN), Long-short term memory (LSTM), and all their variants. <strong>Now it is time to drop them!</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">It is the year 2014 and LSTM and RNN make a great come-back from the dead. We all read <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_self">Colah’s blog</a> and <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_self">Karpathy’s ode to RNN</a>. But we were all young and unexperienced. For a few years this was the way to solve sequence learning, sequence translation (seq2seq), which also resulted in amazing results in speech to text comprehension and the raise of <a href="http://www.businessinsider.com/apples-siri-using-neural-networks-2016-8" target="_self">Siri</a>, <a href="https://venturebeat.com/2014/04/03/behind-cortana-how-microsoft-aims-to-stand-out-with-its-personal-assistant/" target="_self">Cortana</a>, <a href="https://cloud.google.com/speech-to-text/" target="_self">Google voice assistant</a>, <a href="https://www.zdnet.com/article/amazon-echo-the-four-hard-problems-amazon-had-to-solve-to-make-it-work/" target="_self">Alexa</a>. Also let us not forget machine translation, which resulted in the ability to translate documents into different languages or <a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html" target="_self">neural machine translation</a>, but also translate <a href="https://arxiv.org/abs/1502.03044" target="_self">images into text</a>, <a href="https://arxiv.org/abs/1511.02793" target="_self">text into images</a>, and <a href="https://www.cs.utexas.edu/~vsub/naacl15_project.html" target="_self">captioning video</a>, and … well you got the idea.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Then in the following years (2015–16) came <a href="https://arxiv.org/abs/1512.03385" target="_self">ResNet</a> and <a href="https://arxiv.org/abs/1502.03044" target="_self">Attention</a>. One could then better understand that LSTM were a clever bypass technique. Also attention showed that MLP network could be replaced by <em>averaging</em> networks influenced by a <em>context vector</em>. More on this later.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">It only took 2 more years, but today we can definitely say:</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4">“Drop your RNN and LSTM, they are no good!”</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">But do not take our words for it, also see evidence that Attention based networks are used more and more by <a href="https://arxiv.org/abs/1706.03762" target="_self">Google</a>, <a href="https://code.facebook.com/posts/1978007565818999/a-novel-approach-to-neural-machine-translation/" target="_self">Facebook</a>, <a href="https://einstein.ai/research/non-autoregressive-neural-machine-translation" target="_self">Salesforce</a>, to name a few. All these companies have replaced RNN and variants for attention based models, and it is just the beginning. RNN have the days counted in all applications, because they require more resources to train and run than attention-based models. See <a href="https://medium.com/memory-attention-sequences-37456d271992" target="_self">this post</a> for more info.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">But why?</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Remember RNN and LSTM and derivatives use mainly sequential processing over time. See the horizontal arrow in the diagram below:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*NKhwsOYNUT5xU7Pyf6Znhg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*NKhwsOYNUT5xU7Pyf6Znhg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*NKhwsOYNUT5xU7Pyf6Znhg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*NKhwsOYNUT5xU7Pyf6Znhg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*NKhwsOYNUT5xU7Pyf6Znhg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*NKhwsOYNUT5xU7Pyf6Znhg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NKhwsOYNUT5xU7Pyf6Znhg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*NKhwsOYNUT5xU7Pyf6Znhg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*NKhwsOYNUT5xU7Pyf6Znhg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*NKhwsOYNUT5xU7Pyf6Znhg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*NKhwsOYNUT5xU7Pyf6Znhg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*NKhwsOYNUT5xU7Pyf6Znhg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*NKhwsOYNUT5xU7Pyf6Znhg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*NKhwsOYNUT5xU7Pyf6Znhg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*NKhwsOYNUT5xU7Pyf6Znhg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Sequential processing in RNN, from: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_self">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This arrow means that long-term information has to sequentially travel through all cells before getting to the present processing cell. This means it can be easily corrupted by being multiplied many time by small numbers &lt; 0. This is the cause of <a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/" target="_self">vanishing gradients</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To the rescue, came the LSTM module, which today can be seen as multiple switch gates, and a bit like <a href="https://arxiv.org/abs/1512.03385" target="_self">ResNet</a> it can bypass units and thus remember for longer time steps. LSTM thus have a way to remove some of the vanishing gradients problems.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*J5W8FrASMi93Z81NlAui4w.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*J5W8FrASMi93Z81NlAui4w.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*J5W8FrASMi93Z81NlAui4w.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*J5W8FrASMi93Z81NlAui4w.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*J5W8FrASMi93Z81NlAui4w.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*J5W8FrASMi93Z81NlAui4w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J5W8FrASMi93Z81NlAui4w.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*J5W8FrASMi93Z81NlAui4w.png 640w, https://miro.medium.com/v2/resize:fit:720/1*J5W8FrASMi93Z81NlAui4w.png 720w, https://miro.medium.com/v2/resize:fit:750/1*J5W8FrASMi93Z81NlAui4w.png 750w, https://miro.medium.com/v2/resize:fit:786/1*J5W8FrASMi93Z81NlAui4w.png 786w, https://miro.medium.com/v2/resize:fit:828/1*J5W8FrASMi93Z81NlAui4w.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*J5W8FrASMi93Z81NlAui4w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*J5W8FrASMi93Z81NlAui4w.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*J5W8FrASMi93Z81NlAui4w.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Sequential processing in LSTM, from: <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_self">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">But not all of it, as you can see from the figure above. Still we have a sequential path from older past cells to the current one. In fact the path is now even more complicated, because it has additive and forget branches attached to it. No question LSTM and GRU and derivatives are able to learn a lot of longer term information! See results <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_self">here</a>; but they can remember sequences of 100s, not 1000s or 10,000s or more.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">And one issue of RNN is that they are <a href="https://medium.com/@culurciello/computation-and-memory-bandwidth-in-deep-neural-networks-16cbac63ebd5" target="_self">not hardware friendly</a>. Let me explain: it takes a lot of resources we do not have to train these network fast. Also it takes much resources to run these model in the cloud, and given that the demand for speech-to-text is growing rapidly, the cloud is not scalable. We will need to process at the edge, right into the Amazon Echo! See note below for more details.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">What do you do?</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">At this time (September 2018) I would seriously consider this approach <a href="https://arxiv.org/abs/1808.03867" target="_self">here</a>. This is a 2D convolutional based neural network with causal convolution that can outperform both RNN/LSTM and Attention based models like the <a href="https://arxiv.org/abs/1706.03762" target="_self">Transformer</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The <a href="https://arxiv.org/abs/1706.03762" target="_self">Transformer</a> has definitely been a great suggestion from 2017 until the paper above. It has great advantages in training and in number of parameters, as we discussed <a href="https://medium.com/memory-attention-sequences-37456d271992" target="_self">here</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Alternatively:</strong> If sequential processing is to be avoided, then we can find units that “look-ahead” or better “look-back”, since most of the time we deal with real-time causal data where we know the past and want to affect future decisions. Not so in translating sentences, or analyzing recorded videos, for example, where we have all data and can reason on it more time. Such look-back/ahead units are neural attention modules, which we previously explained <a href="https://medium.com/@culurciello/neural-networks-building-blocks-a5c47bcd7c8d" target="_self">here</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To the rescue, and combining multiple neural attention modules, comes the “<em>hierarchical neural attention encoder</em>”, shown in the figure below:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*I5s9DOjKW3QTKaT0tHrNLg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*I5s9DOjKW3QTKaT0tHrNLg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*I5s9DOjKW3QTKaT0tHrNLg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*I5s9DOjKW3QTKaT0tHrNLg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*I5s9DOjKW3QTKaT0tHrNLg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*I5s9DOjKW3QTKaT0tHrNLg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I5s9DOjKW3QTKaT0tHrNLg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*I5s9DOjKW3QTKaT0tHrNLg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*I5s9DOjKW3QTKaT0tHrNLg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*I5s9DOjKW3QTKaT0tHrNLg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*I5s9DOjKW3QTKaT0tHrNLg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*I5s9DOjKW3QTKaT0tHrNLg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*I5s9DOjKW3QTKaT0tHrNLg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*I5s9DOjKW3QTKaT0tHrNLg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*I5s9DOjKW3QTKaT0tHrNLg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Hierarchical neural attention encoder</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">A better way to look into the past is to use attention modules to summarize all past encoded vectors into a context vector Ct.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Notice there is a hierarchy of attention modules here, very similar to the hierarchy of neural networks. This is also similar to <a href="https://arxiv.org/abs/1803.01271" target="_self">Temporal convolutional network (TCN)</a>, reported in Note 3 below.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In the <em>hierarchical neural attention encoder </em>multiple layers of attention can look at a small portion of recent past, say 100 vectors, while layers above can look at 100 of these attention modules, effectively integrating the information of 100 x 100 vectors. This extends the ability of the <em>hierarchical neural attention encoder</em> to 10,000 past vectors.</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4">This is the way to look back more into the past and be able to influence the future.</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">But more importantly look at the length of the path needed to propagate a representation vector to the output of the network: in hierarchical networks it is proportional to <em>log(N) </em>where N are the number of hierarchy layers. This is in contrast to the T steps that a RNN needs to do, where T is the maximum length of the sequence to be remembered, and T &gt;&gt; N.</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4">It is easier to remember sequences if you hop 3–4 times, as opposed to hopping 100 times!</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This architecture is similar to a <a href="https://arxiv.org/abs/1410.5401" target="_self">neural Turing machine</a>, but lets the neural network decide what is read out from memory via attention. This means an actual neural network will decide which vectors from the past are important for future decisions.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">But what about storing to memory? The architecture above stores all previous representation in memory, unlike neural Turning machines. This can be rather inefficient: think about storing the representation of every frame in a video — most times the representation vector does not change frame-to-frame, so we really are storing too much of the same! What can we do is add another unit to prevent correlated data to be stored. For example by not storing vectors too similar to previously stored ones. But this is really a hack, the best would be to be let the application guide what vectors should be saved or not. This is the focus of current research studies. Stay tuned for more information.</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4">So in summary forget RNN and variants. Use attention. <mark>Attention really is all you need!</mark></p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Tell your friends! </strong>It is very surprising to us to see so many companies still use RNN/LSTM for speech to text, many unaware that these networks are so inefficient and not scalable. Please tell them about this post.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5"><strong>Additional information</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>About training RNN/LSTM:</strong> RNN and LSTM are difficult to train because they require memory-bandwidth-bound computation, which is the worst nightmare for hardware designer and ultimately limits the applicability of neural networks solutions. In short, LSTM require 4 linear layer (MLP layer) per cell to run at and for each sequence time-step. Linear layers require large amounts of memory bandwidth to be computed, in fact they cannot use many compute unit often because the system has not enough memory bandwidth to feed the computational units. And it is easy to add more computational units, but hard to add more memory bandwidth (note enough lines on a chip, long wires from processors to memory, etc). As a result, RNN/LSTM and variants are not a good match for hardware acceleration, and we talked about this issue before <a href="https://medium.com/@culurciello/computation-and-memory-bandwidth-in-deep-neural-networks-16cbac63ebd5" target="_self">here</a> and <a href="https://medium.com/memory-attention-sequences-37456d271992" target="_self">here</a>. A solution will be compute in memory-devices like the ones we work on at <a href="http://fwdnxt.com/" target="_self">FWDNXT.</a></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Notes</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Note 1:</strong> <em>Hierarchical neural attention</em> is similar to the ideas in <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" target="_self">WaveNet</a>. But instead of a convolutional neural network we use hierarchical attention modules. Also: <em>Hierarchical neural attention</em> can be also bi-directional.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Note 2:</strong><em> </em>RNN and LSTM are memory-bandwidth limited problems (<a href="https://medium.com/@culurciello/computation-and-memory-bandwidth-in-deep-neural-networks-16cbac63ebd5" target="_self">see this for details</a>). The processing unit(s) need as much memory bandwidth as the number of operations/s they can provide, making it impossible to fully utilize them! The external bandwidth is never going to be enough, and a way to slightly ameliorate the problem is to use internal fast caches with high bandwidth. The best way is to use techniques that do not require large amount of parameters to be moved back and forth from memory, or that can be re-used for multiple computation per byte transferred (high arithmetic intensity).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Note 3</strong>: Here is a <a href="https://arxiv.org/abs/1803.01271" target="_self">paper comparing CNN to RNN</a>. Temporal convolutional network (TCN) “outperform canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory”.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Note 4</strong>: Related to this topic, is the fact that <a href="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004592" target="_self">we know little</a> of how our human brain learns and remembers sequences. “We often learn and recall long sequences in smaller segments, such as a phone number 858 534 22 30 memorized as four segments. Behavioral experiments suggest that humans and some animals employ this strategy of breaking down cognitive or behavioral sequences into chunks in a wide variety of tasks” — these chunks remind me of small convolutional or attention like networks on smaller sequences, that then are hierarchically strung together like in the <em>hierarchical neural attention encoder</em> and Temporal convolutional network (TCN). <a href="https://www.ncbi.nlm.nih.gov/pubmed/23541152" target="_self">More studies</a> make me think that working memory is similar to RNN networks that uses recurrent real neuron networks, and their capacity is very low. On the other hand both the cortex and <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3242327/" target="_self">hippocampus</a> give us the ability to remember really long sequences of steps (like: where did I park my car at airport 5 days ago), suggesting that more parallel pathways may be involved to recall long sequences, where attention mechanism gate important chunks and force hops in parts of the sequence that is not relevant to the final goal or task.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*hghzwOLy6IBZ7fTwkWuuVQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*hghzwOLy6IBZ7fTwkWuuVQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*hghzwOLy6IBZ7fTwkWuuVQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*hghzwOLy6IBZ7fTwkWuuVQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*hghzwOLy6IBZ7fTwkWuuVQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*hghzwOLy6IBZ7fTwkWuuVQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hghzwOLy6IBZ7fTwkWuuVQ.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*hghzwOLy6IBZ7fTwkWuuVQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*hghzwOLy6IBZ7fTwkWuuVQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*hghzwOLy6IBZ7fTwkWuuVQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*hghzwOLy6IBZ7fTwkWuuVQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*hghzwOLy6IBZ7fTwkWuuVQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*hghzwOLy6IBZ7fTwkWuuVQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*hghzwOLy6IBZ7fTwkWuuVQ.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*hghzwOLy6IBZ7fTwkWuuVQ.jpeg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Note 5:</strong> The above evidence shows we do not read sequentially, in fact we interpret characters, words and sentences as a group. An attention-based or convolutional module perceives the sequence and projects a representation in our mind. We would not be misreading this if we processed this information sequentially! We would stop and notice the inconsistencies!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Note 6:</strong> A <a href="https://blog.openai.com/language-unsupervised/" target="_self">recent paper</a> trained unsupervised with attention/transformer and showed amazing performance in transfer learning. The VGG of NLP? This works is also an extension of <a href="https://arxiv.org/abs/1801.06146" target="_self">pioneering work by Jeremy and Sebastian</a>, where an LSTM with ad-hoc training procedures was able to learn unsupervised to predict the next word in a sequence of text, and then also able to transfer that knowledge to new tasks. A comparison of the effectiveness of LSTM and Transformer (attention based) is given <a href="https://blog.openai.com/language-unsupervised/" target="_self">here</a> and shows that attention is usually attention wins, and that “The LSTM only<br></br>outperforms the Transformer on one dataset — MRPC.”</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Note7</strong>: <a href="https://jalammar.github.io/illustrated-transformer/" target="_self">Here</a> you can find a great explanation of the Transformer architecture and data flow!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">About the author</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: <a href="https://medium.com/@culurciello/" target="_self">Medium</a>, <a href="https://e-lab.github.io/html/contact-eugenio-culurciello.html" target="_self">webpage</a>, <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ" target="_self">Scholar</a>, <a href="https://www.linkedin.com/in/eugenioculurciello/" target="_self">LinkedIn</a>, and more…</p></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>