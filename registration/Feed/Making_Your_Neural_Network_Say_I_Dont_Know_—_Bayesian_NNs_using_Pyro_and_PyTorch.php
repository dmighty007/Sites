<!DOCTYPE html>
                <html>
                <head>
                    <title>Making Your Neural Network Say “I Don’t Know” — Bayesian NNs using Pyro and PyTorch</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://medium.com/@paraschopra?source=post_page-----b1c24e6ab8cd--------------------------------">Author : Paras Chopra</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>Making Your Neural Network Say “I Don’t Know” — Bayesian NNs using Pyro and PyTorch</h3></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Building an image classifier has become the new “hello world”. Remember the day when you first came across Python and your <em>print “hello world”</em> felt magical? I got the same feeling a couple months back when I followed <a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_self">the PyTorch official tutorial</a> and built myself a simple classifier that worked pretty well.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*gPrIyl8CnoI6ljdLCy7EcA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*gPrIyl8CnoI6ljdLCy7EcA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*gPrIyl8CnoI6ljdLCy7EcA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*gPrIyl8CnoI6ljdLCy7EcA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*gPrIyl8CnoI6ljdLCy7EcA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*gPrIyl8CnoI6ljdLCy7EcA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gPrIyl8CnoI6ljdLCy7EcA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*gPrIyl8CnoI6ljdLCy7EcA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*gPrIyl8CnoI6ljdLCy7EcA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*gPrIyl8CnoI6ljdLCy7EcA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*gPrIyl8CnoI6ljdLCy7EcA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*gPrIyl8CnoI6ljdLCy7EcA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*gPrIyl8CnoI6ljdLCy7EcA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*gPrIyl8CnoI6ljdLCy7EcA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*gPrIyl8CnoI6ljdLCy7EcA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I was astounded by the accuracy of my simple classifier. If I recall correctly, on the <a href="https://en.wikipedia.org/wiki/MNIST_database" target="_self">MNIST handwritten digits dataset</a>, it was north of 98% on the test set. (As a side note, this shows how far we’ve come along when a highly accurate image classifier can be built within hours. The ML community — and yes, that includes you— is awesome because of such liberal sharing of knowledge and tools)</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Despite the high accuracy of the classifier <strong>one issue kept nagging me:</strong></p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4">The neural network would spit out a category even if I gave it images completely unrelated to what it has been trained on.</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">You know the drill. Train a cat vs dog classifier, throw an image of a person and the network would either classify it as a cat or as a dog. (Perhaps — if the network has some sense of humor — happy people as dogs and unhappy ones as cats).</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*oFpS80A25TD7pgdSM3ml4g.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*oFpS80A25TD7pgdSM3ml4g.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*oFpS80A25TD7pgdSM3ml4g.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*oFpS80A25TD7pgdSM3ml4g.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*oFpS80A25TD7pgdSM3ml4g.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*oFpS80A25TD7pgdSM3ml4g.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oFpS80A25TD7pgdSM3ml4g.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*oFpS80A25TD7pgdSM3ml4g.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*oFpS80A25TD7pgdSM3ml4g.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*oFpS80A25TD7pgdSM3ml4g.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*oFpS80A25TD7pgdSM3ml4g.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*oFpS80A25TD7pgdSM3ml4g.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*oFpS80A25TD7pgdSM3ml4g.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*oFpS80A25TD7pgdSM3ml4g.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*oFpS80A25TD7pgdSM3ml4g.jpeg"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Build your model to throw up its (metaphorical) hands when it’s not sure (Photo via <a href="https://pixabay.com/en/upset-sad-confused-figurine-534103/" target="_self">Pixabay</a>)</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I knew that my expectations from the classifier were unrealistic. It behaved exactly how it was programmed. If I interpreted the final layer (softmax) output as probabilities, there will always be a category with the maximum value for any image that’s given as an input. The network simply didn’t know the concept of throwing its hands and saying: “this looks like something I’m not trained for.”</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">But that’s exactly what I wanted my neural network to do.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In almost all real world problems, <strong>what you want is not just a result but you also need knowledge of confidence / certainty in that result</strong>. If you’re making self-driving car, you want to not just detect pedestrians but also express how confident you are that the object is a pedestrian and not a traffic cone. Similarly, if you are writing a bot that trades on the stock market, you want it to recognize when situation goes out of its comfort zone, so it can stop acting and not go bankrupt. A big part of intelligence is not acting when one is uncertain. So it’s surprising that for many ML projects, expressing uncertainity isn’t what’s aimed for.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*HG51qQU8I34_fUgB.jpg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*HG51qQU8I34_fUgB.jpg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*HG51qQU8I34_fUgB.jpg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*HG51qQU8I34_fUgB.jpg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*HG51qQU8I34_fUgB.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*HG51qQU8I34_fUgB.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HG51qQU8I34_fUgB.jpg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*HG51qQU8I34_fUgB.jpg 640w, https://miro.medium.com/v2/resize:fit:720/0*HG51qQU8I34_fUgB.jpg 720w, https://miro.medium.com/v2/resize:fit:750/0*HG51qQU8I34_fUgB.jpg 750w, https://miro.medium.com/v2/resize:fit:786/0*HG51qQU8I34_fUgB.jpg 786w, https://miro.medium.com/v2/resize:fit:828/0*HG51qQU8I34_fUgB.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/0*HG51qQU8I34_fUgB.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/0*HG51qQU8I34_fUgB.jpg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*HG51qQU8I34_fUgB.jpg"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Probably one noisy boi (via <a href="https://ml.berkeley.edu/blog/2018/01/10/adversarial-examples/" target="_self">Tricking Neural Networks: Create your own Adversarial Examples</a>)</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I wanted to explore this direction by building an MNIST classifier which can express (un)certainty of the input image being a particular digit. Such a classifier will have a high accuracy when you show it digits but refuse to classify when you throw unrelated images at it. <strong>My final classifier had accuracy of ~97% on MNIST and it refused to classify white noise and the majority of unrelated (non-MNIST) images</strong>. You can <a href="https://github.com/paraschopra/bayesian-neural-network-mnist" target="_self">access the code</a> here and may want to follow the Jupyter notebook contained in the repo along with this tutorial.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">How bayesian neural networks work</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I will not introduce the full extent of Bayesian analysis here, but I’ll provide enough context for you to understand and then tinker with the <a href="https://github.com/paraschopra/bayesian-neural-network-mnist" target="_self">code</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The key idea is pretty simple: in the Bayesian worldview, <strong>everything has a probability distribution attached to it</strong>, including model parameters (weights and biases in NNs). In programming languages, we have variables that can take a specific value and every-time you access the variable, you get the same value. In contrast to that, in the bayesian world, we have similar entities that are called <strong>random variables </strong>that give a different value every time you access it. So if X is a random variable representing the normal distribution, every time you access X, it’ll have a different value.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><mark>This process of getting a new value from a random variable is called </mark><mark>sampling</mark><mark>.</mark> What value comes out depends on the random variable’s associated probability distribution. The wider the probability distribution associated with a random variable, the more uncertainty there is regarding its value because it could then take any value as per the (wide) probability distribution.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:69%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 512px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*KBxA2607_9al_s8i.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*KBxA2607_9al_s8i.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*KBxA2607_9al_s8i.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*KBxA2607_9al_s8i.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*KBxA2607_9al_s8i.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*KBxA2607_9al_s8i.png 1100w, https://miro.medium.com/v2/resize:fit:1024/format:webp/0*KBxA2607_9al_s8i.png 1024w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 512px" srcset="https://miro.medium.com/v2/resize:fit:640/0*KBxA2607_9al_s8i.png 640w, https://miro.medium.com/v2/resize:fit:720/0*KBxA2607_9al_s8i.png 720w, https://miro.medium.com/v2/resize:fit:750/0*KBxA2607_9al_s8i.png 750w, https://miro.medium.com/v2/resize:fit:786/0*KBxA2607_9al_s8i.png 786w, https://miro.medium.com/v2/resize:fit:828/0*KBxA2607_9al_s8i.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*KBxA2607_9al_s8i.png 1100w, https://miro.medium.com/v2/resize:fit:1024/0*KBxA2607_9al_s8i.png 1024w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/512/0*KBxA2607_9al_s8i.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">If your random variable is the sum of digits of two dice throws, at each throw you’ll get a value whose probability depends on the distribution above. This means the most likely sum that you can get is 7, and least likely is 2 and 12. (From <a href="https://en.wikipedia.org/wiki/Probability_distribution" target="_self">Wikipedia</a>)</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In a traditional neural networks you have fixed weights and biases that determine how an input is transformed into an output. In a bayesian neural network, all weights and biases have a probability distribution attached to them. <strong>To classify an image, you do multiple runs (forward passes) of the network, each time with a new set of sampled weights and biases</strong>. Instead of a single set of output values what you get is multiple sets, one for each of the multiple runs. The set of output values represent a probability distribution on output values and hence you can find out confidence and uncertainty in <em>each</em> of the outputs. As you will see, if the input image is something the network has never seen, for all output classes, the uncertainty will be high which you should interpret the network saying: “I really don’t know what this image is about”.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Writing your first Bayesian Neural Network in Pyro and PyTorch</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The code assumes familiarity with basic ideas of probabilistic programming and PyTorch. In case you’re new to either of these, I recommend following resources:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><a href="http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/" target="_self">Bayesian Methods for Hackers</a> to learn the basics of Bayesian modeling and probabilistic programming</li><li class="ff3" style="font-size:22px;"><a href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" target="_self">Deep Learning with PyTorch: A 60 minute Blitz</a>. Specifically, the tutorial on training a classifier.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">PyTorch has a companion library called <a href="http://pyro.ai/" target="_self">Pyro</a> that gives the functionality to do probabilistic programming on neural networks written in PyTorch. This “automatic” conversion of NNs into bayesian counterparts has two steps:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">First, it helps in assigning probability distributions to all weights and biases in the network, hence converting them into random variables</li><li class="ff3" style="font-size:22px;">Second, it helps in using the training data to <strong>infer</strong> those probability distributions so that you can use it to classify images</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Inference is the most difficult step of the entire process. It’s based on the famous <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem" target="_self">Bayes theorem</a> that you may have seen before.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:55%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 414px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*77JdEb5Ub9hzATOCwc82HQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*77JdEb5Ub9hzATOCwc82HQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*77JdEb5Ub9hzATOCwc82HQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*77JdEb5Ub9hzATOCwc82HQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*77JdEb5Ub9hzATOCwc82HQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*77JdEb5Ub9hzATOCwc82HQ.png 1100w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*77JdEb5Ub9hzATOCwc82HQ.png 828w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 414px" srcset="https://miro.medium.com/v2/resize:fit:640/1*77JdEb5Ub9hzATOCwc82HQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*77JdEb5Ub9hzATOCwc82HQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*77JdEb5Ub9hzATOCwc82HQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*77JdEb5Ub9hzATOCwc82HQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*77JdEb5Ub9hzATOCwc82HQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*77JdEb5Ub9hzATOCwc82HQ.png 1100w, https://miro.medium.com/v2/resize:fit:828/1*77JdEb5Ub9hzATOCwc82HQ.png 828w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/414/1*77JdEb5Ub9hzATOCwc82HQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">The deceptively simple equation that rules the world</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Going into nitty-gritties of this equation is out of the scope of this tutorial but I’ll try giving you intuition of what’s happening. Assume <em>A</em> is the initial probability distributions of weights and biases (known as <strong>priors, </strong>usually some standard distribution like normal or uniform random) and <em>B</em> is the training data (input/output pairs of images/labels).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The key idea of the Bayes theorem which you should remember is that we want to use data to find out the updated distributions of weights and biases <em>P(A | B)</em> (<strong>posterior</strong>). Just like using initially randomly assigned weights and biases of a network, the initial distributions of parameters (priors) will give us wrong results. Only after using data to get updated distributions of parameters can we use the network to classify images.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The probability distributions of weights and biases are updated via the Bayes theorem taking into account their initial values <em>P(A)</em> and <strong>likelihood</strong> of those initial distributions to describe the input data <em>P (B|A) </em>(it’s read as probability of B given A). The updated distributions of weights <em>P(A | B)</em> (<strong>posterior</strong>) depends on which one has a stronger pull — the prior or the likelihood. (If you’re curious about the P(B) term, it’ll get clear later in this tutorial).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I know that the paragraph above may make strict Bayesians cry in horror. I know the definitions are imprecise. But this tutorial isn’t to introduce the full glory of bayesian ways of looking at the data. There are entire <a href="http://www.stat.columbia.edu/~gelman/book/" target="_self">books</a> and <a href="https://www.coursera.org/learn/bayesian" target="_self">courses</a> on it and I can’t do justice to it in one tutorial. This tutorial is about practical implementation of a Bayesian neural network. I scratched my head for days diving into <a href="http://pyro.ai/examples/" target="_self">Pyro tutorials</a> and trying to convert one of their examples into a classifier. I finally found a brief tutorial on <a href="https://www.ibm.com/blogs/research/2018/11/pyro-wml/" target="_self">IBM Watson’s website</a> on using Pyro on MNIST. <a href="https://github.com/paraschopra/bayesian-neural-network-mnist" target="_self">My code</a> is based on that tutorial but I extend it to on non-MNIST and white-noise data to see if bayesian neural networks can really say “I don’t know” when presented with an input they have not seen before.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Even though I’ll try explaining the basics of Pyro, you will get a lot of value from this tutorial if you go through their first three tutorials — <a href="http://pyro.ai/examples/intro_part_i.html" target="_self">part I</a>, <a href="http://pyro.ai/examples/intro_part_ii.html" target="_self">part II</a> and <a href="http://pyro.ai/examples/svi_part_i.html" target="_self">part III</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Ready? Let’s get straight to the code</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre>class NN(nn.Module):
	
    def __init__(self, input_size, hidden_size, output_size):
        super(NN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.out = nn.Linear(hidden_size, output_size)
        
    def forward(self, x):
        output = self.fc1(x)
        output = F.relu(output)
        output = self.out(output)
        return output

train_loader = torch.utils.data.DataLoader(
        datasets.MNIST('mnist-data/', train=True, download=True,
                       transform=transforms.Compose([transforms.ToTensor(),])),
        batch_size=128, shuffle=True)

test_loader = torch.utils.data.DataLoader(
        datasets.MNIST('mnist-data/', train=False, transform=transforms.Compose([transforms.ToTensor(),])
                       ),
        batch_size=128, shuffle=True)

net = NN(28*28, 1024, 10)</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After importing PyTorch, Pyro and other standard libraries (like matplotlib and numpy), we define a standard feedforward neural network of one hidden layer of 1024 units. We also load MNIST data.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre>def model(x_data, y_data):
    
    fc1w_prior = Normal(loc=torch.zeros_like(net.fc1.weight), scale=torch.ones_like(net.fc1.weight))
    fc1b_prior = Normal(loc=torch.zeros_like(net.fc1.bias), scale=torch.ones_like(net.fc1.bias))
    
    outw_prior = Normal(loc=torch.zeros_like(net.out.weight), scale=torch.ones_like(net.out.weight))
    outb_prior = Normal(loc=torch.zeros_like(net.out.bias), scale=torch.ones_like(net.out.bias))
    
    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior,  'out.weight': outw_prior, 'out.bias': outb_prior}
    
    # lift module parameters to random variables sampled from the priors
    lifted_module = pyro.random_module("module", net, priors)
    # sample a regressor (which also samples w and b)
    lifted_reg_model = lifted_module()
    
    lhat = log_softmax(lifted_reg_model(x_data))
    
    pyro.sample("obs", Categorical(logits=lhat), obs=y_data)</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In Pyro, the <em>model()</em> function defines how the output data is generated. In our classifier, the 10 output values corresponding to each digit are generated when we run the neural network (initialised in the <em>net</em> variable above) with a flattened 28*28 pixel image. Within <em>model()</em>, the function <em>pyro.random_module()</em> converts parameters of our neural network (weights and biases) into random variables that have the initial (prior) probability distribution given by <em>fc1w_prior</em>, <em>fc1b_prior</em>, <em>outw_prior</em> and <em>outb_prior</em> (in our case, as you can see, we’re initialising these with a <a href="https://en.wikipedia.org/wiki/Normal_distribution" target="_self">normal distribution</a>). Finally, through pyro.sample(), we tell Pyro that the output of this network is categorical in nature (i.e. it can either be 0, 1, 2, and so on.)</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre>def guide(x_data, y_data):
    
    # First layer weight distribution priors
    fc1w_mu = torch.randn_like(net.fc1.weight)
    fc1w_sigma = torch.randn_like(net.fc1.weight)
    fc1w_mu_param = pyro.param("fc1w_mu", fc1w_mu)
    fc1w_sigma_param = softplus(pyro.param("fc1w_sigma", fc1w_sigma))
    fc1w_prior = Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param)
    # First layer bias distribution priors
    fc1b_mu = torch.randn_like(net.fc1.bias)
    fc1b_sigma = torch.randn_like(net.fc1.bias)
    fc1b_mu_param = pyro.param("fc1b_mu", fc1b_mu)
    fc1b_sigma_param = softplus(pyro.param("fc1b_sigma", fc1b_sigma))
    fc1b_prior = Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param)
    # Output layer weight distribution priors
    outw_mu = torch.randn_like(net.out.weight)
    outw_sigma = torch.randn_like(net.out.weight)
    outw_mu_param = pyro.param("outw_mu", outw_mu)
    outw_sigma_param = softplus(pyro.param("outw_sigma", outw_sigma))
    outw_prior = Normal(loc=outw_mu_param, scale=outw_sigma_param).independent(1)
    # Output layer bias distribution priors
    outb_mu = torch.randn_like(net.out.bias)
    outb_sigma = torch.randn_like(net.out.bias)
    outb_mu_param = pyro.param("outb_mu", outb_mu)
    outb_sigma_param = softplus(pyro.param("outb_sigma", outb_sigma))
    outb_prior = Normal(loc=outb_mu_param, scale=outb_sigma_param)
    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior, 'out.weight': outw_prior, 'out.bias': outb_prior}
    
    lifted_module = pyro.random_module("module", net, priors)
    
    return lifted_module()</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Understanding this part — represented by the <em>guide()</em> function — was the trickiest thing for me. For quite some time, I didn’t understand why was it needed, especially because it looked very much like the <em>model()</em> function. Explaining it will be hard, but I’ll try. (If you aren’t able to understand, my explanation I recommend <a href="http://pyro.ai/examples/" target="_self">Pyro tutorials</a> or links below that I’ve provided on the topic).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Take a look at the Bayes equation again:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:55%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 414px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*77JdEb5Ub9hzATOCwc82HQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*77JdEb5Ub9hzATOCwc82HQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*77JdEb5Ub9hzATOCwc82HQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*77JdEb5Ub9hzATOCwc82HQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*77JdEb5Ub9hzATOCwc82HQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*77JdEb5Ub9hzATOCwc82HQ.png 1100w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*77JdEb5Ub9hzATOCwc82HQ.png 828w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 414px" srcset="https://miro.medium.com/v2/resize:fit:640/1*77JdEb5Ub9hzATOCwc82HQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*77JdEb5Ub9hzATOCwc82HQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*77JdEb5Ub9hzATOCwc82HQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*77JdEb5Ub9hzATOCwc82HQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*77JdEb5Ub9hzATOCwc82HQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*77JdEb5Ub9hzATOCwc82HQ.png 1100w, https://miro.medium.com/v2/resize:fit:828/1*77JdEb5Ub9hzATOCwc82HQ.png 828w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/414/1*77JdEb5Ub9hzATOCwc82HQ.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In the <em>model()</em> function, we have defined <em>P(A)</em> — the priors on weights and biases. The <em>P(B|A)</em> part of the equation is represented by the neural network because given the parameters (weights and biases), we can do multiple runs on image, label pairs and find out the corresponding probability distribution of training data. Before training, initially since priors on weights and priors are all the same (all are normal distribution), the likelihood of getting a high probability for the correct label for a given image will be low.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In fact, <strong>inference</strong> is the process of learning probability distributions for weights and biases that maximize the <strong>likelihood</strong> of getting a high probability for the correct image, label pairs.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This process of inference is represented by <em>P(A |B)</em> which is the <strong>posterior</strong> probability of parameters <em>A</em> given the input/output pairs (<em>B</em>). I wrote earlier that inference is difficult. That’s because of the term you see in the denominator <em>P(B)</em>. This term is called <strong>evidence </strong>and it is simply THE probability of observing the data (input/output pairs) under all possible parameter values, weighted by their respective probabilities.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:61%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 456px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*aZGLILW5v_tdTdIw2DLCXg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*aZGLILW5v_tdTdIw2DLCXg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*aZGLILW5v_tdTdIw2DLCXg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*aZGLILW5v_tdTdIw2DLCXg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*aZGLILW5v_tdTdIw2DLCXg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*aZGLILW5v_tdTdIw2DLCXg.png 1100w, https://miro.medium.com/v2/resize:fit:912/format:webp/1*aZGLILW5v_tdTdIw2DLCXg.png 912w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 456px" srcset="https://miro.medium.com/v2/resize:fit:640/1*aZGLILW5v_tdTdIw2DLCXg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*aZGLILW5v_tdTdIw2DLCXg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*aZGLILW5v_tdTdIw2DLCXg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*aZGLILW5v_tdTdIw2DLCXg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*aZGLILW5v_tdTdIw2DLCXg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*aZGLILW5v_tdTdIw2DLCXg.png 1100w, https://miro.medium.com/v2/resize:fit:912/1*aZGLILW5v_tdTdIw2DLCXg.png 912w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/456/1*aZGLILW5v_tdTdIw2DLCXg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Calculating this sum is hard because of three reasons:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Hypothetically, values of parameters <em>Aj</em> could range from -infinity to +infinity</li><li class="ff3" style="font-size:22px;">For <em>each</em> value of <em>Aj</em> in that range, you have to run the model to find the likelihood of generating the input, output pairs you observe (the total dataset could be in millions of pairs)</li><li class="ff3" style="font-size:22px;">There could be not one but many such parameters (j &gt;&gt; 1). In fact, for a neural network of our size, we have ~8million parameters (number of weights = 1024*28*28*10).</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The type of enumeration approach for posterior that I describe above is not practical for anything but very trivial models. Instead of this grid-like enumeration, what if we could do random sampling? In fact, sampling based approaches are widely used and they go by the name <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method" target="_self">Monte-Carlo methods</a>. Particularly, <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm" target="_self">Metropolis-Hastings</a> is a popular algorithm for Monte-Carlo sampling. (It’s included in Pyro and most of the other probabilistic programming languages).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Unfortunately, for complex bayesian models such as a neural network with 8 million parameters, Monte-Carlo methods are still slow to converge and may take weeks to discover the full posterior.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Thankfully, there’s an increasingly popular method called <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods" target="_self">Variational Bayes</a> that seems perfect for finding posteriors for neural network parameters, even for large datasets. To understand the intuition behind this technique, I highly recommend watching the following video (up to the first 40 minutes).</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:80%;"><iframe  scrolling="auto" width="854.0" height="480.0" frameborder="0" src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FDYRK0-_K2UU%3Ffeature%3Doembed&url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DDYRK0-_K2UU&image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FDYRK0-_K2UU%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=youtube" allowfullscreen=""></iframe></div><br><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The gist of Variational Bayes methods is that since we can’t exactly compute the posterior, we can find the closest probability distribution to it that is “well-behaved”. By “well-behaved”, I mean a distribution (like a normal or an exponential distribution) that can be represented by a small set of parameters like mean or variance. So, after random initialization of those parameters in a “well-behaved” distribution, you can do gradient descent and modify parameters of the distribution (like mean or variance) a little bit each time to see if the resulting distribution is closer to the posterior that you want to calculate. (If you’re thinking how do we know if resulting distribution is closer to the posterior if posterior is exactly what we want to calculate, you’ve understood the idea. The answer is that, surprisingly, we don’t need the exact posterior to find closeness between it and the other “well-behaved” distribution. Watch the video above to understand the measure of closeness that we actually optimize for: <a href="http://legacydirs.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf" target="_self">Evidence Lower Bound</a> or ELBO. I also found this <a href="https://chrisorm.github.io/VI-Why.html" target="_self">series</a> <a href="https://chrisorm.github.io/VI-ELBO.html" target="_self">of</a> <a href="https://chrisorm.github.io/VI-MC.html" target="_self">posts</a> useful on the topic).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To understand Variational Bayes intuitively, see the diagram below:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:67%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 501px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*YVFAbC7DgfAj94-0TRt8IQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*YVFAbC7DgfAj94-0TRt8IQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*YVFAbC7DgfAj94-0TRt8IQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*YVFAbC7DgfAj94-0TRt8IQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*YVFAbC7DgfAj94-0TRt8IQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*YVFAbC7DgfAj94-0TRt8IQ.png 1100w, https://miro.medium.com/v2/resize:fit:1002/format:webp/1*YVFAbC7DgfAj94-0TRt8IQ.png 1002w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 501px" srcset="https://miro.medium.com/v2/resize:fit:640/1*YVFAbC7DgfAj94-0TRt8IQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*YVFAbC7DgfAj94-0TRt8IQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*YVFAbC7DgfAj94-0TRt8IQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*YVFAbC7DgfAj94-0TRt8IQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*YVFAbC7DgfAj94-0TRt8IQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*YVFAbC7DgfAj94-0TRt8IQ.png 1100w, https://miro.medium.com/v2/resize:fit:1002/1*YVFAbC7DgfAj94-0TRt8IQ.png 1002w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/501/1*YVFAbC7DgfAj94-0TRt8IQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Via <a href="https://blog.evjang.com/2016/08/variational-bayes.html" target="_self">A Beginner’s Guide to Variational Methods: Mean-Field Approximation</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The blue curve is the true posterior that you’ll get if you do that long (enumerative) calculation we talked about before. This curve can take any arbitrary shape because it’s a result of enumerative calculation. In contrast to that, because it’s a well behaved distribution like a normal distribution, the green curve’s entire shape can be described one parameter Z. What Variational Bayes methods do is to then use gradient descent methods to change the value of Z parameter from initial randomly initialised value to the value whose resultant distribution best approximates the true posterior. At the end of optimization, the green curve isn’t exactly like the blue curve but it’s pretty similar. And we can safely use the approximating green curve instead of unknown true blue curve for making predictions. (If all this is hard to understand, I recommend watching the video above.)</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Now this is where the guide function comes in. It helps us initialize a well-behaved distribution that later we can optimize to approximate the true posterior. Take a look at it again:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre>def guide(x_data, y_data):
    
    # First layer weight distribution priors
    fc1w_mu = torch.randn_like(net.fc1.weight)
    fc1w_sigma = torch.randn_like(net.fc1.weight)
    fc1w_mu_param = pyro.param("fc1w_mu", fc1w_mu)
    fc1w_sigma_param = softplus(pyro.param("fc1w_sigma", fc1w_sigma))
    fc1w_prior = Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param)
    # First layer bias distribution priors
    fc1b_mu = torch.randn_like(net.fc1.bias)
    fc1b_sigma = torch.randn_like(net.fc1.bias)
    fc1b_mu_param = pyro.param("fc1b_mu", fc1b_mu)
    fc1b_sigma_param = softplus(pyro.param("fc1b_sigma", fc1b_sigma))
    fc1b_prior = Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param)
    # Output layer weight distribution priors
    outw_mu = torch.randn_like(net.out.weight)
    outw_sigma = torch.randn_like(net.out.weight)
    outw_mu_param = pyro.param("outw_mu", outw_mu)
    outw_sigma_param = softplus(pyro.param("outw_sigma", outw_sigma))
    outw_prior = Normal(loc=outw_mu_param, scale=outw_sigma_param).independent(1)
    # Output layer bias distribution priors
    outb_mu = torch.randn_like(net.out.bias)
    outb_sigma = torch.randn_like(net.out.bias)
    outb_mu_param = pyro.param("outb_mu", outb_mu)
    outb_sigma_param = softplus(pyro.param("outb_sigma", outb_sigma))
    outb_prior = Normal(loc=outb_mu_param, scale=outb_sigma_param)
    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior, 'out.weight': outw_prior, 'out.bias': outb_prior}
    
    lifted_module = pyro.random_module("module", net, priors)
    
    return lifted_module()</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This <em>guide()</em> function describes the Z parameters (like mean and variance of weights and biases) that can be changed to see if resultant distribution closely approximates the posterior that comes out of <em>model(). </em>Now, in our case the <em>model()</em> looks very similar to <em>guide()</em> but that need not always be the case. In theory, the <em>model()</em> function could be much more complicated than the <em>guide()</em> function.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">With <em>model()</em> and <em>guide()</em> functions figured out, we’re ready to do inference. First, let’s tell Pyro which optimizer to use for doing variational inference.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre>optim = Adam({"lr": 0.01})
svi = SVI(model, guide, optim, loss=Trace_ELBO())</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">You’ll notice that we’re using the Adam optimizer from PyTorch (to know more about it and other optimization algorithms, <a href="http://ruder.io/optimizing-gradient-descent/" target="_self">here’s a fantastic series</a>). The loss function that we’re using for optimization is ELBO (this is like using Mean Squared Error or Cross Entropy loss when training a non-bayesian neural network via backpropagation).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Let’s write the optimization loop.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre>num_iterations = 5
loss = 0

for j in range(num_iterations):
    loss = 0
    for batch_id, data in enumerate(train_loader):
        # calculate the loss and take a gradient step
        loss += svi.step(data[0].view(-1,28*28), data[1])
    normalizer_train = len(train_loader.dataset)
    total_epoch_loss_train = loss / normalizer_train
    
    print("Epoch ", j, " Loss ", total_epoch_loss_train)</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">You’d notice that this loop is pretty much how we train a standard neural network. There are multiple epochs / iterations (in this case it’s 5). And in each iteration, we go through a mini-batch of data (input/output pairs of images, labels). One more benefit of variational inference is that we do not have to feed in the entire dataset in one go (which could be in millions). Since an optimizer takes many thousands of steps to find the best value of parameters of guide function, at each step we can feed it the a separate mini-batch of data. This speeds up inference tremendously.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Once the loss seems to be stabilizing / converging to a value, we can stop the optimization and see how accurate our bayesian neural network is. Here’s the code for doing that.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre>num_samples = 10
def predict(x):
    sampled_models = [guide(None, None) for _ in range(num_samples)]
    yhats = [model(x).data for model in sampled_models]
    mean = torch.mean(torch.stack(yhats), 0)
    return np.argmax(mean.numpy(), axis=1)

print('Prediction when network is forced to predict')
correct = 0
total = 0
for j, data in enumerate(test_loader):
    images, labels = data
    predicted = predict(images.view(-1,28*28))
    total += labels.size(0)
    correct += (predicted == labels).sum().item()
print("accuracy: %d %%" % (100 * correct / total))</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">First thing to notice in the <em>predict()</em> function is that we’re using the learned <em>guide()</em> function (and not the <em>model() </em>function) to do predictions. This is because for <em>model()</em>, all we know is priors for weights and not the posterior. But for <em>guide()</em> after optimization iterations, the distribution given by the parameter values approximate the true posterior and so we can use it for predictions.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Second thing to notice is that for each prediction, we’re sampling a new set of weights and parameters 10 times (given by <em>num_samples</em>). This effectively means that we’re sampling a new neural network 10 times for making one prediction. As you will see later, this is what enables us to give uncertainities on outputs. In the case above, to make a prediction, we’re averaging final layer output values of the 10 sampled nets for the given input and taking the max activation value as the predicted digit. Doing that, we see that <strong>our net is accurate 89% of times on the test set</strong>. But note that in this case, we’re forcing our net to make a prediction in each case. We haven’t used the magic of Bayes theorem to enable our net to say: “I refuse to make a prediction here”.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">That is exactly we will do next using the code below.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre>prob = np.percentile(histo_exp, 50) #sampling median probability
        
if(prob>0.2): #select if network thinks this sample is 20% chance of this being a label
  highlight = True #possibly an answer</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I won’t go into the full code of estimating uncertanity (which you can see <a href="https://github.com/paraschopra/bayesian-neural-network-mnist" target="_self">in the notebook</a>). Essentially, what we’re doing is this:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">For an input image, take 100 samples of neural networks to get 100 different output values from the last layer</li><li class="ff3" style="font-size:22px;">Convert those outputs (which are logsoftmaxed) into probabilities by exponentiating them</li><li class="ff3" style="font-size:22px;">Now, given the input image, for <em>each</em> digit we have 100 probability values</li><li class="ff3" style="font-size:22px;">We take median (50th percentile) of these 100 probability values as the threshold probability for each digit</li><li class="ff3" style="font-size:22px;">If the threshold probability is greater than 0.2, we select the digit as a classification output from the network</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In other words, we want the neural network to output a digit as a recommendation if out of multiple samples of probability, the median probability for that digit is at least 0.2. This means that for some inputs, the network can output two digits as classification output while for others it can output no digits (which is exactly what we want if we give it non-digit images).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Results on the MNIST dataset</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">When I ran the network on the entire MNIST test set of 10,000 images, I got these results:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><strong>Percentage of images which the network refused to classify: 12.5%</strong> (1250 out of 10,000)</li><li class="ff3" style="font-size:22px;"><strong>Accuracy on the remaining 8750 “accepted” images: 96%</strong></li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Note that this 96% accuracy when we gave the network a chance to refuse classification is much higher than the 88% accuracy when it was forced to classify.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To visualize what’s happening under the hood. I plotted 100 random images from the MNIST test batch. For most of the 100 images, the network classified accurately.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*s4LB_kBri3xYk3CgILNymw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*s4LB_kBri3xYk3CgILNymw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*s4LB_kBri3xYk3CgILNymw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*s4LB_kBri3xYk3CgILNymw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*s4LB_kBri3xYk3CgILNymw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*s4LB_kBri3xYk3CgILNymw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s4LB_kBri3xYk3CgILNymw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*s4LB_kBri3xYk3CgILNymw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*s4LB_kBri3xYk3CgILNymw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*s4LB_kBri3xYk3CgILNymw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*s4LB_kBri3xYk3CgILNymw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*s4LB_kBri3xYk3CgILNymw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*s4LB_kBri3xYk3CgILNymw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*s4LB_kBri3xYk3CgILNymw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*s4LB_kBri3xYk3CgILNymw.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">What the plot above shows is that the real label for input image was 3, and for each of the 10 digits, a histogram of log-probabilities is shown. For the label 3, the median log-probability was actually close to 0 which means the probability of this image being 3 is close to 1 (exp(0) = 1). That is why it’s highlighted in yellow. Since the label that network selected is same as the real label, it shows “Correct”. You can also see what the input image actually looked like.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In my multiple runs with 100 images, the accuracy of network when it made predictions was 94–96%. <strong>The network regularly chose to not make predictions on 10–15% images</strong> and it was fun to look at some of the images where network said: “I’m not really sure”.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*T02yysKHV-o_RN-6vkPexg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*T02yysKHV-o_RN-6vkPexg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*T02yysKHV-o_RN-6vkPexg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*T02yysKHV-o_RN-6vkPexg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*T02yysKHV-o_RN-6vkPexg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*T02yysKHV-o_RN-6vkPexg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T02yysKHV-o_RN-6vkPexg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*T02yysKHV-o_RN-6vkPexg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*T02yysKHV-o_RN-6vkPexg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*T02yysKHV-o_RN-6vkPexg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*T02yysKHV-o_RN-6vkPexg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*T02yysKHV-o_RN-6vkPexg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*T02yysKHV-o_RN-6vkPexg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*T02yysKHV-o_RN-6vkPexg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*T02yysKHV-o_RN-6vkPexg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">It’s hard for even me to tell that the digit is a “2”. You can see from histograms that the network had a high uncertainty both for 2 and 3 labels . For such cases where network is undecided, the distribution of log-probabilities is wide for <em>all</em> labels while in the case of the accurate classification of “3” in the plot above, you’d notice that the distribution for the digit 3 was narrow while for all other digits it was wide (which meant the network was pretty sure it was 3).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Another case where the network was undecided.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*3Wcwi0r3jkEXW8krKYxxXQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*3Wcwi0r3jkEXW8krKYxxXQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*3Wcwi0r3jkEXW8krKYxxXQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*3Wcwi0r3jkEXW8krKYxxXQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*3Wcwi0r3jkEXW8krKYxxXQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*3Wcwi0r3jkEXW8krKYxxXQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Wcwi0r3jkEXW8krKYxxXQ.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*3Wcwi0r3jkEXW8krKYxxXQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*3Wcwi0r3jkEXW8krKYxxXQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*3Wcwi0r3jkEXW8krKYxxXQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*3Wcwi0r3jkEXW8krKYxxXQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*3Wcwi0r3jkEXW8krKYxxXQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*3Wcwi0r3jkEXW8krKYxxXQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*3Wcwi0r3jkEXW8krKYxxXQ.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*3Wcwi0r3jkEXW8krKYxxXQ.jpeg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">You see the image is all messed up. A traditional neural network might have spitted out something but our bayesian network refuses to say anything.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Results on randomly generated images</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To see how the network does when it is fed pure white noise, I generated 100 random images.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre># generate random data

images_random = torch.rand(100,28,28)
labels_random = torch.randint(0,10, (100,))</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">When these images were given as an input, <strong>the network refused to make predictions on 95% of them</strong>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This is how a typical randomly generated image looked like:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*acjpzYgL6FOVBo1YDrfkyA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*acjpzYgL6FOVBo1YDrfkyA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*acjpzYgL6FOVBo1YDrfkyA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*acjpzYgL6FOVBo1YDrfkyA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*acjpzYgL6FOVBo1YDrfkyA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*acjpzYgL6FOVBo1YDrfkyA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*acjpzYgL6FOVBo1YDrfkyA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*acjpzYgL6FOVBo1YDrfkyA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*acjpzYgL6FOVBo1YDrfkyA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*acjpzYgL6FOVBo1YDrfkyA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*acjpzYgL6FOVBo1YDrfkyA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*acjpzYgL6FOVBo1YDrfkyA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*acjpzYgL6FOVBo1YDrfkyA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*acjpzYgL6FOVBo1YDrfkyA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*acjpzYgL6FOVBo1YDrfkyA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Results on the not-MNIST dataset</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I went one step further and downloaded the <a href="http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html" target="_self">not-MNIST</a> dataset which is a dataset of alphabets rather than digits. It looks like this:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:95%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 693px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*rYvnrDYnWy30WX80R1aLYg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*rYvnrDYnWy30WX80R1aLYg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*rYvnrDYnWy30WX80R1aLYg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*rYvnrDYnWy30WX80R1aLYg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*rYvnrDYnWy30WX80R1aLYg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*rYvnrDYnWy30WX80R1aLYg.png 1100w, https://miro.medium.com/v2/resize:fit:1386/format:webp/1*rYvnrDYnWy30WX80R1aLYg.png 1386w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 693px" srcset="https://miro.medium.com/v2/resize:fit:640/1*rYvnrDYnWy30WX80R1aLYg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*rYvnrDYnWy30WX80R1aLYg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*rYvnrDYnWy30WX80R1aLYg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*rYvnrDYnWy30WX80R1aLYg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*rYvnrDYnWy30WX80R1aLYg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*rYvnrDYnWy30WX80R1aLYg.png 1100w, https://miro.medium.com/v2/resize:fit:1386/1*rYvnrDYnWy30WX80R1aLYg.png 1386w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/693/1*rYvnrDYnWy30WX80R1aLYg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For the not-MNIST test set, <strong>the network refused to classify ~80% of images</strong> (363 out of a total of 459 in the test set).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">An example of not-MNIST image is shown below.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Hk6NUhxex3R6XFNoFVmw3Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Hk6NUhxex3R6XFNoFVmw3Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Hk6NUhxex3R6XFNoFVmw3Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Hk6NUhxex3R6XFNoFVmw3Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Hk6NUhxex3R6XFNoFVmw3Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Hk6NUhxex3R6XFNoFVmw3Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hk6NUhxex3R6XFNoFVmw3Q.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Hk6NUhxex3R6XFNoFVmw3Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*Hk6NUhxex3R6XFNoFVmw3Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*Hk6NUhxex3R6XFNoFVmw3Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*Hk6NUhxex3R6XFNoFVmw3Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*Hk6NUhxex3R6XFNoFVmw3Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*Hk6NUhxex3R6XFNoFVmw3Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*Hk6NUhxex3R6XFNoFVmw3Q.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*Hk6NUhxex3R6XFNoFVmw3Q.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">It’s great to see our network give good accuracy on what it was trained on (MNSIT) while not getting fooled by a dataset that was custom designed to fool it (not-MNIST).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Conclusion and how to make our Bayesian network even better</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The <a href="https://en.wikipedia.org/wiki/MNIST_database" target="_self">state of the art results on MNIST dataset</a> have 99.8% accuracy. So our ~96% accuracy (when we want to make a prediction) is a far cry from that.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">There are four ways to get better accuracy:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">We used a very simple model: single layer neural network with 1024 neurons. If we use a more advanced convolutional network, I’m sure we can improve our accuracy.</li><li class="ff3" style="font-size:22px;">If we keep running our optimization for much longer, we can improve our accuracy</li><li class="ff3" style="font-size:22px;">If we sample more data points (rather than 100) per image, results could improve</li><li class="ff3" style="font-size:22px;">If we make our acceptance criteria from median probability to be minimum 0.2 to perhaps 10th percentile probability to be minimum 0.5, our network will reject a lot more images but on accepted ones, it may have a higher accuracy</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Overall, I’m very happy with the results. I hope you have fun playing with the <a href="https://github.com/paraschopra/bayesian-neural-network-mnist" target="_self">code</a> :)</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Feel free to comment on this post with your questions and I’ll try my best to answer them. If you are able to improve the code, send a pull request to me on github. And in case, you use the basic code on a new data set or problem, please email me at paras1987 &lt;at&gt; gmail &lt;dot&gt; com and I’d love to hear from you.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>Thanks Nirant Kasliwal, Divyanshu Kalra and S. Adithya for reviewing the draft and giving helpful suggestions.</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">PS: I’ve recently made a 20 minute video on what makes deep learning so effective. Go <a href="https://www.youtube.com/watch?v=Y-WgVcWQYs4" target="_self">watch it now</a>!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Liked this tutorial? Check out my other tutorials too:</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><a href="https://medium.com/one-neural-network-many-uses-image-captioning-image-search-similar-image-and-words-in-one-model-1e22080ce73d" target="_self">One neural network, many uses.</a> Build image search, image captioning, similar words and similar images using a single model</li><li class="ff3" style="font-size:22px;"><a href="https://medium.com/making-deep-neural-networks-paint-to-understand-how-they-work-4be0901582ee?source=your_stories_page---------------------------" target="_self">Making deep neural networks paint to understand how they work.</a> Generate abstract art in 100 lines of PyTorch code and explore how neural networks work</li><li class="ff3" style="font-size:22px;"><a href="https://medium.com/generating-new-ideas-for-machine-learning-projects-through-machine-learning-ce3fee50ec2" target="_self">Generating New Ideas for Machine Learning Projects Through Machine Learning</a>. Generating style-specific text from a small corpus of 2.5k sentences using a pre-trained language model. Code in PyTorch</li><li class="ff3" style="font-size:22px;"><a href="https://medium.com/reinforcement-learning-without-gradients-evolving-agents-using-genetic-algorithms-8685817d84f" target="_self">Reinforcement learning without gradients: evolving agents using Genetic Algorithms</a>. Implementing Deep Neuroevolution in PyTorch to evolve an agent for CartPole [code + tutorial]</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>I tweet about deep learning and AI</strong>. Follow me at <a href="https://twitter.com/paraschopra" target="_self">https://twitter.com/paraschopra</a></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><div class="sketchy"><a href="https://twitter.com/paraschopra"><h2 style="color:blueviolet; font-family:Arial, Helvetica, sans-serif; font-size:25px;">Paras Chopra (@paraschopra) | Twitter</h2><h3 style="color:rgb(45, 34, 54); font-family:Arial, Helvetica, sans-serif; font-size:20px;">The latest Tweets from Paras Chopra (@paraschopra). Follow me if you have ever wondered why does the universe exist…</h3><p>twitter.com</p></a></div></div></div></div></section><br><?php include_once 'Elemental/footer.php'; ?>