<!DOCTYPE html>
                <html>
                <head>
                    <title>ConvNets Series. Actual Project Prototyping with Mask R-CNN</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://towardsdatascience.com/convnets-series-actual-project-prototyping-with-mask-r-cnn-dbcd0b4ab519"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://medium.com/@kd_is_typing?source=post_page-----dbcd0b4ab519--------------------------------">Author : Kirill Danilyuk</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>ConvNets Series. Actual Project Prototyping with Mask R-CNN</h3></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Intro</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This ConvNets series progresses from a toy dataset of German traffic signs to a more practical real life problem I was asked to solve: <strong>“Is it possible to implement a piece of deep learning magic that distinguishes good quality dishes from those of bad quality using only photos as a single input?”</strong>. In a nutshell, business wanted this:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:44%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 342px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*U1DYB3vbT3MMfIMa0l11vQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*U1DYB3vbT3MMfIMa0l11vQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*U1DYB3vbT3MMfIMa0l11vQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*U1DYB3vbT3MMfIMa0l11vQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*U1DYB3vbT3MMfIMa0l11vQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*U1DYB3vbT3MMfIMa0l11vQ.png 1100w, https://miro.medium.com/v2/resize:fit:684/format:webp/1*U1DYB3vbT3MMfIMa0l11vQ.png 684w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 342px" srcset="https://miro.medium.com/v2/resize:fit:640/1*U1DYB3vbT3MMfIMa0l11vQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*U1DYB3vbT3MMfIMa0l11vQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*U1DYB3vbT3MMfIMa0l11vQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*U1DYB3vbT3MMfIMa0l11vQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*U1DYB3vbT3MMfIMa0l11vQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*U1DYB3vbT3MMfIMa0l11vQ.png 1100w, https://miro.medium.com/v2/resize:fit:684/1*U1DYB3vbT3MMfIMa0l11vQ.png 684w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/342/1*U1DYB3vbT3MMfIMa0l11vQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">When a business looks at ML through pink glasses, they imagine this</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This is an example of an ill-imposed problem: it is impossible to figure out whether a solution exists and that the solution is unique and stable as the very definition of done is extremely vague (let alone the implementation). While this post is not about efficient communications or project management, one remark is necessary: <strong>you should never commit to badly scoped projects.</strong> One of the proven ways to cope with this ambiguity is to build up a well-defined prototype first and to structure the rest of the task afterwards. That was the strategy we took.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Problem Definition</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In my prototype, I focused on a single item in the menu — an omelette—and constructed an extensible data pipeline that outputs the perceived “quality” of the omelette. It can be summarized as this:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><strong>Problem type:</strong> multiclass classification, 6 discrete classes of quality: <code>[good, broken_yolk, overroasted, two_eggs, four_eggs, misplaced_pieces]</code>.</li><li class="ff3" style="font-size:22px;"><strong>Dataset:</strong> 351 manually collected DSLR camera photos of various omelettes. Train/val/test: 139/32/180 shuffled photos.</li><li class="ff3" style="font-size:22px;"><strong>Labels:</strong> a subjective quality class is assigned to each of the photos.</li><li class="ff3" style="font-size:22px;"><strong>Metric:</strong> categorical cross-entropy.</li><li class="ff3" style="font-size:22px;"><strong>Necessary domain knowledge:</strong> a “good” omelette is defined as an omelette of three eggs with unbroken yolks, some bacon, no burnt pieces, a single piece of parsley in the center. Also, its composition should be visually correct, e.g., there should be no scattered pieces.</li><li class="ff3" style="font-size:22px;"><strong>Definition of done:</strong> best possible cross-entropy on the test set after two weeks of prototyping.</li><li class="ff3" style="font-size:22px;"><mark>Results visualization: </mark><mark>t-SNE of the test set’s low-dimensional data representation.</mark></li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Lu59Q4iGBT7qg4u-hKt5Uw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Lu59Q4iGBT7qg4u-hKt5Uw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Lu59Q4iGBT7qg4u-hKt5Uw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Lu59Q4iGBT7qg4u-hKt5Uw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Lu59Q4iGBT7qg4u-hKt5Uw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Lu59Q4iGBT7qg4u-hKt5Uw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lu59Q4iGBT7qg4u-hKt5Uw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Lu59Q4iGBT7qg4u-hKt5Uw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*Lu59Q4iGBT7qg4u-hKt5Uw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*Lu59Q4iGBT7qg4u-hKt5Uw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*Lu59Q4iGBT7qg4u-hKt5Uw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*Lu59Q4iGBT7qg4u-hKt5Uw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*Lu59Q4iGBT7qg4u-hKt5Uw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*Lu59Q4iGBT7qg4u-hKt5Uw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*Lu59Q4iGBT7qg4u-hKt5Uw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Input images as they are captured with camera</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The main goal was to <em>obtain and combine extracted signals</em> using a neural network classifier and to let the classifier make its softmax predictions regarding the class probabilities of the items in the test set. Such a goal would render this prototype viable and applicable for later usage. Here are the signals we extracted and found useful:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Key ingredients masks (Mask R-CNN): <em>Signal #1.</em></li><li class="ff3" style="font-size:22px;">Key ingredients counts grouped by each ingredient (which is basically a matrix of distinct ingredients counts): <em>Signal #2.</em></li><li class="ff3" style="font-size:22px;">RGB crops of plates with omelettes with background removed. For simplicity, I decided not to add them to the model for now. It might have been the most obvious signal: just train a ConvNet classifier on these images using some fancy loss function and take a L2 distance from a chosen paragon image to the current one in a low-dimensional embedding. Unfortunately, I had no chance to test this hypothesis as I was limited to only 139 samples in the training set.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">General 50K Pipeline Overview</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I am omitting several important stages such as data discovery and exploratory analysis, baseline solutions and active labeling (which is my own fancy name for a semi-supervised instance annotation inspired by <a href="https://www.youtube.com/watch?v=S1UUR4FlJ84" target="_self">Polygon-RNN demo video</a>) pipeline for Mask R-CNN (more on this in later posts). To embrace the overall pipeline, here its 50K feet view:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*b9vQhMcGUA5hq_SwYzAKUQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*b9vQhMcGUA5hq_SwYzAKUQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*b9vQhMcGUA5hq_SwYzAKUQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*b9vQhMcGUA5hq_SwYzAKUQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*b9vQhMcGUA5hq_SwYzAKUQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*b9vQhMcGUA5hq_SwYzAKUQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b9vQhMcGUA5hq_SwYzAKUQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*b9vQhMcGUA5hq_SwYzAKUQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*b9vQhMcGUA5hq_SwYzAKUQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*b9vQhMcGUA5hq_SwYzAKUQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*b9vQhMcGUA5hq_SwYzAKUQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*b9vQhMcGUA5hq_SwYzAKUQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*b9vQhMcGUA5hq_SwYzAKUQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*b9vQhMcGUA5hq_SwYzAKUQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*b9vQhMcGUA5hq_SwYzAKUQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">We are mostly interested in the Mask R-CNN and classification stages of the pipeline</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For the rest of this post, I’m focusing on three stages: [1] Mask R-CNN for ingredient masks inference, [2] Keras-based ConvNet classifier, [3] results visualization with t-SNE.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Stage 1: Mask R-CNN and Masks Inference</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Mask R-CNN (MRCNN) got a lot of coverage and hype recently. Starting from the original <a href="https://arxiv.org/abs/1703.06870" target="_self">Facebook’s paper</a> and moving forward to the <a href="https://www.kaggle.com/c/data-science-bowl-2018" target="_self">Data Science Bowl 2018</a> on Kaggle, Mask R-CNN proved itself as a powerful architecture for instance segmentation (object aware segmentation). The Keras-based <a href="https://github.com/matterport/Mask_RCNN" target="_self">Matterport’s implementation</a> of MRCNN that I used is an absolute pleasure to work with. The code is well-structured, nicely documented and works right out of the box, though slower than I expected.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">MRCNN in one paragraph:</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4">MRCNN consists of two definitive parts: the <strong>backbone network</strong><em> </em>and the <strong>network head</strong><em> </em>thus inheriting the Faster R-CNN architecture. The convolutional backbone network, which is based either on Feature Pyramid Network (FPN) or ResNet101, works as features extractor over the whole image. On top of this lies Region Proposal Network (RPN) that samples multiscale RoI (regions of interest) for the head. The network head does bounding box recognition and mask prediction that is applied to each RoI. In between, RoIAlign layer finely aligns RPN-extracted multiscale features with the input.</p></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:61%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 461px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*ThnTPZE0mtebI--OtspEFA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*ThnTPZE0mtebI--OtspEFA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*ThnTPZE0mtebI--OtspEFA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*ThnTPZE0mtebI--OtspEFA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*ThnTPZE0mtebI--OtspEFA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ThnTPZE0mtebI--OtspEFA.png 1100w, https://miro.medium.com/v2/resize:fit:922/format:webp/1*ThnTPZE0mtebI--OtspEFA.png 922w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 461px" srcset="https://miro.medium.com/v2/resize:fit:640/1*ThnTPZE0mtebI--OtspEFA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*ThnTPZE0mtebI--OtspEFA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*ThnTPZE0mtebI--OtspEFA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*ThnTPZE0mtebI--OtspEFA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*ThnTPZE0mtebI--OtspEFA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*ThnTPZE0mtebI--OtspEFA.png 1100w, https://miro.medium.com/v2/resize:fit:922/1*ThnTPZE0mtebI--OtspEFA.png 922w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/461/1*ThnTPZE0mtebI--OtspEFA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">MRCNN framework as presented in the original paper</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>For practical applications, especially for prototyping, having a pretrained ConvNet is critical.</strong> In many real life scenarios, a data scientist has a very limited annotated dataset, or even has no annotations whatsoever. In contrast, <em>ConvNets require large labeled datasets to converge</em> (e.g., ImageNet dataset contains 1.2M labeled images). This is where <a href="http://cs231n.github.io/transfer-learning/" target="_self">transfer learning</a> helps: one strategy is to freeze the weights of convolutional layers and only retrain the classifier. Conv layers weights freeze is important for a small dataset in order to save the model from overfitting.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here is the sample of what I got after a single epoch of training:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*3w9Jo-lcflGFsaYaWr80kg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*3w9Jo-lcflGFsaYaWr80kg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*3w9Jo-lcflGFsaYaWr80kg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*3w9Jo-lcflGFsaYaWr80kg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*3w9Jo-lcflGFsaYaWr80kg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*3w9Jo-lcflGFsaYaWr80kg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3w9Jo-lcflGFsaYaWr80kg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*3w9Jo-lcflGFsaYaWr80kg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*3w9Jo-lcflGFsaYaWr80kg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*3w9Jo-lcflGFsaYaWr80kg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*3w9Jo-lcflGFsaYaWr80kg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*3w9Jo-lcflGFsaYaWr80kg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*3w9Jo-lcflGFsaYaWr80kg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*3w9Jo-lcflGFsaYaWr80kg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*3w9Jo-lcflGFsaYaWr80kg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">The result of instance segmentation: all key ingredients are detected</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The next stage (<em>Process Inferenced Data for Classifier</em> in my 50K pipeline view) is to crop the image part that contains the plate and to extract 2D binary masks for each ingredient from that crop:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*kzQ7iRL-jHZmugl6E0xOwA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*kzQ7iRL-jHZmugl6E0xOwA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*kzQ7iRL-jHZmugl6E0xOwA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*kzQ7iRL-jHZmugl6E0xOwA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*kzQ7iRL-jHZmugl6E0xOwA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*kzQ7iRL-jHZmugl6E0xOwA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kzQ7iRL-jHZmugl6E0xOwA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*kzQ7iRL-jHZmugl6E0xOwA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*kzQ7iRL-jHZmugl6E0xOwA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*kzQ7iRL-jHZmugl6E0xOwA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*kzQ7iRL-jHZmugl6E0xOwA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*kzQ7iRL-jHZmugl6E0xOwA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*kzQ7iRL-jHZmugl6E0xOwA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*kzQ7iRL-jHZmugl6E0xOwA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*kzQ7iRL-jHZmugl6E0xOwA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Cropped image with the target dish and its key ingredients as binary masks</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">These binary masks are then combined into a 8-channel image (as I defined 8 mask classes for MRCNN)—that is my <em>Signal #1</em>:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:56%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 424px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*aiacmxaGyzR0JHzlxDOJXg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*aiacmxaGyzR0JHzlxDOJXg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*aiacmxaGyzR0JHzlxDOJXg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*aiacmxaGyzR0JHzlxDOJXg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*aiacmxaGyzR0JHzlxDOJXg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*aiacmxaGyzR0JHzlxDOJXg.png 1100w, https://miro.medium.com/v2/resize:fit:848/format:webp/1*aiacmxaGyzR0JHzlxDOJXg.png 848w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 424px" srcset="https://miro.medium.com/v2/resize:fit:640/1*aiacmxaGyzR0JHzlxDOJXg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*aiacmxaGyzR0JHzlxDOJXg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*aiacmxaGyzR0JHzlxDOJXg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*aiacmxaGyzR0JHzlxDOJXg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*aiacmxaGyzR0JHzlxDOJXg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*aiacmxaGyzR0JHzlxDOJXg.png 1100w, https://miro.medium.com/v2/resize:fit:848/1*aiacmxaGyzR0JHzlxDOJXg.png 848w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/424/1*aiacmxaGyzR0JHzlxDOJXg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Signal #1: 8-channel image composed of binary masks. Colors are just for better visualization</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For the <em>Signal #2</em>, I calculated the counts of each ingredient from the MRCNN inference and just packed them into a feature vector for each crop.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Stage 2: Keras-based ConvNet classifier</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The CNN classifier has been implemented from scratch using Keras. The goal I had in mind was to fuse several signals (<em>Signal #1</em> and <em>Signal #2</em>, as well as to add more data in the future) and let the network make its predictions regarding the quality class of the dish. The following architecture is experimental and it is far from ideal:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Ih8hl4sBpkVG3IQRUvv_8A.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Ih8hl4sBpkVG3IQRUvv_8A.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Ih8hl4sBpkVG3IQRUvv_8A.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Ih8hl4sBpkVG3IQRUvv_8A.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Ih8hl4sBpkVG3IQRUvv_8A.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Ih8hl4sBpkVG3IQRUvv_8A.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ih8hl4sBpkVG3IQRUvv_8A.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Ih8hl4sBpkVG3IQRUvv_8A.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*Ih8hl4sBpkVG3IQRUvv_8A.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*Ih8hl4sBpkVG3IQRUvv_8A.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*Ih8hl4sBpkVG3IQRUvv_8A.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*Ih8hl4sBpkVG3IQRUvv_8A.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*Ih8hl4sBpkVG3IQRUvv_8A.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*Ih8hl4sBpkVG3IQRUvv_8A.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*Ih8hl4sBpkVG3IQRUvv_8A.jpeg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Several observations and comments about the classifier architecture:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><strong>Multiscale convolutions module</strong>: initially I selected a 5x5 kernel for convolutional layers, but that decision got me only to a satisfactory score. A better result has been achieved by <code>AveragePooling2D</code> of several convolutional layers with various kernels: 3x3, 5x5, 7x7, 11x11. Additional 1x1 convolutional layer has been added in front of each layer to reduce dimensionality. This component slightly resembles the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf" target="_self">Inception module</a>, though I restrained myself from building a deep network.</li><li class="ff3" style="font-size:22px;"><strong>Larger kernels:</strong> I used larger kernel sizes as the larger scale features could be easily extracted from the input image (which itself could be viewed as an activation layer with 8 filters—each ingredient’s binary mask is basically a filter).</li><li class="ff3" style="font-size:22px;"><strong>Signals fusion:</strong> my naive implementation just used a single layer of non-linearity to merge two feature sets: processed binary masks (Signal #1) and ingredients counts (Signal #2). Despite its naivety, adding Signal #2 provided a nice boost to the score (improved cross-entropy from <code>0.8</code> to <code>[0.7, 0.72]</code>)</li><li class="ff3" style="font-size:22px;"><strong>Logits:</strong> in terms of TensorFlow, this is the layer on which <code>tf.nn.softmax_cross_entropy_with_logits</code> is applied to calculate the batch loss.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Stage 3: Results visualization with t-SNE</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For test set results visualization I used t-SNE, a manifold learning technique for data visualization. t-SNE minimizes KL-divergence between joint probabilities of a low-dimensional embedded data points and original high-dimensional data using quite a notable non-convex loss function. You should definitely read the <a href="https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf" target="_self">original paper</a>, it is extremely informative and well-written.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To visualize the test set classification results, I inferenced the test set images, extracted the logits layer of the classifier and applied t-SNE to this dataset. Though I should have played with different perplexity values, the results look pretty nice anyway. Animated GIF:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*OXE2tG1b2pcJqJcizGqgFA.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*OXE2tG1b2pcJqJcizGqgFA.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*OXE2tG1b2pcJqJcizGqgFA.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*OXE2tG1b2pcJqJcizGqgFA.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*OXE2tG1b2pcJqJcizGqgFA.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*OXE2tG1b2pcJqJcizGqgFA.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*OXE2tG1b2pcJqJcizGqgFA.gif 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*OXE2tG1b2pcJqJcizGqgFA.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*OXE2tG1b2pcJqJcizGqgFA.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*OXE2tG1b2pcJqJcizGqgFA.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*OXE2tG1b2pcJqJcizGqgFA.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*OXE2tG1b2pcJqJcizGqgFA.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*OXE2tG1b2pcJqJcizGqgFA.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*OXE2tG1b2pcJqJcizGqgFA.gif 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*OXE2tG1b2pcJqJcizGqgFA.gif"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">t-SNE of the test set predictions by the classifier</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">While not perfect, such an approach works indeed. There is, though, lots of improvements to be made:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><strong>More data.</strong> ConvNets require lots of data, while I had only 139 samples for training. Tricks like data augmentation work great (I used a <a href="https://en.wikipedia.org/wiki/Dihedral_group" target="_self">D4, or dihedral, symmetry group</a> augmentation that resulted in 2K+ augmented images), but more real data is critical for good performance.</li><li class="ff3" style="font-size:22px;"><strong>Suitable loss function.</strong> For simplicity I used categorical cross-entropy loss that works out-of-box. I would switch to a more suitable loss function, the one that better leverages intra-class variance. A good option to start with might have been a triplet loss (see <a href="https://arxiv.org/pdf/1703.07737.pdf" target="_self">FaceNet paper</a> for details).</li><li class="ff3" style="font-size:22px;"><strong>Better overall classifier architecture.</strong> The current classifier is basically a prototype which goal is to interpret input binary masks and to combine multiple feature sets into a single inference pipeline.</li><li class="ff3" style="font-size:22px;"><strong>Better labeling.</strong> I was quite sloppy on manual image labeling (6 classes of quality): the classifier outperformed myself for a dozen of test set images!</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Outro and introspection.</strong> It is very common in practice (and we should stop denying this fact) that businesses have no data, no annotations and no clear and well-articulated technical task to be accomplished. This is a good thing (otherwise, why would they need you?): it is your job to have the tools, enough pieces of multi-GPU hardware, a combination of both business and technical expertise, pretrained models and everything else needed to bring value to business.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Start small: a working prototype that can be built from LEGO blocks of code can boost the productivity of further conversations—and it is your job as a data scientist to suggest such an approach to business.</p></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>