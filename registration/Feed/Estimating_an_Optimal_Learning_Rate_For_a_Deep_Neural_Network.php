<!DOCTYPE html>
                <html>
                <head>
                    <title>Estimating an Optimal Learning Rate For a Deep Neural Network</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://medium.com/@surmenok?source=post_page-----ce32f2556ce0--------------------------------">Author : Pavel Surmenok</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>Estimating an Optimal Learning Rate For a Deep Neural Network</h3></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The learning rate is one of the most important hyper-parameters to tune for training deep neural networks.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In this post, I’m describing a simple and powerful way to find a reasonable learning rate that I learned from <a href="http://www.fast.ai/" target="_self">fast.ai Deep Learning course</a>. I’m taking the new version of the course in person at <a href="https://www.usfca.edu/data-institute/certificates/deep-learning-part-one" target="_self">University of San Francisco</a>. It’s not available to the general public yet, but will be at the end of the year at <a href="http://course.fast.ai/" target="_self">course.fast.ai</a> (which currently has the last year’s version).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">How does learning rate impact training?</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Deep learning models are typically trained by a stochastic gradient descent optimizer. There are many variations of stochastic gradient descent: Adam, RMSProp, Adagrad, etc. All of them let you set the learning rate. This parameter tells the optimizer how far to move the weights in the direction opposite of the gradient for a mini-batch.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If the learning rate is low, then training is more reliable, but optimization will take a lot of time because steps towards the minimum of the loss function are tiny.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If the learning rate is high, then training may not converge or even diverge. Weight changes can be so big that the optimizer overshoots the minimum and makes the loss worse.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The training should start from a relatively large learning rate because, in the beginning, random weights are far from optimal, and then the learning rate can decrease during training to allow more fine-grained weight updates.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">There are multiple ways to select a good starting point for the learning rate. A naive approach is to try a few different values and see which one gives you the best loss without sacrificing speed of training. We might start with a large value like 0.1, then try exponentially lower values: 0.01, 0.001, etc. When we start training with a large learning rate, the loss doesn’t improve and probably even grows while we run the first few iterations of training. When training with a smaller learning rate, at some point the value of the loss function starts decreasing in the first few iterations. This learning rate is the maximum we can use, any higher value doesn’t let the training converge. Even this value is too high: it won’t be good enough to train for multiple epochs because over time the network will require more fine-grained weight updates. Therefore, a reasonable learning rate to start training from will be probably 1–2 orders of magnitude lower.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">There must be a smarter way</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Leslie N. Smith describes a powerful technique to select a range of learning rates for a neural network in section 3.3 of the 2015 paper “<a href="https://arxiv.org/abs/1506.01186" target="_self">Cyclical Learning Rates for Training Neural Networks</a>” .</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The trick is to train a network starting from a low learning rate and increase the learning rate exponentially for every batch.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:51%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 389px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*zgm3iy7aD4ZsXLiva0xtFg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*zgm3iy7aD4ZsXLiva0xtFg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*zgm3iy7aD4ZsXLiva0xtFg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*zgm3iy7aD4ZsXLiva0xtFg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*zgm3iy7aD4ZsXLiva0xtFg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*zgm3iy7aD4ZsXLiva0xtFg.png 1100w, https://miro.medium.com/v2/resize:fit:778/format:webp/1*zgm3iy7aD4ZsXLiva0xtFg.png 778w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 389px" srcset="https://miro.medium.com/v2/resize:fit:640/1*zgm3iy7aD4ZsXLiva0xtFg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*zgm3iy7aD4ZsXLiva0xtFg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*zgm3iy7aD4ZsXLiva0xtFg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*zgm3iy7aD4ZsXLiva0xtFg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*zgm3iy7aD4ZsXLiva0xtFg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*zgm3iy7aD4ZsXLiva0xtFg.png 1100w, https://miro.medium.com/v2/resize:fit:778/1*zgm3iy7aD4ZsXLiva0xtFg.png 778w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/389/1*zgm3iy7aD4ZsXLiva0xtFg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Learning rate increases after each mini-batch</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Record the learning rate and training loss for every batch. Then, plot the loss and the learning rate. Typically, it looks like this:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:51%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 389px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*HVj_4LWemjvOWv-cQO9y9g.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*HVj_4LWemjvOWv-cQO9y9g.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*HVj_4LWemjvOWv-cQO9y9g.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*HVj_4LWemjvOWv-cQO9y9g.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*HVj_4LWemjvOWv-cQO9y9g.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*HVj_4LWemjvOWv-cQO9y9g.png 1100w, https://miro.medium.com/v2/resize:fit:778/format:webp/1*HVj_4LWemjvOWv-cQO9y9g.png 778w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 389px" srcset="https://miro.medium.com/v2/resize:fit:640/1*HVj_4LWemjvOWv-cQO9y9g.png 640w, https://miro.medium.com/v2/resize:fit:720/1*HVj_4LWemjvOWv-cQO9y9g.png 720w, https://miro.medium.com/v2/resize:fit:750/1*HVj_4LWemjvOWv-cQO9y9g.png 750w, https://miro.medium.com/v2/resize:fit:786/1*HVj_4LWemjvOWv-cQO9y9g.png 786w, https://miro.medium.com/v2/resize:fit:828/1*HVj_4LWemjvOWv-cQO9y9g.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*HVj_4LWemjvOWv-cQO9y9g.png 1100w, https://miro.medium.com/v2/resize:fit:778/1*HVj_4LWemjvOWv-cQO9y9g.png 778w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/389/1*HVj_4LWemjvOWv-cQO9y9g.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">The loss decreases in the beginning, then the training process starts diverging</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">First, with low learning rates, the loss improves slowly, then training accelerates until the learning rate becomes too large and loss goes up: the training process diverges.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><mark>We need to select a point on the graph with the fastest decrease in the loss. In this example, the loss function decreases fast when the learning rate is between 0.001 and 0.01.</mark></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Another way to look at these numbers is calculating the rate of change of the loss (a derivative of the loss function with respect to iteration number), then plot the change rate on the y-axis and the learning rate on the x-axis.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:55%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 417px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*eYewkhRqRyGg7UsNNaX0Hg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*eYewkhRqRyGg7UsNNaX0Hg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*eYewkhRqRyGg7UsNNaX0Hg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*eYewkhRqRyGg7UsNNaX0Hg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*eYewkhRqRyGg7UsNNaX0Hg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*eYewkhRqRyGg7UsNNaX0Hg.png 1100w, https://miro.medium.com/v2/resize:fit:834/format:webp/1*eYewkhRqRyGg7UsNNaX0Hg.png 834w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 417px" srcset="https://miro.medium.com/v2/resize:fit:640/1*eYewkhRqRyGg7UsNNaX0Hg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*eYewkhRqRyGg7UsNNaX0Hg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*eYewkhRqRyGg7UsNNaX0Hg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*eYewkhRqRyGg7UsNNaX0Hg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*eYewkhRqRyGg7UsNNaX0Hg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*eYewkhRqRyGg7UsNNaX0Hg.png 1100w, https://miro.medium.com/v2/resize:fit:834/1*eYewkhRqRyGg7UsNNaX0Hg.png 834w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/417/1*eYewkhRqRyGg7UsNNaX0Hg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Rate of change of the loss</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">It looks too noisy, let’s smooth it out using simple moving average.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:55%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 417px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*87mKq_XomYyJE29l91K0dw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*87mKq_XomYyJE29l91K0dw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*87mKq_XomYyJE29l91K0dw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*87mKq_XomYyJE29l91K0dw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*87mKq_XomYyJE29l91K0dw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*87mKq_XomYyJE29l91K0dw.png 1100w, https://miro.medium.com/v2/resize:fit:834/format:webp/1*87mKq_XomYyJE29l91K0dw.png 834w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 417px" srcset="https://miro.medium.com/v2/resize:fit:640/1*87mKq_XomYyJE29l91K0dw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*87mKq_XomYyJE29l91K0dw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*87mKq_XomYyJE29l91K0dw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*87mKq_XomYyJE29l91K0dw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*87mKq_XomYyJE29l91K0dw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*87mKq_XomYyJE29l91K0dw.png 1100w, https://miro.medium.com/v2/resize:fit:834/1*87mKq_XomYyJE29l91K0dw.png 834w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/417/1*87mKq_XomYyJE29l91K0dw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Rate of change of the loss, simple moving average</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This looks better. On this graph, we need to find the minimum. It is close to lr=0.01.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Implementation</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Jeremy Howard and his team at <a href="https://www.usfca.edu/data-institute" target="_self">USF Data Institute</a> developed <a href="https://github.com/fastai/fastai" target="_self">fast.ai</a>, a deep learning library that is a high-level abstraction on top of PyTorch. It’s an easy to use and yet powerful toolset for training state of the art deep learning models. Jeremy uses the library in the latest version of the Deep Learning course (<a href="http://www.fast.ai/" target="_self">fast.ai</a>).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The library provides an implementation of the learning rate finder. You need just two lines of code to plot the loss over learning rates for your model:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre># learn is an instance of Learner class or one of derived classes like ConvLearner
learn.lr_find()
learn.sched.plot_lr()</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The library doesn’t have the code to plot the rate of change of the loss function, but it’s trivial to calculate:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre>def plot_loss_change(sched, sma=1, n_skip=20, y_lim=(-0.01,0.01)):
    """
    Plots rate of change of the loss function.
    Parameters:
        sched - learning rate scheduler, an instance of LR_Finder class.
        sma - number of batches for simple moving average to smooth out the curve.
        n_skip - number of batches to skip on the left.
        y_lim - limits for the y axis.
    """
    derivatives = [0] * (sma + 1)
    for i in range(1 + sma, len(learn.sched.lrs)):
        derivative = (learn.sched.losses[i] - learn.sched.losses[i - sma]) / sma
        derivatives.append(derivative)
        
    plt.ylabel("d/loss")
    plt.xlabel("learning rate (log scale)")
    plt.plot(learn.sched.lrs[n_skip:], derivatives[n_skip:])
    plt.xscale('log')
    plt.ylim(y_lim)

plot_loss_change(learn.sched, sma=20)
</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Note that selecting a learning rate once, before training, is not enough. The optimal learning rate decreases while training. You can rerun the same learning rate search procedure periodically to find the learning rate at a later point in the training process.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Implementing the method using other libraries</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I haven’t seen ready to use implementations of this learning rate search method for other libraries like Keras, but it should be trivial to write. Just run the training multiple times, one mini-batch at a time. Increase the learning rate after each mini-batch by multiplying it by a small constant. Stop the procedure when the loss gets a lot higher than the previously observed best value (e.g., when current loss &gt; best loss * 4).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">There is more to it</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Selecting a starting value for the learning rate is just one part of the problem. Another thing to optimize is the learning schedule: how to change the learning rate during training. The conventional wisdom is that the learning rate should decrease over time, and there are multiple ways to set this up: step-wise learning rate annealing when the loss stops improving, exponential learning rate decay, cosine annealing, etc.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><a href="https://arxiv.org/abs/1506.01186" target="_self">The paper</a> that I referenced above describes a novel way to change the learning rate cyclically. This method improves performance of convolutional neural networks on a variety of image classification tasks.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Please send me a message if you know other interesting tips and tricks for training deep neural networks.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">See also:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><div class="sketchy"><a href="https://hackernoon.com/fast-ai-what-i-learned-from-lessons-1-3-b10f9958e3ff"><h2 style="color:blueviolet; font-family:Arial, Helvetica, sans-serif; font-size:25px;">Fast.ai: What I Learned from Lessons 1–3</h2><h3 style="color:rgb(45, 34, 54); font-family:Arial, Helvetica, sans-serif; font-size:20px;">Fast.ai is a great deep learning course for those who prefer to learn by doing. Unlike other courses, here you will…</h3><p>hackernoon.com</p></a></div></div></div></div></section><br><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><div class="sketchy"><a href="https://medium.com/@surmenok/best-sources-of-deep-learning-news-fbc98815bad3"><h2 style="color:blueviolet; font-family:Arial, Helvetica, sans-serif; font-size:25px;">Best Sources of Deep Learning News</h2><h3 style="color:rgb(45, 34, 54); font-family:Arial, Helvetica, sans-serif; font-size:20px;">The field of deep learning is very active, arguably there are one or two breakthroughs every week. Research papers…</h3><p>medium.com</p></a></div></div></div></div></section><br><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><div class="sketchy"><a href="https://becominghuman.ai/jeff-deans-talk-on-large-scale-deep-learning-171fb8c8ac57"><h2 style="color:blueviolet; font-family:Arial, Helvetica, sans-serif; font-size:25px;">Jeff Dean’s Talk on Large-Scale Deep Learning</h2><h3 style="color:rgb(45, 34, 54); font-family:Arial, Helvetica, sans-serif; font-size:20px;">Jeff Dean is a Google Senior Fellow. He leads the Google Brain project. He spoke at Y Combinator in August 2017. The…</h3><p>becominghuman.ai</p></a></div></div></div></div></section><br><?php include_once 'Elemental/footer.php'; ?>