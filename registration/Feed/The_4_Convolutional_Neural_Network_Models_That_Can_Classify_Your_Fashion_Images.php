<!DOCTYPE html>
                <html>
                <head>
                    <title>The 4 Convolutional Neural Network Models That Can Classify Your Fashion Images</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://towardsdatascience.com/the-4-convolutional-neural-network-models-that-can-classify-your-fashion-images-9fe7f3e5399d"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://le-james94.medium.com/?source=post_page-----9fe7f3e5399d--------------------------------">Author : James Le</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>The 4 Convolutional Neural Network Models That Can Classify Your Fashion Images</h3></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*zzRM4CrMs25WiCE1j_phlw.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*zzRM4CrMs25WiCE1j_phlw.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*zzRM4CrMs25WiCE1j_phlw.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*zzRM4CrMs25WiCE1j_phlw.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*zzRM4CrMs25WiCE1j_phlw.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*zzRM4CrMs25WiCE1j_phlw.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zzRM4CrMs25WiCE1j_phlw.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*zzRM4CrMs25WiCE1j_phlw.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*zzRM4CrMs25WiCE1j_phlw.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*zzRM4CrMs25WiCE1j_phlw.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*zzRM4CrMs25WiCE1j_phlw.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*zzRM4CrMs25WiCE1j_phlw.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*zzRM4CrMs25WiCE1j_phlw.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*zzRM4CrMs25WiCE1j_phlw.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*zzRM4CrMs25WiCE1j_phlw.jpeg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Clothes shopping is a taxing experience. My eyes get bombarded with too much information. Sales, coupons, colors, toddlers, flashing lights, and crowded aisles are just a few examples of all the signals forwarded to my visual cortex, whether or not I actively try to pay attention. The visual system absorbs an abundance of information. Should I go for that H&M khaki pants? Is that a Nike tank top? What color are those Adidas sneakers?</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Can a computer automatically detect pictures of shirts, pants, dresses, and sneakers? It turns out that accurately classifying images of fashion items is surprisingly straight-forward to do, given quality training data to start from. In this tutorial, we’ll walk through building a machine learning model for recognizing images of fashion objects using the Fashion-MNIST dataset. We’ll walk through how to train a model, design the input and output for category classifications, and finally display the accuracy results for each model.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><strong>Image Classification</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The problem of Image Classification goes like this: Given a set of images that are all labeled with a single category, we are asked to predict these categories for a novel set of test images and measure the accuracy of the predictions. There are a variety of challenges associated with this task, including viewpoint variation, scale variation, intra-class variation, image deformation, image occlusion, illumination conditions, background clutter etc.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">How might we go about writing an algorithm that can classify images into distinct categories? Computer Vision researchers have come up with a data-driven approach to solve this. Instead of trying to specify what every one of the image categories of interest look like directly in code, they provide the computer with many examples of each image class and then develop learning algorithms that look at these examples and learn about the visual appearance of each class. In other words, they first accumulate a training dataset of labeled images, then feed it to the computer in order for it to get familiar with the data.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*4-yegWD8qQeTCtGnHW-7GQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*4-yegWD8qQeTCtGnHW-7GQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*4-yegWD8qQeTCtGnHW-7GQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*4-yegWD8qQeTCtGnHW-7GQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*4-yegWD8qQeTCtGnHW-7GQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*4-yegWD8qQeTCtGnHW-7GQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4-yegWD8qQeTCtGnHW-7GQ.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*4-yegWD8qQeTCtGnHW-7GQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*4-yegWD8qQeTCtGnHW-7GQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*4-yegWD8qQeTCtGnHW-7GQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*4-yegWD8qQeTCtGnHW-7GQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*4-yegWD8qQeTCtGnHW-7GQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*4-yegWD8qQeTCtGnHW-7GQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*4-yegWD8qQeTCtGnHW-7GQ.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*4-yegWD8qQeTCtGnHW-7GQ.jpeg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Given that fact, the complete image classification pipeline can be formalized as follows:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Our input is a training dataset that consists of <em>N</em> images, each labeled with one of <em>K</em> different classes.</li><li class="ff3" style="font-size:22px;">Then, we use this training set to train a classifier to learn what every one of the classes looks like.</li><li class="ff3" style="font-size:22px;">In the end, we evaluate the quality of the classifier by asking it to predict labels for a new set of images that it has never seen before. We will then compare the true labels of these images to the ones predicted by the classifier.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><strong>Convolutional Neural Networks</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Convolutional Neural Networks (CNNs)</strong> is the most popular neural network model being used for image classification problem. The big idea behind CNNs is that a local understanding of an image is good enough. The practical benefit is that having fewer parameters greatly improves the time it takes to learn as well as reduces the amount of data required to train the model. Instead of a fully connected network of weights from each pixel, a CNN has just enough weights to look at a small patch of the image. It’s like reading a book by using a magnifying glass; eventually, you read the whole page, but you look at only a small patch of the page at any given time.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:54%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 408px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*jNOmERWFNSDugvcvykMfQQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*jNOmERWFNSDugvcvykMfQQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*jNOmERWFNSDugvcvykMfQQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*jNOmERWFNSDugvcvykMfQQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*jNOmERWFNSDugvcvykMfQQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*jNOmERWFNSDugvcvykMfQQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:816/format:webp/1*jNOmERWFNSDugvcvykMfQQ.jpeg 816w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 408px" srcset="https://miro.medium.com/v2/resize:fit:640/1*jNOmERWFNSDugvcvykMfQQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*jNOmERWFNSDugvcvykMfQQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*jNOmERWFNSDugvcvykMfQQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*jNOmERWFNSDugvcvykMfQQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*jNOmERWFNSDugvcvykMfQQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*jNOmERWFNSDugvcvykMfQQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:816/1*jNOmERWFNSDugvcvykMfQQ.jpeg 816w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/408/1*jNOmERWFNSDugvcvykMfQQ.jpeg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Consider a 256 x 256 image. CNN can efficiently scan it chunk by chunk — say, a 5 × 5 window. The 5 × 5 window slides along the image (usually left to right, and top to bottom), as shown below. How “quickly” it slides is called its <strong>stride length</strong>. For example, a stride length of 2 means the 5 × 5 sliding window moves by 2 pixels at a time until it spans the entire image.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">A <strong>convolution</strong> is a weighted sum of the pixel values of the image, as the window slides across the whole image. Turns out, this convolution process throughout an image with a weight matrix produces another image (of the same size, depending on the convention). Convolving is the process of applying a convolution.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The sliding-window shenanigans happen in the <strong>convolution layer</strong> of the neural network. A typical CNN has multiple convolution layers. Each convolutional layer typically generates many alternate convolutions, so the weight matrix is a tensor of 5 × 5 × n, where n is the number of convolutions.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">As an example, let’s say an image goes through a convolution layer on a weight matrix of 5 × 5 × 64. It generates 64 convolutions by sliding a 5 × 5 window. Therefore, this model has 5 × 5 × 64 (= 1,600) parameters, which is remarkably fewer parameters than a fully connected network, 256 × 256 (= 65,536).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><mark>The beauty of the CNN is that the number of parameters is independent of the size of the original image. You can run the same CNN on a 300 × 300 image, and the number of parameters won’t change in the convolution layer.</mark></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><strong>Data Augmentation</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Image classification research datasets are typically very large. Nevertheless, data augmentation is often used in order to improve generalisation properties. Typically, random cropping of rescaled images together with random horizontal ﬂipping and random RGB colour and brightness shifts are used. Different schemes exist for rescaling and cropping the images (i.e. single scale vs. multi scale training). Multi-crop evaluation during test time is also often used, although computationally more expensive and with limited performance improvement. Note that the goal of the random rescaling and cropping is to learn the important features of each object at different scales and positions. Keras does not implement all of these data augmentation techniques out of the box, but they can easily implemented through the preprocessing function of the ImageDataGenerator modules.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*eepf79k7yQs-IQHYQd-xxA.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*eepf79k7yQs-IQHYQd-xxA.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*eepf79k7yQs-IQHYQd-xxA.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*eepf79k7yQs-IQHYQd-xxA.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*eepf79k7yQs-IQHYQd-xxA.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*eepf79k7yQs-IQHYQd-xxA.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eepf79k7yQs-IQHYQd-xxA.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*eepf79k7yQs-IQHYQd-xxA.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*eepf79k7yQs-IQHYQd-xxA.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*eepf79k7yQs-IQHYQd-xxA.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*eepf79k7yQs-IQHYQd-xxA.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*eepf79k7yQs-IQHYQd-xxA.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*eepf79k7yQs-IQHYQd-xxA.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*eepf79k7yQs-IQHYQd-xxA.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*eepf79k7yQs-IQHYQd-xxA.jpeg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><strong>Fashion MNIST Dataset</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Recently, Zalando research published a new dataset, which is very similar to the well known <a href="https://yann.lecun.com/exdb/mnist/" target="_self">MNIST database of handwritten digits</a>. The dataset is designed for machine learning classification tasks and contains in total 60 000 training and 10 000 test images (gray scale) with each 28x28 pixel. Each training and test case is associated with one of ten labels (0–9). Up till here Zalando’s dataset is basically the same as the original handwritten digits data. However, instead of having images of the digits 0–9, Zalando’s data contains (not unsurprisingly) images with 10 different fashion products. Consequently, the dataset is called <a href="https://github.com/zalandoresearch/fashion-mnist" target="_self">Fashion-MNIST dataset</a>, which can be downloaded from <a href="https://github.com/zalandoresearch/fashion-mnist" target="_self">GitHub</a>. The data is also featured on <a href="https://www.kaggle.com/zalando-research/fashionmnist" target="_self">Kaggle</a>. A few examples are shown in the following image, where each row contains one fashion item.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*RkysnlFejHNE4Us5aXmnHQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*RkysnlFejHNE4Us5aXmnHQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*RkysnlFejHNE4Us5aXmnHQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*RkysnlFejHNE4Us5aXmnHQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*RkysnlFejHNE4Us5aXmnHQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*RkysnlFejHNE4Us5aXmnHQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RkysnlFejHNE4Us5aXmnHQ.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*RkysnlFejHNE4Us5aXmnHQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*RkysnlFejHNE4Us5aXmnHQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*RkysnlFejHNE4Us5aXmnHQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*RkysnlFejHNE4Us5aXmnHQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*RkysnlFejHNE4Us5aXmnHQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*RkysnlFejHNE4Us5aXmnHQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*RkysnlFejHNE4Us5aXmnHQ.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*RkysnlFejHNE4Us5aXmnHQ.jpeg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The 10 different class labels are:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">0 T-shirt/top</li><li class="ff3" style="font-size:22px;">1 Trouser</li><li class="ff3" style="font-size:22px;">2 Pullover</li><li class="ff3" style="font-size:22px;">3 Dress</li><li class="ff3" style="font-size:22px;">4 Coat</li><li class="ff3" style="font-size:22px;">5 Sandal</li><li class="ff3" style="font-size:22px;">6 Shirt</li><li class="ff3" style="font-size:22px;">7 Sneaker</li><li class="ff3" style="font-size:22px;">8 Bag</li><li class="ff3" style="font-size:22px;">9 Ankle boot</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">According to the authors, the Fashion-MNIST data is intended to be a direct drop-in replacement for the old MNIST handwritten digits data, since there were several issues with the handwritten digits. For example, it was possible to correctly distinguish between several digits, by simply looking at a few pixels. Even with linear classifiers it was possible to achieve high classification accuracy. The Fashion-MNIST data promises to be more diverse so that machine learning (ML) algorithms have to learn more advanced features in order to be able to separate the individual classes reliably.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><strong>Embedding Visualization of Fashion MNIST</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Embedding is a way to map discrete objects (images, words, etc.) to high dimensional vectors. The individual dimensions in these vectors typically have no inherent meaning. Instead, it’s the overall patterns of location and distance between vectors that machine learning takes advantage of. Embeddings, thus, are important for input to machine learning; since classifiers and neural networks, more generally, work on vectors of real numbers. They train best on dense vectors, where all values contribute to define an object.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">TensorBoard has a built-in visualizer, called the <a href="https://www.tensorflow.org/versions/r1.1/get_started/embedding_viz" target="_self">Embedding Projector</a>, for interactive visualization and analysis of high-dimensional data like embeddings. The embedding projector will read the embeddings from my model checkpoint file. Although it’s most useful for embeddings, it will load any 2D tensor, including my training weights.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here, I’ll attempt to represent the high-dimensional Fashion MNIST data using TensorBoard. After reading the data and create the test labels, I use this code to build TensorBoard’s Embedding Projector:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre>from tensorflow.contrib.tensorboard.plugins import projector

logdir = 'fashionMNIST-logs'

# Creating the embedding variable with all the images defined above under X_test
embedding_var = tf.Variable(X_test, name='fmnist_embedding')

# Format: tensorflow/contrib/tensorboard/plugins/projector/projector_config.proto
config = projector.ProjectorConfig()

# You can add multiple embeddings. Here I add only one.
embedding = config.embeddings.add()
embedding.tensor_name = embedding_var.name

# Link this tensor to its metadata file (e.g. labels).
embedding.metadata_path = os.path.join(logdir, 'metadata.tsv')

# Use this logdir to create a summary writer
summary_writer = tf.summary.FileWriter(logdir)

# The next line writes a projector_config.pbtxt in the logdir. TensorBoard will read this file during startup.
projector.visualize_embeddings(summary_writer,config)

# Periodically save the model variables in a checkpoint in logdir.
with tf.Session() as sesh:
    sesh.run(tf.global_variables_initializer())
    saver = tf.train.Saver()
    saver.save(sesh, os.path.join(logdir, 'model.ckpt'))
    
# Create the sprite image
rows = 28 
cols = 28
label = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

sprite_dim = int(np.sqrt(X_test.shape[0]))
sprite_image = np.ones((cols * sprite_dim, rows * sprite_dim))

index = 0 
labels = [] 
for i in range(sprite_dim): 
    for j in range(sprite_dim):
        labels.append(label[int(Y_test[index])])

        sprite_image[
            i * cols: (i + 1) * cols,
            j * rows: (j + 1) * rows
        ] = X_test[index].reshape(28, 28) * -1 + 1

        index += 1
        
# After constructing the sprite, I need to tell the Embedding Projector where to find it
embedding.sprite.image_path = os.path.join(logdir, 'sprite.png')
embedding.sprite.single_image_dim.extend([28, 28])

# Create the metadata (labels) file
with open(embedding.metadata_path, 'w') as meta:
    meta.write('Index\tLabel\n')
    for index, label in enumerate(labels):
        meta.write('{}\t{}\n'.format(index, label))</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The Embedding Projector has three methods of reducing the dimensionality of a data set: two linear and one nonlinear. Each method can be used to create either a two- or three-dimensional view.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Principal Component Analysis:</strong> A straightforward technique for reducing dimensions is Principal Component Analysis (PCA). The Embedding Projector computes the top 10 principal components. The menu lets me project those components onto any combination of two or three. PCA is a linear projection, often effective at examining global geometry.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:81%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 600px" srcset="https://miro.medium.com/v2/resize:fit:640/1*m8Zl3Mp7SD7dVPA_SDcENA.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*m8Zl3Mp7SD7dVPA_SDcENA.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*m8Zl3Mp7SD7dVPA_SDcENA.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*m8Zl3Mp7SD7dVPA_SDcENA.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*m8Zl3Mp7SD7dVPA_SDcENA.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*m8Zl3Mp7SD7dVPA_SDcENA.gif 1100w, https://miro.medium.com/v2/resize:fit:1200/1*m8Zl3Mp7SD7dVPA_SDcENA.gif 1200w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 600px" srcset="https://miro.medium.com/v2/resize:fit:640/1*m8Zl3Mp7SD7dVPA_SDcENA.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*m8Zl3Mp7SD7dVPA_SDcENA.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*m8Zl3Mp7SD7dVPA_SDcENA.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*m8Zl3Mp7SD7dVPA_SDcENA.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*m8Zl3Mp7SD7dVPA_SDcENA.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*m8Zl3Mp7SD7dVPA_SDcENA.gif 1100w, https://miro.medium.com/v2/resize:fit:1200/1*m8Zl3Mp7SD7dVPA_SDcENA.gif 1200w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/600/1*m8Zl3Mp7SD7dVPA_SDcENA.gif"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>t-SNE:</strong> A popular non-linear dimensionality reduction technique is t-SNE. The Embedding Projector offers both two- and three-dimensional t-SNE views. Layout is performed client-side animating every step of the algorithm. Because t-SNE often preserves some local structure, it is useful for exploring local neighborhoods and finding clusters.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*_rGbQSqnyH6YajHXFCtLXg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*_rGbQSqnyH6YajHXFCtLXg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*_rGbQSqnyH6YajHXFCtLXg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*_rGbQSqnyH6YajHXFCtLXg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*_rGbQSqnyH6YajHXFCtLXg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*_rGbQSqnyH6YajHXFCtLXg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_rGbQSqnyH6YajHXFCtLXg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*_rGbQSqnyH6YajHXFCtLXg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*_rGbQSqnyH6YajHXFCtLXg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*_rGbQSqnyH6YajHXFCtLXg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*_rGbQSqnyH6YajHXFCtLXg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*_rGbQSqnyH6YajHXFCtLXg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*_rGbQSqnyH6YajHXFCtLXg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*_rGbQSqnyH6YajHXFCtLXg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*_rGbQSqnyH6YajHXFCtLXg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Custom:</strong> I can also construct specialized linear projections based on text searches for finding meaningful directions in space. To define a projection axis, enter two search strings or regular expressions. The program computes the centroids of the sets of points whose labels match these searches, and uses the difference vector between centroids as a projection axis.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*ka2cfdTbyrPbVdF8yoftLg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*ka2cfdTbyrPbVdF8yoftLg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*ka2cfdTbyrPbVdF8yoftLg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*ka2cfdTbyrPbVdF8yoftLg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*ka2cfdTbyrPbVdF8yoftLg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ka2cfdTbyrPbVdF8yoftLg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ka2cfdTbyrPbVdF8yoftLg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*ka2cfdTbyrPbVdF8yoftLg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*ka2cfdTbyrPbVdF8yoftLg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*ka2cfdTbyrPbVdF8yoftLg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*ka2cfdTbyrPbVdF8yoftLg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*ka2cfdTbyrPbVdF8yoftLg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*ka2cfdTbyrPbVdF8yoftLg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*ka2cfdTbyrPbVdF8yoftLg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*ka2cfdTbyrPbVdF8yoftLg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">You can view the full code for the visualization steps at this notebook: <a href="https://github.com/khanhnamle1994/fashion-mnist/blob/master/TensorBoard-Visualization.ipynb" target="_self">TensorBoard-Visualization.ipynb</a></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><strong>Training CNN Models on Fashion MNIST</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Let’s now move to the fun part: I will create a variety of different CNN-based classification models to evaluate performances on Fashion MNIST. I will be building our model using the Keras framework. For more information on the framework, you can refer to the documentation <a href="https://keras.io/" target="_self">here</a>. Here are the list of models I will try out and compare their results:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;">CNN with 1 Convolutional Layer</li><li class="ff3" style="font-size:22px;">CNN with 3 Convolutional Layer</li><li class="ff3" style="font-size:22px;">CNN with 4 Convolutional Layer</li><li class="ff3" style="font-size:22px;">VGG-19 Pre-Trained Model</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For all the models (except for the pre-trained one), here is my approach:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Split the original training data (60,000 images) into <strong>80% training</strong> (48,000 images) and <strong>20% validation</strong> (12000 images) optimize the classifier, while keeping the test data (10,000 images) to finally evaluate the accuracy of the model on the data it has never seen. This helps to see whether I’m over-fitting on the training data and whether I should lower the learning rate and train for more epochs if validation accuracy is higher than training accuracy or stop over-training if training accuracy shift higher than the validation.</li><li class="ff3" style="font-size:22px;">Train the model for 10 epochs with batch size of 256, compiled with <strong>categorical_crossentropy</strong> loss function and <strong>Adam</strong> optimizer.</li><li class="ff3" style="font-size:22px;">Then, add <strong>data augmentation</strong>, which generates new training samples by rotating, shifting and zooming on the training samples, and train the model on updated data for another 50 epochs.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here’s the code to load and split the data:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre># Import libraries
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split

# Load training and test data into dataframes
data_train = pd.read_csv('data/fashion-mnist_train.csv')
data_test = pd.read_csv('data/fashion-mnist_test.csv')

# X forms the training images, and y forms the training labels
X = np.array(data_train.iloc[:, 1:])
y = to_categorical(np.array(data_train.iloc[:, 0]))

# Here I split original training data to sub-training (80%) and validation data (20%)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=13)

# X_test forms the test images, and y_test forms the test labels
X_test = np.array(data_test.iloc[:, 1:])
y_test = to_categorical(np.array(data_test.iloc[:, 0]))</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After loading and splitting the data, I preprocess them by reshaping them into the shape the network expects and scaling them so that all values are in the [0, 1] interval. Previously, for instance, the training data were stored in an array of shape (60000, 28, 28) of type uint8 with values in the [0, 255] interval. I transform it into a float32 array of shape (60000, 28 * 28) with values between 0 and 1.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre># Each image's dimension is 28 x 28
img_rows, img_cols = 28, 28
input_shape = (img_rows, img_cols, 1)

# Prepare the training images
X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)
X_train = X_train.astype('float32')
X_train /= 255

# Prepare the test images
X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)
X_test = X_test.astype('float32')
X_test /= 255

# Prepare the validation images
X_val = X_val.reshape(X_val.shape[0], img_rows, img_cols, 1)
X_val = X_val.astype('float32')
X_val /= 255</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><strong>1 — 1-Conv CNN</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here’s the code for the CNN with 1 Convolutional Layer:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre>from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D

cnn1 = Sequential()
cnn1.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
cnn1.add(MaxPooling2D(pool_size=(2, 2)))
cnn1.add(Dropout(0.2))

cnn1.add(Flatten())

cnn1.add(Dense(128, activation='relu'))
cnn1.add(Dense(10, activation='softmax'))

cnn1.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adam(),
              metrics=['accuracy'])</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After training the model, here’s the test loss and test accuracy:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:68%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 506px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*2jLyAlLlxXxICBLo7mHFYw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*2jLyAlLlxXxICBLo7mHFYw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*2jLyAlLlxXxICBLo7mHFYw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*2jLyAlLlxXxICBLo7mHFYw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*2jLyAlLlxXxICBLo7mHFYw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*2jLyAlLlxXxICBLo7mHFYw.png 1100w, https://miro.medium.com/v2/resize:fit:1012/format:webp/1*2jLyAlLlxXxICBLo7mHFYw.png 1012w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 506px" srcset="https://miro.medium.com/v2/resize:fit:640/1*2jLyAlLlxXxICBLo7mHFYw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*2jLyAlLlxXxICBLo7mHFYw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*2jLyAlLlxXxICBLo7mHFYw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*2jLyAlLlxXxICBLo7mHFYw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*2jLyAlLlxXxICBLo7mHFYw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*2jLyAlLlxXxICBLo7mHFYw.png 1100w, https://miro.medium.com/v2/resize:fit:1012/1*2jLyAlLlxXxICBLo7mHFYw.png 1012w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/506/1*2jLyAlLlxXxICBLo7mHFYw.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After applying data augmentation, here’s the test loss and test accuracy:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:71%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 526px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*dAhXjPwl_rXW3FBWP45SMw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*dAhXjPwl_rXW3FBWP45SMw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*dAhXjPwl_rXW3FBWP45SMw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*dAhXjPwl_rXW3FBWP45SMw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*dAhXjPwl_rXW3FBWP45SMw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*dAhXjPwl_rXW3FBWP45SMw.png 1100w, https://miro.medium.com/v2/resize:fit:1052/format:webp/1*dAhXjPwl_rXW3FBWP45SMw.png 1052w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 526px" srcset="https://miro.medium.com/v2/resize:fit:640/1*dAhXjPwl_rXW3FBWP45SMw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*dAhXjPwl_rXW3FBWP45SMw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*dAhXjPwl_rXW3FBWP45SMw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*dAhXjPwl_rXW3FBWP45SMw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*dAhXjPwl_rXW3FBWP45SMw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*dAhXjPwl_rXW3FBWP45SMw.png 1100w, https://miro.medium.com/v2/resize:fit:1052/1*dAhXjPwl_rXW3FBWP45SMw.png 1052w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/526/1*dAhXjPwl_rXW3FBWP45SMw.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For visual purpose, I plot the training and validation accuracy and loss:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:50%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 381px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*ArcjMmJhIo1K5_Fy06WrxQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*ArcjMmJhIo1K5_Fy06WrxQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*ArcjMmJhIo1K5_Fy06WrxQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*ArcjMmJhIo1K5_Fy06WrxQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*ArcjMmJhIo1K5_Fy06WrxQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ArcjMmJhIo1K5_Fy06WrxQ.png 1100w, https://miro.medium.com/v2/resize:fit:762/format:webp/1*ArcjMmJhIo1K5_Fy06WrxQ.png 762w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 381px" srcset="https://miro.medium.com/v2/resize:fit:640/1*ArcjMmJhIo1K5_Fy06WrxQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*ArcjMmJhIo1K5_Fy06WrxQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*ArcjMmJhIo1K5_Fy06WrxQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*ArcjMmJhIo1K5_Fy06WrxQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*ArcjMmJhIo1K5_Fy06WrxQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*ArcjMmJhIo1K5_Fy06WrxQ.png 1100w, https://miro.medium.com/v2/resize:fit:762/1*ArcjMmJhIo1K5_Fy06WrxQ.png 762w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/381/1*ArcjMmJhIo1K5_Fy06WrxQ.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:50%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 381px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*zRejnU8E_puX6Sc5z7OavA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*zRejnU8E_puX6Sc5z7OavA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*zRejnU8E_puX6Sc5z7OavA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*zRejnU8E_puX6Sc5z7OavA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*zRejnU8E_puX6Sc5z7OavA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*zRejnU8E_puX6Sc5z7OavA.png 1100w, https://miro.medium.com/v2/resize:fit:762/format:webp/1*zRejnU8E_puX6Sc5z7OavA.png 762w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 381px" srcset="https://miro.medium.com/v2/resize:fit:640/1*zRejnU8E_puX6Sc5z7OavA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*zRejnU8E_puX6Sc5z7OavA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*zRejnU8E_puX6Sc5z7OavA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*zRejnU8E_puX6Sc5z7OavA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*zRejnU8E_puX6Sc5z7OavA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*zRejnU8E_puX6Sc5z7OavA.png 1100w, https://miro.medium.com/v2/resize:fit:762/1*zRejnU8E_puX6Sc5z7OavA.png 762w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/381/1*zRejnU8E_puX6Sc5z7OavA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">You can view the full code for this model at this notebook: <a href="https://github.com/khanhnamle1994/fashion-mnist/blob/master/CNN-1Conv.ipynb" target="_self">CNN-1Conv.ipynb</a></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><strong>2 — 3-Conv CNN</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here’s the code for the CNN with 3 Convolutional Layer:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre>from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D

cnn3 = Sequential()
cnn3.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
cnn3.add(MaxPooling2D((2, 2)))
cnn3.add(Dropout(0.25))

cnn3.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
cnn3.add(MaxPooling2D(pool_size=(2, 2)))
cnn3.add(Dropout(0.25))

cnn3.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
cnn3.add(Dropout(0.4))

cnn3.add(Flatten())

cnn3.add(Dense(128, activation='relu'))
cnn3.add(Dropout(0.3))
cnn3.add(Dense(10, activation='softmax'))

cnn3.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adam(),
              metrics=['accuracy'])</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After training the model, here’s the test loss and test accuracy:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:70%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 524px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*PkCSOeseVwDR30rnb_U-VQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*PkCSOeseVwDR30rnb_U-VQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*PkCSOeseVwDR30rnb_U-VQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*PkCSOeseVwDR30rnb_U-VQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*PkCSOeseVwDR30rnb_U-VQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*PkCSOeseVwDR30rnb_U-VQ.png 1100w, https://miro.medium.com/v2/resize:fit:1048/format:webp/1*PkCSOeseVwDR30rnb_U-VQ.png 1048w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 524px" srcset="https://miro.medium.com/v2/resize:fit:640/1*PkCSOeseVwDR30rnb_U-VQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*PkCSOeseVwDR30rnb_U-VQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*PkCSOeseVwDR30rnb_U-VQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*PkCSOeseVwDR30rnb_U-VQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*PkCSOeseVwDR30rnb_U-VQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*PkCSOeseVwDR30rnb_U-VQ.png 1100w, https://miro.medium.com/v2/resize:fit:1048/1*PkCSOeseVwDR30rnb_U-VQ.png 1048w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/524/1*PkCSOeseVwDR30rnb_U-VQ.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After applying data augmentation, here’s the test loss and test accuracy:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:70%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 520px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*msfZVgl0LrG5LURE-xaRwA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*msfZVgl0LrG5LURE-xaRwA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*msfZVgl0LrG5LURE-xaRwA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*msfZVgl0LrG5LURE-xaRwA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*msfZVgl0LrG5LURE-xaRwA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*msfZVgl0LrG5LURE-xaRwA.png 1100w, https://miro.medium.com/v2/resize:fit:1040/format:webp/1*msfZVgl0LrG5LURE-xaRwA.png 1040w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 520px" srcset="https://miro.medium.com/v2/resize:fit:640/1*msfZVgl0LrG5LURE-xaRwA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*msfZVgl0LrG5LURE-xaRwA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*msfZVgl0LrG5LURE-xaRwA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*msfZVgl0LrG5LURE-xaRwA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*msfZVgl0LrG5LURE-xaRwA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*msfZVgl0LrG5LURE-xaRwA.png 1100w, https://miro.medium.com/v2/resize:fit:1040/1*msfZVgl0LrG5LURE-xaRwA.png 1040w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/520/1*msfZVgl0LrG5LURE-xaRwA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For visual purpose, I plot the training and validation accuracy and loss:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:50%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 381px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*PGZtV6kcMAJBG7VxBWEyGg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*PGZtV6kcMAJBG7VxBWEyGg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*PGZtV6kcMAJBG7VxBWEyGg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*PGZtV6kcMAJBG7VxBWEyGg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*PGZtV6kcMAJBG7VxBWEyGg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*PGZtV6kcMAJBG7VxBWEyGg.png 1100w, https://miro.medium.com/v2/resize:fit:762/format:webp/1*PGZtV6kcMAJBG7VxBWEyGg.png 762w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 381px" srcset="https://miro.medium.com/v2/resize:fit:640/1*PGZtV6kcMAJBG7VxBWEyGg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*PGZtV6kcMAJBG7VxBWEyGg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*PGZtV6kcMAJBG7VxBWEyGg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*PGZtV6kcMAJBG7VxBWEyGg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*PGZtV6kcMAJBG7VxBWEyGg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*PGZtV6kcMAJBG7VxBWEyGg.png 1100w, https://miro.medium.com/v2/resize:fit:762/1*PGZtV6kcMAJBG7VxBWEyGg.png 762w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/381/1*PGZtV6kcMAJBG7VxBWEyGg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:50%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 381px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*vUddUfUYRXYSIH6moXVIpA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*vUddUfUYRXYSIH6moXVIpA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*vUddUfUYRXYSIH6moXVIpA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*vUddUfUYRXYSIH6moXVIpA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*vUddUfUYRXYSIH6moXVIpA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*vUddUfUYRXYSIH6moXVIpA.png 1100w, https://miro.medium.com/v2/resize:fit:762/format:webp/1*vUddUfUYRXYSIH6moXVIpA.png 762w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 381px" srcset="https://miro.medium.com/v2/resize:fit:640/1*vUddUfUYRXYSIH6moXVIpA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*vUddUfUYRXYSIH6moXVIpA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*vUddUfUYRXYSIH6moXVIpA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*vUddUfUYRXYSIH6moXVIpA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*vUddUfUYRXYSIH6moXVIpA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*vUddUfUYRXYSIH6moXVIpA.png 1100w, https://miro.medium.com/v2/resize:fit:762/1*vUddUfUYRXYSIH6moXVIpA.png 762w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/381/1*vUddUfUYRXYSIH6moXVIpA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">You can view the full code for this model at this notebook: <a href="https://github.com/khanhnamle1994/fashion-mnist/blob/master/CNN-3Conv.ipynb" target="_self">CNN-3Conv.ipynb</a></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><strong>3 — 4-Conv CNN</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here’s the code for the CNN with 4 Convolutional Layer:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre>from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization

cnn4 = Sequential()
cnn4.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
cnn4.add(BatchNormalization())

cnn4.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))
cnn4.add(BatchNormalization())
cnn4.add(MaxPooling2D(pool_size=(2, 2)))
cnn4.add(Dropout(0.25))

cnn4.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
cnn4.add(BatchNormalization())
cnn4.add(Dropout(0.25))

cnn4.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
cnn4.add(BatchNormalization())
cnn4.add(MaxPooling2D(pool_size=(2, 2)))
cnn4.add(Dropout(0.25))

cnn4.add(Flatten())

cnn4.add(Dense(512, activation='relu'))
cnn4.add(BatchNormalization())
cnn4.add(Dropout(0.5))

cnn4.add(Dense(128, activation='relu'))
cnn4.add(BatchNormalization())
cnn4.add(Dropout(0.5))

cnn4.add(Dense(10, activation='softmax'))

cnn4.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adam(),
              metrics=['accuracy'])</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After training the model, here’s the test loss and test accuracy:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:71%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 530px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*5rLC0iOlQYMB68j1LVpnOA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*5rLC0iOlQYMB68j1LVpnOA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*5rLC0iOlQYMB68j1LVpnOA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*5rLC0iOlQYMB68j1LVpnOA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*5rLC0iOlQYMB68j1LVpnOA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*5rLC0iOlQYMB68j1LVpnOA.png 1100w, https://miro.medium.com/v2/resize:fit:1060/format:webp/1*5rLC0iOlQYMB68j1LVpnOA.png 1060w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 530px" srcset="https://miro.medium.com/v2/resize:fit:640/1*5rLC0iOlQYMB68j1LVpnOA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*5rLC0iOlQYMB68j1LVpnOA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*5rLC0iOlQYMB68j1LVpnOA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*5rLC0iOlQYMB68j1LVpnOA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*5rLC0iOlQYMB68j1LVpnOA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*5rLC0iOlQYMB68j1LVpnOA.png 1100w, https://miro.medium.com/v2/resize:fit:1060/1*5rLC0iOlQYMB68j1LVpnOA.png 1060w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/530/1*5rLC0iOlQYMB68j1LVpnOA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After applying data augmentation, here’s the test loss and test accuracy:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:68%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 506px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*cxEG7QnhZvXKq5KnvOnXQA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*cxEG7QnhZvXKq5KnvOnXQA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*cxEG7QnhZvXKq5KnvOnXQA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*cxEG7QnhZvXKq5KnvOnXQA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*cxEG7QnhZvXKq5KnvOnXQA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*cxEG7QnhZvXKq5KnvOnXQA.png 1100w, https://miro.medium.com/v2/resize:fit:1012/format:webp/1*cxEG7QnhZvXKq5KnvOnXQA.png 1012w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 506px" srcset="https://miro.medium.com/v2/resize:fit:640/1*cxEG7QnhZvXKq5KnvOnXQA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*cxEG7QnhZvXKq5KnvOnXQA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*cxEG7QnhZvXKq5KnvOnXQA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*cxEG7QnhZvXKq5KnvOnXQA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*cxEG7QnhZvXKq5KnvOnXQA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*cxEG7QnhZvXKq5KnvOnXQA.png 1100w, https://miro.medium.com/v2/resize:fit:1012/1*cxEG7QnhZvXKq5KnvOnXQA.png 1012w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/506/1*cxEG7QnhZvXKq5KnvOnXQA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For visual purpose, I plot the training and validation accuracy and loss:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:50%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 381px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*p1EyBJSJr7a3eMwC6xhDpA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*p1EyBJSJr7a3eMwC6xhDpA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*p1EyBJSJr7a3eMwC6xhDpA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*p1EyBJSJr7a3eMwC6xhDpA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*p1EyBJSJr7a3eMwC6xhDpA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*p1EyBJSJr7a3eMwC6xhDpA.png 1100w, https://miro.medium.com/v2/resize:fit:762/format:webp/1*p1EyBJSJr7a3eMwC6xhDpA.png 762w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 381px" srcset="https://miro.medium.com/v2/resize:fit:640/1*p1EyBJSJr7a3eMwC6xhDpA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*p1EyBJSJr7a3eMwC6xhDpA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*p1EyBJSJr7a3eMwC6xhDpA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*p1EyBJSJr7a3eMwC6xhDpA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*p1EyBJSJr7a3eMwC6xhDpA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*p1EyBJSJr7a3eMwC6xhDpA.png 1100w, https://miro.medium.com/v2/resize:fit:762/1*p1EyBJSJr7a3eMwC6xhDpA.png 762w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/381/1*p1EyBJSJr7a3eMwC6xhDpA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:51%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 388px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*6JRK-66Cs7uUxTdHQo9xEw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*6JRK-66Cs7uUxTdHQo9xEw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*6JRK-66Cs7uUxTdHQo9xEw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*6JRK-66Cs7uUxTdHQo9xEw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*6JRK-66Cs7uUxTdHQo9xEw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*6JRK-66Cs7uUxTdHQo9xEw.png 1100w, https://miro.medium.com/v2/resize:fit:776/format:webp/1*6JRK-66Cs7uUxTdHQo9xEw.png 776w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 388px" srcset="https://miro.medium.com/v2/resize:fit:640/1*6JRK-66Cs7uUxTdHQo9xEw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*6JRK-66Cs7uUxTdHQo9xEw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*6JRK-66Cs7uUxTdHQo9xEw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*6JRK-66Cs7uUxTdHQo9xEw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*6JRK-66Cs7uUxTdHQo9xEw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*6JRK-66Cs7uUxTdHQo9xEw.png 1100w, https://miro.medium.com/v2/resize:fit:776/1*6JRK-66Cs7uUxTdHQo9xEw.png 776w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/388/1*6JRK-66Cs7uUxTdHQo9xEw.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">You can view the full code for this model at this notebook: <a href="https://github.com/khanhnamle1994/fashion-mnist/blob/master/CNN-4Conv.ipynb" target="_self">CNN-4Conv.ipynb</a></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><strong>4 — Transfer Learning</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">A common and highly effective approach to deep learning on small image datasets is to use a pre-trained network. A <strong>pre-trained network</strong> is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. If this original dataset is large enough and general enough, then the spatial hierarchy of features learned by the pre-trained network can effectively act as a generic model of the visual world, and hence its features can prove useful for many different computer-vision problems, even though these new problems may involve completely different classes than those of the original task.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I attempted to implement the VGG19 pre-trained model, which is a widely used ConvNets architecture for ImageNet. Here’s the code you can follow:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre>import keras
from keras.applications import VGG19
from keras.applications.vgg19 import preprocess_input
from keras.layers import Dense, Dropout
from keras.models import Model
from keras import models
from keras import layers
from keras import optimizers

# Create the base model of VGG19
vgg19 = VGG19(weights='imagenet', include_top=False, input_shape = (150, 150, 3), classes = 10)

# Preprocessing the input 
X_train = preprocess_input(X_train)
X_val = preprocess_input(X_val)
X_test = preprocess_input(X_test)

# Extracting features
train_features = vgg19.predict(np.array(X_train), batch_size=256, verbose=1)
test_features = vgg19.predict(np.array(X_test), batch_size=256, verbose=1)
val_features = vgg19.predict(np.array(X_val), batch_size=256, verbose=1)

# Flatten extracted features
train_features = np.reshape(train_features, (48000, 4*4*512))
test_features = np.reshape(test_features, (10000, 4*4*512))
val_features = np.reshape(val_features, (12000, 4*4*512))

# Add Dense and Dropout layers on top of VGG19 pre-trained
model = models.Sequential()
model.add(layers.Dense(512, activation='relu', input_dim=4 * 4 * 512))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(10, activation="softmax"))

# Compile the model
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adam(),
              metrics=['accuracy'])</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After training the model, here’s the test loss and test accuracy:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:80%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 594px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Yxg75wspHzrL7eYHtdP1fw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Yxg75wspHzrL7eYHtdP1fw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Yxg75wspHzrL7eYHtdP1fw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Yxg75wspHzrL7eYHtdP1fw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Yxg75wspHzrL7eYHtdP1fw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Yxg75wspHzrL7eYHtdP1fw.png 1100w, https://miro.medium.com/v2/resize:fit:1188/format:webp/1*Yxg75wspHzrL7eYHtdP1fw.png 1188w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 594px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Yxg75wspHzrL7eYHtdP1fw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*Yxg75wspHzrL7eYHtdP1fw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*Yxg75wspHzrL7eYHtdP1fw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*Yxg75wspHzrL7eYHtdP1fw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*Yxg75wspHzrL7eYHtdP1fw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*Yxg75wspHzrL7eYHtdP1fw.png 1100w, https://miro.medium.com/v2/resize:fit:1188/1*Yxg75wspHzrL7eYHtdP1fw.png 1188w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/594/1*Yxg75wspHzrL7eYHtdP1fw.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For visual purpose, I plot the training and validation accuracy and loss:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:49%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 375px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*75QYfK5zL0I9eHh33oEVOQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*75QYfK5zL0I9eHh33oEVOQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*75QYfK5zL0I9eHh33oEVOQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*75QYfK5zL0I9eHh33oEVOQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*75QYfK5zL0I9eHh33oEVOQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*75QYfK5zL0I9eHh33oEVOQ.png 1100w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*75QYfK5zL0I9eHh33oEVOQ.png 750w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 375px" srcset="https://miro.medium.com/v2/resize:fit:640/1*75QYfK5zL0I9eHh33oEVOQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*75QYfK5zL0I9eHh33oEVOQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*75QYfK5zL0I9eHh33oEVOQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*75QYfK5zL0I9eHh33oEVOQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*75QYfK5zL0I9eHh33oEVOQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*75QYfK5zL0I9eHh33oEVOQ.png 1100w, https://miro.medium.com/v2/resize:fit:750/1*75QYfK5zL0I9eHh33oEVOQ.png 750w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/375/1*75QYfK5zL0I9eHh33oEVOQ.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:49%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 375px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Yc7rx8NRhenu-lADh2lz3Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Yc7rx8NRhenu-lADh2lz3Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Yc7rx8NRhenu-lADh2lz3Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Yc7rx8NRhenu-lADh2lz3Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Yc7rx8NRhenu-lADh2lz3Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Yc7rx8NRhenu-lADh2lz3Q.png 1100w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Yc7rx8NRhenu-lADh2lz3Q.png 750w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 375px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Yc7rx8NRhenu-lADh2lz3Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*Yc7rx8NRhenu-lADh2lz3Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*Yc7rx8NRhenu-lADh2lz3Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*Yc7rx8NRhenu-lADh2lz3Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*Yc7rx8NRhenu-lADh2lz3Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*Yc7rx8NRhenu-lADh2lz3Q.png 1100w, https://miro.medium.com/v2/resize:fit:750/1*Yc7rx8NRhenu-lADh2lz3Q.png 750w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/375/1*Yc7rx8NRhenu-lADh2lz3Q.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">You can view the full code for this model at this notebook: <a href="https://github.com/khanhnamle1994/fashion-mnist/blob/master/VGG19-GPU.ipynb" target="_self">VGG19-GPU.ipynb</a></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><strong>Last Takeaway</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The fashion domain is a very popular playground for applications of machine learning and computer vision. The problems in this domain is challenging due to the high level of subjectivity and the semantic complexity of the features involved. I hope that this post has been helpful for you to learn about the 4 different approaches to build your own convolutional neural networks to classify fashion images. You can view all the source code in my GitHub repo <a href="https://github.com/khanhnamle1994/fashion-mnist" target="_self">at this link</a>. Let me know if you have any questions or suggestions on improvement!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">— —</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>If you enjoyed this piece, I’d love it if you hit the clap button</em> 👏 <em>so others might stumble upon it. You can find my own code on</em> <a href="https://github.com/khanhnamle1994" target="_self">GitHub</a><em>, and more of my writing and projects at</em> <a href="https://jameskle.com" target="_self">https://jameskle.com/</a><em>. You can also follow me on </em><a href="https://twitter.com/@james_aka_yale" target="_self">Twitter</a><em>, </em><a href="mailto:khanhle.1013@gmail.com" target="_self">email me directly</a><em> or </em><a href="http://www.linkedin.com/in/khanhnamle94" target="_self">find me on LinkedIn</a><em>. </em><a href="http://eepurl.com/deWjzb" target="_self">Sign up for my newsletter</a><em> to receive my latest thoughts on data science, machine learning, and artificial intelligence right at your inbox!</em></p></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>