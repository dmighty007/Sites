<!DOCTYPE html>
                <html>
                <head>
                    <title>A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://medium.com/@dhruvp?source=post_page-----34ea83205de4--------------------------------">Author : Dhruv Parthasarathy</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN</h3></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">At <a href="http://athelas.com" target="_self">Athelas</a>, we use Convolutional Neural Networks(CNNs) for a lot more than just classification! In this post, we’ll see how CNNs can be used, with great results, in image instance segmentation.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Ever since <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" target="_self">Alex Krizhevsky, Geoff Hinton, and Ilya Sutskever won ImageNet in 2012</a>, Convolutional Neural Networks(CNNs) have become the gold standard for image classification. In fact, since then, CNNs have improved to the point where they now outperform humans on the ImageNet challenge!</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:85%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 624px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*bGTawFxQwzc5yV1_szDrwQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*bGTawFxQwzc5yV1_szDrwQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*bGTawFxQwzc5yV1_szDrwQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*bGTawFxQwzc5yV1_szDrwQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*bGTawFxQwzc5yV1_szDrwQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*bGTawFxQwzc5yV1_szDrwQ.png 1100w, https://miro.medium.com/v2/resize:fit:1248/format:webp/1*bGTawFxQwzc5yV1_szDrwQ.png 1248w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 624px" srcset="https://miro.medium.com/v2/resize:fit:640/1*bGTawFxQwzc5yV1_szDrwQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*bGTawFxQwzc5yV1_szDrwQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*bGTawFxQwzc5yV1_szDrwQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*bGTawFxQwzc5yV1_szDrwQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*bGTawFxQwzc5yV1_szDrwQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*bGTawFxQwzc5yV1_szDrwQ.png 1100w, https://miro.medium.com/v2/resize:fit:1248/1*bGTawFxQwzc5yV1_szDrwQ.png 1248w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/624/1*bGTawFxQwzc5yV1_szDrwQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">CNNs now outperform humans on the ImageNet challenge. The y-axis in the above graph is the error rate on ImageNet.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">While these results are impressive, image classification is far simpler than the complexity and diversity of true human visual understanding.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:53%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 399px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*8GVucX9yhnL21KCtcyFDRQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*8GVucX9yhnL21KCtcyFDRQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*8GVucX9yhnL21KCtcyFDRQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*8GVucX9yhnL21KCtcyFDRQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*8GVucX9yhnL21KCtcyFDRQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*8GVucX9yhnL21KCtcyFDRQ.png 1100w, https://miro.medium.com/v2/resize:fit:798/format:webp/1*8GVucX9yhnL21KCtcyFDRQ.png 798w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 399px" srcset="https://miro.medium.com/v2/resize:fit:640/1*8GVucX9yhnL21KCtcyFDRQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*8GVucX9yhnL21KCtcyFDRQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*8GVucX9yhnL21KCtcyFDRQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*8GVucX9yhnL21KCtcyFDRQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*8GVucX9yhnL21KCtcyFDRQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*8GVucX9yhnL21KCtcyFDRQ.png 1100w, https://miro.medium.com/v2/resize:fit:798/1*8GVucX9yhnL21KCtcyFDRQ.png 798w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/399/1*8GVucX9yhnL21KCtcyFDRQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">An example of an image used in the classification challenge. Note how the image is well framed and has just one object.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In classification, there’s generally an image with a single object as the focus and the task is to say what that image is (see above). But when we look at the world around us, we carry out far more complex tasks.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*eJjj2TVUVZDiVSTcnzh7fA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*eJjj2TVUVZDiVSTcnzh7fA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*eJjj2TVUVZDiVSTcnzh7fA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*eJjj2TVUVZDiVSTcnzh7fA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*eJjj2TVUVZDiVSTcnzh7fA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*eJjj2TVUVZDiVSTcnzh7fA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eJjj2TVUVZDiVSTcnzh7fA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*eJjj2TVUVZDiVSTcnzh7fA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*eJjj2TVUVZDiVSTcnzh7fA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*eJjj2TVUVZDiVSTcnzh7fA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*eJjj2TVUVZDiVSTcnzh7fA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*eJjj2TVUVZDiVSTcnzh7fA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*eJjj2TVUVZDiVSTcnzh7fA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*eJjj2TVUVZDiVSTcnzh7fA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*eJjj2TVUVZDiVSTcnzh7fA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Sights in real life are often composed of a multitude of different, overlapping objects, backgrounds, and actions.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We see complicated sights with multiple overlapping objects, and different backgrounds and we not only classify these different objects but also identify their boundaries, differences, and relations to one another!</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:75%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 558px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*NdwfHMrW3rpj5SW_VQtWVw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*NdwfHMrW3rpj5SW_VQtWVw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*NdwfHMrW3rpj5SW_VQtWVw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*NdwfHMrW3rpj5SW_VQtWVw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*NdwfHMrW3rpj5SW_VQtWVw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*NdwfHMrW3rpj5SW_VQtWVw.png 1100w, https://miro.medium.com/v2/resize:fit:1116/format:webp/1*NdwfHMrW3rpj5SW_VQtWVw.png 1116w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 558px" srcset="https://miro.medium.com/v2/resize:fit:640/1*NdwfHMrW3rpj5SW_VQtWVw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*NdwfHMrW3rpj5SW_VQtWVw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*NdwfHMrW3rpj5SW_VQtWVw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*NdwfHMrW3rpj5SW_VQtWVw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*NdwfHMrW3rpj5SW_VQtWVw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*NdwfHMrW3rpj5SW_VQtWVw.png 1100w, https://miro.medium.com/v2/resize:fit:1116/1*NdwfHMrW3rpj5SW_VQtWVw.png 1116w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/558/1*NdwfHMrW3rpj5SW_VQtWVw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">In image segmentation, our goal is to classify the different objects in the image, and identify their boundaries. Source: Mask R-CNN paper.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Can CNNs help us with such complex tasks? Namely, given a more complicated image, can we use CNNs to identify the different objects in the image, and their boundaries? As has been shown by Ross Girshick and his peers over the last few years, the answer is conclusively yes.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Goals of this Post</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Through this post, we’ll cover the intuition behind some of the main techniques used in object detection and segmentation and see how they’ve evolved from one implementation to the next. In particular, we’ll cover R-CNN (Regional CNN), the original application of CNNs to this problem, along with its descendants Fast R-CNN, and Faster R-CNN. Finally, we’ll cover Mask R-CNN, a paper released recently by Facebook Research that extends such object detection techniques to provide pixel level segmentation. Here are the papers referenced in this post:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;">R-CNN: <a href="https://arxiv.org/abs/1311.2524" target="_self">https://arxiv.org/abs/1311.2524</a></li><li class="ff3" style="font-size:22px;">Fast R-CNN: <a href="https://arxiv.org/abs/1504.08083" target="_self">https://arxiv.org/abs/1504.08083</a></li><li class="ff3" style="font-size:22px;">Faster R-CNN: <a href="https://arxiv.org/abs/1506.01497" target="_self">https://arxiv.org/abs/1506.01497</a></li><li class="ff3" style="font-size:22px;">Mask R-CNN: <a href="https://arxiv.org/abs/1703.06870" target="_self">https://arxiv.org/abs/1703.06870</a></li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">2014: R-CNN - An Early Application of CNNs to Object Detection</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:67%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 500px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*r9ELExnk1B1zHnRReDW9Ow.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*r9ELExnk1B1zHnRReDW9Ow.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*r9ELExnk1B1zHnRReDW9Ow.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*r9ELExnk1B1zHnRReDW9Ow.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*r9ELExnk1B1zHnRReDW9Ow.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*r9ELExnk1B1zHnRReDW9Ow.png 1100w, https://miro.medium.com/v2/resize:fit:1000/format:webp/1*r9ELExnk1B1zHnRReDW9Ow.png 1000w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 500px" srcset="https://miro.medium.com/v2/resize:fit:640/1*r9ELExnk1B1zHnRReDW9Ow.png 640w, https://miro.medium.com/v2/resize:fit:720/1*r9ELExnk1B1zHnRReDW9Ow.png 720w, https://miro.medium.com/v2/resize:fit:750/1*r9ELExnk1B1zHnRReDW9Ow.png 750w, https://miro.medium.com/v2/resize:fit:786/1*r9ELExnk1B1zHnRReDW9Ow.png 786w, https://miro.medium.com/v2/resize:fit:828/1*r9ELExnk1B1zHnRReDW9Ow.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*r9ELExnk1B1zHnRReDW9Ow.png 1100w, https://miro.medium.com/v2/resize:fit:1000/1*r9ELExnk1B1zHnRReDW9Ow.png 1000w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/500/1*r9ELExnk1B1zHnRReDW9Ow.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Object detection algorithms such as R-CNN take in an image and identify the locations and classifications of the main objects in the image. Source: <a href="https://arxiv.org/abs/1311.2524" target="_self">https://arxiv.org/abs/1311.2524</a>.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Inspired by the research of Hinton’s lab at the University of Toronto, a small team at UC Berkeley, led by Professor Jitendra Malik, asked themselves what today seems like an inevitable question:</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4">To what extent do [Krizhevsky et. al’s results] generalize to object detection?</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Object detection is the task of finding the different objects in an image and classifying them (as seen in the image above). The team, comprised of Ross Girshick (a name we’ll see again), Jeff Donahue, and Trevor Darrel found that this problem can be solved with Krizhevsky’s results by testing on the PASCAL VOC Challenge, a popular object detection challenge akin to ImageNet. They write,</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4">This paper is the first to show that a CNN can lead to dramatically higher object detection performance on PASCAL VOC as compared to systems based on simpler HOG-like features.</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Let’s now take a moment to understand how their architecture, Regions With CNNs (R-CNN) works.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Understanding R-CNN</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The goal of R-CNN is to take in an image, and correctly identify where the main objects (via a bounding box) in the image.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><strong>Inputs</strong>: Image</li><li class="ff3" style="font-size:22px;"><strong>Outputs</strong>: Bounding boxes + labels for each object in the image.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">But how do we find out where these bounding boxes are? R-CNN does what we might intuitively do as well - <strong>propose</strong> <strong>a bunch of boxes in the image and see if any of them actually correspond to an object</strong>.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*ZQ03Ib84bYioFKoho5HnKg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*ZQ03Ib84bYioFKoho5HnKg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*ZQ03Ib84bYioFKoho5HnKg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*ZQ03Ib84bYioFKoho5HnKg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*ZQ03Ib84bYioFKoho5HnKg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ZQ03Ib84bYioFKoho5HnKg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZQ03Ib84bYioFKoho5HnKg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*ZQ03Ib84bYioFKoho5HnKg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*ZQ03Ib84bYioFKoho5HnKg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*ZQ03Ib84bYioFKoho5HnKg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*ZQ03Ib84bYioFKoho5HnKg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*ZQ03Ib84bYioFKoho5HnKg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*ZQ03Ib84bYioFKoho5HnKg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*ZQ03Ib84bYioFKoho5HnKg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*ZQ03Ib84bYioFKoho5HnKg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Selective Search looks through windows of multiple scales and looks for adjacent pixels that share textures, colors, or intensities. Image source: <a href="https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf" target="_self">https://www.koen.me/research/pub/uijlings-ijcv2013-draft.pdf</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">R-CNN creates these bounding boxes, or region proposals, using a process called <mark>Selective Search</mark> which you can read about <a href="http://www.cs.cornell.edu/courses/cs7670/2014sp/slides/VisionSeminar14.pdf" target="_self">here</a>. At a high level, Selective Search (shown in the image above) looks at the image through windows of different sizes, and for each size tries to group together adjacent pixels by texture, color, or intensity to identify objects.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*Sdj6sKDRQyZpO6oH. 640w, https://miro.medium.com/v2/resize:fit:720/0*Sdj6sKDRQyZpO6oH. 720w, https://miro.medium.com/v2/resize:fit:750/0*Sdj6sKDRQyZpO6oH. 750w, https://miro.medium.com/v2/resize:fit:786/0*Sdj6sKDRQyZpO6oH. 786w, https://miro.medium.com/v2/resize:fit:828/0*Sdj6sKDRQyZpO6oH. 828w, https://miro.medium.com/v2/resize:fit:1100/0*Sdj6sKDRQyZpO6oH. 1100w, https://miro.medium.com/v2/resize:fit:1400/0*Sdj6sKDRQyZpO6oH. 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*Sdj6sKDRQyZpO6oH. 640w, https://miro.medium.com/v2/resize:fit:720/0*Sdj6sKDRQyZpO6oH. 720w, https://miro.medium.com/v2/resize:fit:750/0*Sdj6sKDRQyZpO6oH. 750w, https://miro.medium.com/v2/resize:fit:786/0*Sdj6sKDRQyZpO6oH. 786w, https://miro.medium.com/v2/resize:fit:828/0*Sdj6sKDRQyZpO6oH. 828w, https://miro.medium.com/v2/resize:fit:1100/0*Sdj6sKDRQyZpO6oH. 1100w, https://miro.medium.com/v2/resize:fit:1400/0*Sdj6sKDRQyZpO6oH. 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*Sdj6sKDRQyZpO6oH."></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">After creating a set of region proposals, R-CNN passes the image through a modified version of AlexNet to determine whether or not it is a valid region. Source: <a href="https://arxiv.org/abs/1311.2524" target="_self">https://arxiv.org/abs/1311.2524</a>.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Once the proposals are created, R-CNN warps the region to a standard square size and passes it through to a modified version of AlexNet (the winning submission to ImageNet 2012 that inspired R-CNN), as shown above.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">On the final layer of the CNN, R-CNN adds a Support Vector Machine (SVM) that simply classifies whether this is an object, and if so what object. This is step 4 in the image above.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Improving the Bounding Boxes</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Now, having found the object in the box, can we tighten the box to fit the true dimensions of the object? We can, and this is the final step of R-CNN. R-CNN runs a simple linear regression on the region proposal to generate tighter bounding box coordinates to get our final result. Here are the inputs and outputs of this regression model:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><strong>Inputs</strong>: sub-regions of the image corresponding to objects.</li><li class="ff3" style="font-size:22px;"><strong>Outputs</strong>: New bounding box coordinates for the object in the sub-region.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">So, to summarize, R-CNN is just the following steps:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;">Generate a set of proposals for bounding boxes.</li><li class="ff3" style="font-size:22px;">Run the images in the bounding boxes through a pre-trained AlexNet and finally an SVM to see what object the image in the box is.</li><li class="ff3" style="font-size:22px;">Run the box through a linear regression model to output tighter coordinates for the box once the object has been classified.</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">2015: Fast R-CNN - Speeding up and Simplifying R-CNN</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:61%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 460px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*3xnXHBEAz6FGzb-EehXtkA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*3xnXHBEAz6FGzb-EehXtkA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*3xnXHBEAz6FGzb-EehXtkA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*3xnXHBEAz6FGzb-EehXtkA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*3xnXHBEAz6FGzb-EehXtkA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*3xnXHBEAz6FGzb-EehXtkA.png 1100w, https://miro.medium.com/v2/resize:fit:920/format:webp/1*3xnXHBEAz6FGzb-EehXtkA.png 920w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 460px" srcset="https://miro.medium.com/v2/resize:fit:640/1*3xnXHBEAz6FGzb-EehXtkA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*3xnXHBEAz6FGzb-EehXtkA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*3xnXHBEAz6FGzb-EehXtkA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*3xnXHBEAz6FGzb-EehXtkA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*3xnXHBEAz6FGzb-EehXtkA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*3xnXHBEAz6FGzb-EehXtkA.png 1100w, https://miro.medium.com/v2/resize:fit:920/1*3xnXHBEAz6FGzb-EehXtkA.png 920w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/460/1*3xnXHBEAz6FGzb-EehXtkA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Ross Girshick wrote both R-CNN and Fast R-CNN. He continues to push the boundaries of Computer Vision at Facebook Research.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">R-CNN works really well, but is really quite slow for a few simple reasons:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;">It requires a forward pass of the CNN (AlexNet) for every single region proposal for every single image (that’s around 2000 forward passes per image!).</li><li class="ff3" style="font-size:22px;">It has to train three different models separately - the CNN to generate image features, the classifier that predicts the class, and the regression model to tighten the bounding boxes. This makes the pipeline extremely hard to train.</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In 2015, Ross Girshick, the first author of R-CNN, solved both these problems, leading to the second algorithm in our short history - Fast R-CNN. Let’s now go over its main insights.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Fast R-CNN Insight 1: RoI (Region of Interest) Pooling</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For the forward pass of the CNN, Girshick realized that for each image, a lot of proposed regions for the image invariably overlapped causing us to run the same CNN computation again and again (~2000 times!). His insight was simple — <strong>Why not run the CNN just once per image and then find a way to share that computation across the ~2000 proposals?</strong></p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*4K_Bq1AhAsTe9vlT0wsdXQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*4K_Bq1AhAsTe9vlT0wsdXQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*4K_Bq1AhAsTe9vlT0wsdXQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*4K_Bq1AhAsTe9vlT0wsdXQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*4K_Bq1AhAsTe9vlT0wsdXQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*4K_Bq1AhAsTe9vlT0wsdXQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4K_Bq1AhAsTe9vlT0wsdXQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*4K_Bq1AhAsTe9vlT0wsdXQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*4K_Bq1AhAsTe9vlT0wsdXQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*4K_Bq1AhAsTe9vlT0wsdXQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*4K_Bq1AhAsTe9vlT0wsdXQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*4K_Bq1AhAsTe9vlT0wsdXQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*4K_Bq1AhAsTe9vlT0wsdXQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*4K_Bq1AhAsTe9vlT0wsdXQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*4K_Bq1AhAsTe9vlT0wsdXQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">In RoIPool, a full forward pass of the image is created and the conv features for each region of interest are extracted from the resulting forward pass. Source: Stanford’s CS231N slides by Fei Fei Li, Andrei Karpathy, and Justin Johnson.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This is exactly what Fast R-CNN does using a technique known as RoIPool (Region of Interest Pooling). At its core, RoIPool shares the forward pass of a CNN for an image across its subregions. In the image above, notice how the CNN features for each region are obtained by selecting a corresponding region from the CNN’s feature map. Then, the features in each region are pooled (usually using max pooling). So all it takes us is one pass of the original image as opposed to ~2000!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Fast R-CNN Insight 2: Combine All Models into One Network</strong></p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*E_P1vAEbGT4HNYjqMtIz4g.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*E_P1vAEbGT4HNYjqMtIz4g.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*E_P1vAEbGT4HNYjqMtIz4g.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*E_P1vAEbGT4HNYjqMtIz4g.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*E_P1vAEbGT4HNYjqMtIz4g.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*E_P1vAEbGT4HNYjqMtIz4g.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E_P1vAEbGT4HNYjqMtIz4g.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*E_P1vAEbGT4HNYjqMtIz4g.png 640w, https://miro.medium.com/v2/resize:fit:720/1*E_P1vAEbGT4HNYjqMtIz4g.png 720w, https://miro.medium.com/v2/resize:fit:750/1*E_P1vAEbGT4HNYjqMtIz4g.png 750w, https://miro.medium.com/v2/resize:fit:786/1*E_P1vAEbGT4HNYjqMtIz4g.png 786w, https://miro.medium.com/v2/resize:fit:828/1*E_P1vAEbGT4HNYjqMtIz4g.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*E_P1vAEbGT4HNYjqMtIz4g.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*E_P1vAEbGT4HNYjqMtIz4g.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*E_P1vAEbGT4HNYjqMtIz4g.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Fast R-CNN combined the CNN, classifier, and bounding box regressor into one, single network. Source: <a href="https://www.slideshare.net/simplyinsimple/detection-52781995" target="_self">https://www.slideshare.net/simplyinsimple/detection-52781995</a>.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The second insight of Fast R-CNN is to jointly train the CNN, classifier, and bounding box regressor in a single model. Where earlier we had different models to extract image features (CNN), classify (SVM), and tighten bounding boxes (regressor), <strong>Fast R-CNN instead used a single network to compute all three.</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">You can see how this was done in the image above. Fast R-CNN replaced the SVM classifier with a softmax layer on top of the CNN to output a classification. It also added a linear regression layer parallel to the softmax layer to output bounding box coordinates. In this way, all the outputs needed came from one single network! Here are the inputs and outputs to this overall model:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><strong>Inputs</strong>: Images with region proposals.</li><li class="ff3" style="font-size:22px;"><strong>Outputs</strong>: Object classifications of each region along with tighter bounding boxes.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">2016: Faster R-CNN - Speeding Up Region Proposal</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Even with all these advancements, there was still one remaining bottleneck in the Fast R-CNN process — the region proposer. As we saw, the very first step to detecting the locations of objects is generating a bunch of potential bounding boxes or regions of interest to test. In Fast R-CNN, these proposals were created using <strong>Selective Search</strong>, a fairly slow process that was found to be the bottleneck of the overall process.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*xY9rmw06KZWQlNIPk6ItqA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*xY9rmw06KZWQlNIPk6ItqA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*xY9rmw06KZWQlNIPk6ItqA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*xY9rmw06KZWQlNIPk6ItqA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*xY9rmw06KZWQlNIPk6ItqA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*xY9rmw06KZWQlNIPk6ItqA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xY9rmw06KZWQlNIPk6ItqA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*xY9rmw06KZWQlNIPk6ItqA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*xY9rmw06KZWQlNIPk6ItqA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*xY9rmw06KZWQlNIPk6ItqA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*xY9rmw06KZWQlNIPk6ItqA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*xY9rmw06KZWQlNIPk6ItqA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*xY9rmw06KZWQlNIPk6ItqA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*xY9rmw06KZWQlNIPk6ItqA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*xY9rmw06KZWQlNIPk6ItqA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Jian Sun, a principal researcher at Microsoft Research, led the team behind Faster R-CNN. Source: <a href="https://blogs.microsoft.com/next/2015/12/10/microsoft-researchers-win-imagenet-computer-vision-challenge/#sm.00017fqnl1bz6fqf11amuo0d9ttdp" target="_self">https://blogs.microsoft.com/next/2015/12/10/microsoft-researchers-win-imagenet-computer-vision-challenge/#sm.00017fqnl1bz6fqf11amuo0d9ttdp</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In the middle 2015, a team at Microsoft Research composed of Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun, found a way to make the region proposal step almost cost free through an architecture they (creatively) named Faster R-CNN.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The insight of Faster R-CNN was that region proposals depended on features of the image that were already calculated with the forward pass of the CNN (first step of classification). <strong>So why not reuse those same CNN results for region proposals instead of running a separate selective search algorithm?</strong></p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:88%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 650px" srcset="https://miro.medium.com/v2/resize:fit:640/0*_nNI03ESXm2P6YXO. 640w, https://miro.medium.com/v2/resize:fit:720/0*_nNI03ESXm2P6YXO. 720w, https://miro.medium.com/v2/resize:fit:750/0*_nNI03ESXm2P6YXO. 750w, https://miro.medium.com/v2/resize:fit:786/0*_nNI03ESXm2P6YXO. 786w, https://miro.medium.com/v2/resize:fit:828/0*_nNI03ESXm2P6YXO. 828w, https://miro.medium.com/v2/resize:fit:1100/0*_nNI03ESXm2P6YXO. 1100w, https://miro.medium.com/v2/resize:fit:1300/0*_nNI03ESXm2P6YXO. 1300w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 650px" srcset="https://miro.medium.com/v2/resize:fit:640/0*_nNI03ESXm2P6YXO. 640w, https://miro.medium.com/v2/resize:fit:720/0*_nNI03ESXm2P6YXO. 720w, https://miro.medium.com/v2/resize:fit:750/0*_nNI03ESXm2P6YXO. 750w, https://miro.medium.com/v2/resize:fit:786/0*_nNI03ESXm2P6YXO. 786w, https://miro.medium.com/v2/resize:fit:828/0*_nNI03ESXm2P6YXO. 828w, https://miro.medium.com/v2/resize:fit:1100/0*_nNI03ESXm2P6YXO. 1100w, https://miro.medium.com/v2/resize:fit:1300/0*_nNI03ESXm2P6YXO. 1300w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/650/0*_nNI03ESXm2P6YXO."></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">In Faster R-CNN, a single CNN is used for region proposals, and classifications. Source: <a href="https://arxiv.org/abs/1506.01497" target="_self">https://arxiv.org/abs/1506.01497</a>.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Indeed, this is just what the Faster R-CNN team achieved. In the image above, you can see how a single CNN is used to both carry out region proposals and classification. This way, <strong>only one CNN needs to be trained</strong> and we get region proposals almost for free! The authors write:</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4">Our observation is that the convolutional feature maps used by region-based detectors, like Fast R- CNN, can also be used for generating region proposals [thus enabling nearly cost-free region proposals].</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here are the inputs and outputs of their model:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><strong>Inputs</strong>: Images (Notice how region proposals are not needed).</li><li class="ff3" style="font-size:22px;"><strong>Outputs</strong>: Classifications and bounding box coordinates of objects in the images.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>How the Regions are Generated</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Let’s take a moment to see how Faster R-CNN generates these region proposals from CNN features. Faster R-CNN adds a Fully Convolutional Network on top of the features of the CNN creating what’s known as the <strong>Region Proposal Network</strong>.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*n6pZEyvW47nlcdQz. 640w, https://miro.medium.com/v2/resize:fit:720/0*n6pZEyvW47nlcdQz. 720w, https://miro.medium.com/v2/resize:fit:750/0*n6pZEyvW47nlcdQz. 750w, https://miro.medium.com/v2/resize:fit:786/0*n6pZEyvW47nlcdQz. 786w, https://miro.medium.com/v2/resize:fit:828/0*n6pZEyvW47nlcdQz. 828w, https://miro.medium.com/v2/resize:fit:1100/0*n6pZEyvW47nlcdQz. 1100w, https://miro.medium.com/v2/resize:fit:1400/0*n6pZEyvW47nlcdQz. 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*n6pZEyvW47nlcdQz. 640w, https://miro.medium.com/v2/resize:fit:720/0*n6pZEyvW47nlcdQz. 720w, https://miro.medium.com/v2/resize:fit:750/0*n6pZEyvW47nlcdQz. 750w, https://miro.medium.com/v2/resize:fit:786/0*n6pZEyvW47nlcdQz. 786w, https://miro.medium.com/v2/resize:fit:828/0*n6pZEyvW47nlcdQz. 828w, https://miro.medium.com/v2/resize:fit:1100/0*n6pZEyvW47nlcdQz. 1100w, https://miro.medium.com/v2/resize:fit:1400/0*n6pZEyvW47nlcdQz. 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*n6pZEyvW47nlcdQz."></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">The Region Proposal Network slides a window over the features of the CNN. At each window location, the network outputs a score and a bounding box per anchor (hence 4k box coordinates where k is the number of anchors). Source: <a href="https://arxiv.org/abs/1506.01497" target="_self">https://arxiv.org/abs/1506.01497</a>.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The Region Proposal Network works by passing a sliding window over the CNN feature map and at each window, outputting <strong>k </strong>potential bounding boxes and scores for how good each of those boxes is expected to be. What do these <strong>k </strong>boxes represent?</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:41%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 320px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*pJ3OTVXjtp9vWfBOPsnWIw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*pJ3OTVXjtp9vWfBOPsnWIw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*pJ3OTVXjtp9vWfBOPsnWIw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*pJ3OTVXjtp9vWfBOPsnWIw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*pJ3OTVXjtp9vWfBOPsnWIw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*pJ3OTVXjtp9vWfBOPsnWIw.png 1100w, https://miro.medium.com/v2/resize:fit:640/format:webp/1*pJ3OTVXjtp9vWfBOPsnWIw.png 640w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 320px" srcset="https://miro.medium.com/v2/resize:fit:640/1*pJ3OTVXjtp9vWfBOPsnWIw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*pJ3OTVXjtp9vWfBOPsnWIw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*pJ3OTVXjtp9vWfBOPsnWIw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*pJ3OTVXjtp9vWfBOPsnWIw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*pJ3OTVXjtp9vWfBOPsnWIw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*pJ3OTVXjtp9vWfBOPsnWIw.png 1100w, https://miro.medium.com/v2/resize:fit:640/1*pJ3OTVXjtp9vWfBOPsnWIw.png 640w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/320/1*pJ3OTVXjtp9vWfBOPsnWIw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">We know that the bounding boxes for people tend to be rectangular and vertical. We can use this intuition to guide our Region Proposal networks through creating an anchor of such dimensions. Image Source: <a href="http://vlm1.uta.edu/~athitsos/courses/cse6367_spring2011/assignments/assignment1/bbox0062.jpg" target="_self">http://vlm1.uta.edu/~athitsos/courses/cse6367_spring2011/assignments/assignment1/bbox0062.jpg</a>.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Intuitively, we know that objects in an image should fit certain common aspect ratios and sizes. For instance, we know that we want some rectangular boxes that resemble the shapes of humans. Likewise, we know we won’t see many boxes that are very very thin. In such a way, we create <strong>k</strong> such common aspect ratios we call <strong>anchor boxes</strong>. For each such anchor box, we output one bounding box and score per position in the image.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">With these anchor boxes in mind, let’s take a look at the inputs and outputs to this Region Proposal Network:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><strong>Inputs</strong>: CNN Feature Map.</li><li class="ff3" style="font-size:22px;"><strong>Outputs</strong>: A bounding box per anchor. A score representing how likely the image in that bounding box will be an object.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We then pass each such bounding box that is likely to be an object into Fast R-CNN to generate a classification and tightened bounding boxes.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">2017: Mask R-CNN - Extending Faster R-CNN for Pixel Level Segmentation</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*E_5qBTrotLzclyaxsekBmQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*E_5qBTrotLzclyaxsekBmQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*E_5qBTrotLzclyaxsekBmQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*E_5qBTrotLzclyaxsekBmQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*E_5qBTrotLzclyaxsekBmQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*E_5qBTrotLzclyaxsekBmQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E_5qBTrotLzclyaxsekBmQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*E_5qBTrotLzclyaxsekBmQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*E_5qBTrotLzclyaxsekBmQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*E_5qBTrotLzclyaxsekBmQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*E_5qBTrotLzclyaxsekBmQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*E_5qBTrotLzclyaxsekBmQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*E_5qBTrotLzclyaxsekBmQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*E_5qBTrotLzclyaxsekBmQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*E_5qBTrotLzclyaxsekBmQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">The goal of image instance segmentation is to identify, at a pixel level, what the different objets in a scene are. Source: <a href="https://arxiv.org/abs/1703.06870" target="_self">https://arxiv.org/abs/1703.06870</a>.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">So far, we’ve seen how we’ve been able to use CNN features in many interesting ways to effectively locate different objects in an image with bounding boxes.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Can we extend such techniques to go one step further and locate exact pixels of each object instead of just bounding boxes? This problem, known as image segmentation, is what Kaiming He and a team of researchers, including Girshick, explored at Facebook AI using an architecture known as <strong>Mask R-CNN</strong>.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:32%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 256px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*cYW3EdKx75Stl1EreATdfw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*cYW3EdKx75Stl1EreATdfw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*cYW3EdKx75Stl1EreATdfw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*cYW3EdKx75Stl1EreATdfw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*cYW3EdKx75Stl1EreATdfw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*cYW3EdKx75Stl1EreATdfw.png 1100w, https://miro.medium.com/v2/resize:fit:512/format:webp/1*cYW3EdKx75Stl1EreATdfw.png 512w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 256px" srcset="https://miro.medium.com/v2/resize:fit:640/1*cYW3EdKx75Stl1EreATdfw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*cYW3EdKx75Stl1EreATdfw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*cYW3EdKx75Stl1EreATdfw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*cYW3EdKx75Stl1EreATdfw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*cYW3EdKx75Stl1EreATdfw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*cYW3EdKx75Stl1EreATdfw.png 1100w, https://miro.medium.com/v2/resize:fit:512/1*cYW3EdKx75Stl1EreATdfw.png 512w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/256/1*cYW3EdKx75Stl1EreATdfw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Kaiming He, a researcher at Facebook AI, is lead author of Mask R-CNN and also a coauthor of Faster R-CNN.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Much like Fast R-CNN, and Faster R-CNN, Mask R-CNN’s underlying intuition is straight forward. Given that Faster R-CNN works so well for object detection, could we extend it to also carry out pixel level segmentation?</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:95%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 696px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*BiRpf-ogjxARQf5LxI17Jw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*BiRpf-ogjxARQf5LxI17Jw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*BiRpf-ogjxARQf5LxI17Jw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*BiRpf-ogjxARQf5LxI17Jw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*BiRpf-ogjxARQf5LxI17Jw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*BiRpf-ogjxARQf5LxI17Jw.png 1100w, https://miro.medium.com/v2/resize:fit:1392/format:webp/1*BiRpf-ogjxARQf5LxI17Jw.png 1392w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 696px" srcset="https://miro.medium.com/v2/resize:fit:640/1*BiRpf-ogjxARQf5LxI17Jw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*BiRpf-ogjxARQf5LxI17Jw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*BiRpf-ogjxARQf5LxI17Jw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*BiRpf-ogjxARQf5LxI17Jw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*BiRpf-ogjxARQf5LxI17Jw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*BiRpf-ogjxARQf5LxI17Jw.png 1100w, https://miro.medium.com/v2/resize:fit:1392/1*BiRpf-ogjxARQf5LxI17Jw.png 1392w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/696/1*BiRpf-ogjxARQf5LxI17Jw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">In Mask R-CNN, a Fully Convolutional Network (FCN) is added on top of the CNN features of Faster R-CNN to generate a mask (segmentation output). Notice how this is in parallel to the classification and bounding box regression network of Faster R-CNN. Source: <a href="https://arxiv.org/abs/1703.06870" target="_self">https://arxiv.org/abs/1703.06870</a>.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Mask R-CNN does this by adding a branch to Faster R-CNN that outputs a binary mask that says whether or not a given pixel is part of an object. The branch (in white in the above image), as before, is just a Fully Convolutional Network on top of a CNN based feature map. Here are its inputs and outputs:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><strong>Inputs</strong>: CNN Feature Map.</li><li class="ff3" style="font-size:22px;"><strong>Outputs</strong>: Matrix with 1s on all locations where the pixel belongs to the object and 0s elsewhere (this is known as a <a href="https://en.wikipedia.org/wiki/Mask_(computing)" target="_self">binary mask</a>).</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">But the Mask R-CNN authors had to make one small adjustment to make this pipeline work as expected.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>RoiAlign - Realigning RoIPool to be More Accurate</strong></p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:94%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 692px" srcset="https://miro.medium.com/v2/resize:fit:640/0*KtaZfpUErYqwH4RX. 640w, https://miro.medium.com/v2/resize:fit:720/0*KtaZfpUErYqwH4RX. 720w, https://miro.medium.com/v2/resize:fit:750/0*KtaZfpUErYqwH4RX. 750w, https://miro.medium.com/v2/resize:fit:786/0*KtaZfpUErYqwH4RX. 786w, https://miro.medium.com/v2/resize:fit:828/0*KtaZfpUErYqwH4RX. 828w, https://miro.medium.com/v2/resize:fit:1100/0*KtaZfpUErYqwH4RX. 1100w, https://miro.medium.com/v2/resize:fit:1384/0*KtaZfpUErYqwH4RX. 1384w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 692px" srcset="https://miro.medium.com/v2/resize:fit:640/0*KtaZfpUErYqwH4RX. 640w, https://miro.medium.com/v2/resize:fit:720/0*KtaZfpUErYqwH4RX. 720w, https://miro.medium.com/v2/resize:fit:750/0*KtaZfpUErYqwH4RX. 750w, https://miro.medium.com/v2/resize:fit:786/0*KtaZfpUErYqwH4RX. 786w, https://miro.medium.com/v2/resize:fit:828/0*KtaZfpUErYqwH4RX. 828w, https://miro.medium.com/v2/resize:fit:1100/0*KtaZfpUErYqwH4RX. 1100w, https://miro.medium.com/v2/resize:fit:1384/0*KtaZfpUErYqwH4RX. 1384w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/692/0*KtaZfpUErYqwH4RX."></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Instead of RoIPool, the image gets passed through RoIAlign so that the regions of the feature map selected by RoIPool correspond more precisely to the regions of the original image. This is needed because pixel level segmentation requires more fine-grained alignment than bounding boxes. Source: <a href="https://arxiv.org/abs/1703.06870" target="_self">https://arxiv.org/abs/1703.06870</a>.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">When run without modifications on the original Faster R-CNN architecture, the Mask R-CNN authors realized that the regions of the feature map selected by RoIPool were slightly misaligned from the regions of the original image. Since image segmentation requires pixel level specificity, unlike bounding boxes, this naturally led to inaccuracies.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The authors were able to solve this problem by cleverly adjusting RoIPool to be more precisely aligned using a method known as RoIAlign.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*VDGql5VDbLWU3jOhRmzwFQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*VDGql5VDbLWU3jOhRmzwFQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*VDGql5VDbLWU3jOhRmzwFQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*VDGql5VDbLWU3jOhRmzwFQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*VDGql5VDbLWU3jOhRmzwFQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*VDGql5VDbLWU3jOhRmzwFQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VDGql5VDbLWU3jOhRmzwFQ.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*VDGql5VDbLWU3jOhRmzwFQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*VDGql5VDbLWU3jOhRmzwFQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*VDGql5VDbLWU3jOhRmzwFQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*VDGql5VDbLWU3jOhRmzwFQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*VDGql5VDbLWU3jOhRmzwFQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*VDGql5VDbLWU3jOhRmzwFQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*VDGql5VDbLWU3jOhRmzwFQ.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*VDGql5VDbLWU3jOhRmzwFQ.jpeg"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">How do we accurately map a region of interest from the original image onto the feature map?</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Imagine we have an image of size <strong>128x128</strong> and a feature map of size <strong>25x25</strong>. Let’s imagine we want features the region corresponding to the top-left <strong>15x15</strong> pixels in the original image (see above). How might we select these pixels from the feature map?</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We know each pixel in the original image corresponds to ~ 25/128 pixels in the feature map. To select 15 pixels from the original image, we just select 15 * 25/128 ~= <strong>2.93</strong> pixels.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In RoIPool, we would round this down and select 2 pixels causing a slight misalignment. However, in RoIAlign, <strong>we avoid such rounding.</strong> Instead, we use <a href="https://en.wikipedia.org/wiki/Bilinear_interpolation" target="_self">bilinear interpolation</a> to get a precise idea of what would be at pixel 2.93. This, at a high level, is what allows us to avoid the misalignments caused by RoIPool.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Once these masks are generated, Mask R-CNN combines them with the classifications and bounding boxes from Faster R-CNN to generate such wonderfully precise segmentations:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*6CClgIKH8zhZjmcftfNoEQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*6CClgIKH8zhZjmcftfNoEQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*6CClgIKH8zhZjmcftfNoEQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*6CClgIKH8zhZjmcftfNoEQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*6CClgIKH8zhZjmcftfNoEQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*6CClgIKH8zhZjmcftfNoEQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6CClgIKH8zhZjmcftfNoEQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*6CClgIKH8zhZjmcftfNoEQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*6CClgIKH8zhZjmcftfNoEQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*6CClgIKH8zhZjmcftfNoEQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*6CClgIKH8zhZjmcftfNoEQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*6CClgIKH8zhZjmcftfNoEQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*6CClgIKH8zhZjmcftfNoEQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*6CClgIKH8zhZjmcftfNoEQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*6CClgIKH8zhZjmcftfNoEQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Mask R-CNN is able to segment as well as classify the objects in an image. Source: <a href="https://arxiv.org/abs/1703.06870" target="_self">https://arxiv.org/abs/1703.06870</a>.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Code</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you’re interested in trying out these algorithms yourselves, here are relevant repositories:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Faster R-CNN</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Caffe: <a href="https://github.com/rbgirshick/py-faster-rcnn" target="_self">https://github.com/rbgirshick/py-faster-rcnn</a></li><li class="ff3" style="font-size:22px;">PyTorch: <a href="https://github.com/longcw/faster_rcnn_pytorch" target="_self">https://github.com/longcw/faster_rcnn_pytorch</a></li><li class="ff3" style="font-size:22px;">MatLab: <a href="https://github.com/ShaoqingRen/faster_rcnn" target="_self">https://github.com/ShaoqingRen/faster_rcnn</a></li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Mask R-CNN</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">PyTorch: <a href="https://github.com/felixgwu/mask_rcnn_pytorch" target="_self">https://github.com/felixgwu/mask_rcnn_pytorch</a></li><li class="ff3" style="font-size:22px;">TensorFlow: <a href="https://github.com/CharlesShang/FastMaskRCNN" target="_self">https://github.com/CharlesShang/FastMaskRCNN</a></li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Looking Forward</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In just 3 years, we’ve seen how the research community has progressed from Krizhevsky et. al’s original result to R-CNN, and finally all the way to such powerful results as Mask R-CNN. Seen in isolation, results like Mask R-CNN seem like incredible leaps of genius that would be unapproachable. Yet, through this post, I hope you’ve seen how such advancements are really the sum of intuitive, incremental improvements through years of hard work and collaboration. Each of the ideas proposed by R-CNN, Fast R-CNN, Faster R-CNN, and finally Mask R-CNN were not necessarily quantum leaps, yet their sum products have led to really remarkable results that bring us closer to a human level understanding of sight.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">What particularly excites me, is that the time between R-CNN and Mask R-CNN was just three years! With continued funding, focus, and support, how much further can Computer Vision improve over the next three years?</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you see any errors or issues in this post, please contact me at dhruv@getathelas.com and I”ll immediately correct them!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you’re interested in applying such techniques, come join us at <a href="http://athelas.com" target="_self">Athelas</a> where we apply Computer Vision to blood diagnostics daily:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><div class="sketchy"><a href="https://athelas.com"><h2 style="color:blueviolet; font-family:Arial, Helvetica, sans-serif; font-size:25px;">Athelas</h2><h3 style="color:rgb(45, 34, 54); font-family:Arial, Helvetica, sans-serif; font-size:20px;">In-home blood diagnostic</h3><p>athelas.com</p></a></div></div></div></div></section><br><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Other posts we’ve written:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><div class="sketchy"><a href="https://medium.com/classifying-white-blood-cells-with-convolutional-neural-networks-2ca6da239331"><h2 style="color:blueviolet; font-family:Arial, Helvetica, sans-serif; font-size:25px;">Classifying White Blood Cells With Deep Learning (Code and data included!)</h2><h3 style="color:rgb(45, 34, 54); font-family:Arial, Helvetica, sans-serif; font-size:20px;">You can follow all the code and recreate the results of this post here.</h3><p>blog.athelas.com</p></a></div></div></div></div></section><br><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><div class="sketchy"><a href="https://medium.com/paper-1-baidus-deep-voice-675a323705df"><h2 style="color:blueviolet; font-family:Arial, Helvetica, sans-serif; font-size:25px;">Baidu Deep Voice explained: Part 1 — the Inference Pipeline</h2><h3 style="color:rgb(45, 34, 54); font-family:Arial, Helvetica, sans-serif; font-size:20px;">This post is the first in what I hope to be a series covering recently published ML/AI papers that I think are…</h3><p>blog.athelas.com</p></a></div></div></div></div></section><br><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><div class="sketchy"><a href="https://medium.com/@dhruvp/how-to-write-a-neural-network-to-play-pong-from-scratch-956b57d4f6e0"><h2 style="color:blueviolet; font-family:Arial, Helvetica, sans-serif; font-size:25px;">Write an AI to win at Pong from scratch with Reinforcement Learning</h2><h3 style="color:rgb(45, 34, 54); font-family:Arial, Helvetica, sans-serif; font-size:20px;">There’s a huge difference between reading about Reinforcement Learning and actually implementing it.</h3><p>medium.com</p></a></div></div></div></div></section><br><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>Thanks to Bharath Ramsundar, </em><a style="color:green" href="https://medium.com/u/a55cb8c3de88?source=post_page-----34ea83205de4--------------------------------" target="_self">Pranav Ramkrishnan</a><em>, Tanay Tandon, and </em><a style="color:green" href="https://medium.com/u/fd8bc37755d8?source=post_page-----34ea83205de4--------------------------------" target="_self">Oliver Cameron</a><em> for help with this post!</em></p></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>