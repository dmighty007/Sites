<!DOCTYPE html>
                <html>
                <head>
                    <title>R-CNN, Fast R-CNN, Faster R-CNN, YOLO — Object Detection Algorithms</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://medium.com/@grohith327?source=post_page-----36d53571365e--------------------------------">Author : Rohith Gandhi</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>R-CNN, Fast R-CNN, Faster R-CNN, YOLO — Object Detection Algorithms</h3></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5 text-muted">Understanding object detection algorithms</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*PpYSe5EbqFtAEfVVEOAMlg.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*PpYSe5EbqFtAEfVVEOAMlg.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*PpYSe5EbqFtAEfVVEOAMlg.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*PpYSe5EbqFtAEfVVEOAMlg.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*PpYSe5EbqFtAEfVVEOAMlg.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*PpYSe5EbqFtAEfVVEOAMlg.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PpYSe5EbqFtAEfVVEOAMlg.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*PpYSe5EbqFtAEfVVEOAMlg.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*PpYSe5EbqFtAEfVVEOAMlg.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*PpYSe5EbqFtAEfVVEOAMlg.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*PpYSe5EbqFtAEfVVEOAMlg.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*PpYSe5EbqFtAEfVVEOAMlg.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*PpYSe5EbqFtAEfVVEOAMlg.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*PpYSe5EbqFtAEfVVEOAMlg.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*PpYSe5EbqFtAEfVVEOAMlg.jpeg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Introduction</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Computer vision is an interdisciplinary field that has been gaining huge amounts of traction in the recent years(since CNN) and self-driving cars have taken centre stage. Another integral part of computer vision is object detection. Object detection aids in pose estimation, vehicle detection, surveillance etc. The difference between object detection algorithms and classification algorithms is that in detection algorithms, we try to draw a bounding box around the object of interest to locate it within the image. Also, you might not necessarily draw just one bounding box in an object detection case, there could be many bounding boxes representing different objects of interest within the image and you would not know how many beforehand.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*95lJePt-70PH3PoVfz2yYQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*95lJePt-70PH3PoVfz2yYQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*95lJePt-70PH3PoVfz2yYQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*95lJePt-70PH3PoVfz2yYQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*95lJePt-70PH3PoVfz2yYQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*95lJePt-70PH3PoVfz2yYQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*95lJePt-70PH3PoVfz2yYQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*95lJePt-70PH3PoVfz2yYQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*95lJePt-70PH3PoVfz2yYQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*95lJePt-70PH3PoVfz2yYQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*95lJePt-70PH3PoVfz2yYQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*95lJePt-70PH3PoVfz2yYQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*95lJePt-70PH3PoVfz2yYQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*95lJePt-70PH3PoVfz2yYQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*95lJePt-70PH3PoVfz2yYQ.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*tqbGt2zj-9OP_rjR57r8Zg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*tqbGt2zj-9OP_rjR57r8Zg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*tqbGt2zj-9OP_rjR57r8Zg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*tqbGt2zj-9OP_rjR57r8Zg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*tqbGt2zj-9OP_rjR57r8Zg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*tqbGt2zj-9OP_rjR57r8Zg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tqbGt2zj-9OP_rjR57r8Zg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*tqbGt2zj-9OP_rjR57r8Zg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*tqbGt2zj-9OP_rjR57r8Zg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*tqbGt2zj-9OP_rjR57r8Zg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*tqbGt2zj-9OP_rjR57r8Zg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*tqbGt2zj-9OP_rjR57r8Zg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*tqbGt2zj-9OP_rjR57r8Zg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*tqbGt2zj-9OP_rjR57r8Zg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*tqbGt2zj-9OP_rjR57r8Zg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The major reason why you cannot proceed with this problem by building a standard convolutional network followed by a fully connected layer is that, the length of the output layer is variable — not constant, this is because the number of occurrences of the objects of interest is not fixed. A naive approach to solve this problem would be to take different regions of interest from the image, and use a CNN to classify the presence of the object within that region. The problem with this approach is that the objects of interest might have different spatial locations within the image and different aspect ratios. Hence, you would have to select a huge number of regions and this could computationally blow up. Therefore, algorithms like R-CNN, YOLO etc have been developed to find these occurrences and find them fast.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">R-CNN</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To bypass the problem of selecting a huge number of regions, <a href="https://arxiv.org/pdf/1311.2524.pdf" target="_self">Ross Girshick et al</a>. proposed a method where we use selective search to extract just 2000 regions from the image and he called them region proposals. Therefore, now, instead of trying to classify a huge number of regions, you can just work with 2000 regions. These 2000 region proposals are generated using the selective search algorithm which is written below.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span><mark>Selective Search:<br/>1. Generate initial sub-segmentation, we generate many candidate     regions<br/>2. Use greedy algorithm to recursively combine similar regions into larger ones <br/>3. Use the generated regions to produce the final candidate region proposals</mark> </span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*REPHY47zAyzgbNKC6zlvBQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*REPHY47zAyzgbNKC6zlvBQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*REPHY47zAyzgbNKC6zlvBQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*REPHY47zAyzgbNKC6zlvBQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*REPHY47zAyzgbNKC6zlvBQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*REPHY47zAyzgbNKC6zlvBQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*REPHY47zAyzgbNKC6zlvBQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*REPHY47zAyzgbNKC6zlvBQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*REPHY47zAyzgbNKC6zlvBQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*REPHY47zAyzgbNKC6zlvBQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*REPHY47zAyzgbNKC6zlvBQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*REPHY47zAyzgbNKC6zlvBQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*REPHY47zAyzgbNKC6zlvBQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*REPHY47zAyzgbNKC6zlvBQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*REPHY47zAyzgbNKC6zlvBQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">R-CNN</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To know more about the selective search algorithm, follow this <a href="https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf" target="_self">link</a>. These 2000 candidate region proposals are warped into a square and fed into a convolutional neural network that produces a 4096-dimensional feature vector as output. The CNN acts as a feature extractor and the output dense layer consists of the features extracted from the image and the extracted features are fed into an <a href="https://medium.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47" target="_self">SVM</a> to classify the presence of the object within that candidate region proposal. In addition to predicting the presence of an object within the region proposals, the algorithm also predicts four values which are offset values to increase the precision of the bounding box. For example, given a region proposal, the algorithm would have predicted the presence of a person but the face of that person within that region proposal could’ve been cut in half. Therefore, the offset values help in adjusting the bounding box of the region proposal.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:75%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 556px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*NX5yYTi-eQjP0pMWs3UbUg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*NX5yYTi-eQjP0pMWs3UbUg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*NX5yYTi-eQjP0pMWs3UbUg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*NX5yYTi-eQjP0pMWs3UbUg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*NX5yYTi-eQjP0pMWs3UbUg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*NX5yYTi-eQjP0pMWs3UbUg.png 1100w, https://miro.medium.com/v2/resize:fit:1112/format:webp/1*NX5yYTi-eQjP0pMWs3UbUg.png 1112w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 556px" srcset="https://miro.medium.com/v2/resize:fit:640/1*NX5yYTi-eQjP0pMWs3UbUg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*NX5yYTi-eQjP0pMWs3UbUg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*NX5yYTi-eQjP0pMWs3UbUg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*NX5yYTi-eQjP0pMWs3UbUg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*NX5yYTi-eQjP0pMWs3UbUg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*NX5yYTi-eQjP0pMWs3UbUg.png 1100w, https://miro.medium.com/v2/resize:fit:1112/1*NX5yYTi-eQjP0pMWs3UbUg.png 1112w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/556/1*NX5yYTi-eQjP0pMWs3UbUg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">R-CNN</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Problems with R-CNN</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">It still takes a huge amount of time to train the network as you would have to classify 2000 region proposals per image.</li><li class="ff3" style="font-size:22px;">It cannot be implemented real time as it takes around 47 seconds for each test image.</li><li class="ff3" style="font-size:22px;">The selective search algorithm is a fixed algorithm. Therefore, no learning is happening at that stage. This could lead to the generation of bad candidate region proposals.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Fast R-CNN</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*0pMP3aY8blSpva5tvWbnKA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*0pMP3aY8blSpva5tvWbnKA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*0pMP3aY8blSpva5tvWbnKA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*0pMP3aY8blSpva5tvWbnKA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*0pMP3aY8blSpva5tvWbnKA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*0pMP3aY8blSpva5tvWbnKA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0pMP3aY8blSpva5tvWbnKA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*0pMP3aY8blSpva5tvWbnKA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*0pMP3aY8blSpva5tvWbnKA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*0pMP3aY8blSpva5tvWbnKA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*0pMP3aY8blSpva5tvWbnKA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*0pMP3aY8blSpva5tvWbnKA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*0pMP3aY8blSpva5tvWbnKA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*0pMP3aY8blSpva5tvWbnKA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*0pMP3aY8blSpva5tvWbnKA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Fast R-CNN</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The same author of the previous paper(R-CNN) solved some of the drawbacks of R-CNN to build a faster object detection algorithm and it was called Fast R-CNN. The approach is similar to the R-CNN algorithm. But, instead of feeding the region proposals to the CNN, we feed the input image to the CNN to generate a convolutional feature map. From the convolutional feature map, we identify the region of proposals and warp them into squares and by using a RoI pooling layer we reshape them into a fixed size so that it can be fed into a fully connected layer. From the RoI feature vector, we use a softmax layer to predict the class of the proposed region and also the offset values for the bounding box.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The reason “Fast R-CNN” is faster than R-CNN is because you don’t have to feed 2000 region proposals to the convolutional neural network every time. Instead, the convolution operation is done only once per image and a feature map is generated from it.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*m2QO_wbUPA05mY2q4v7mjg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*m2QO_wbUPA05mY2q4v7mjg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*m2QO_wbUPA05mY2q4v7mjg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*m2QO_wbUPA05mY2q4v7mjg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*m2QO_wbUPA05mY2q4v7mjg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*m2QO_wbUPA05mY2q4v7mjg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m2QO_wbUPA05mY2q4v7mjg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*m2QO_wbUPA05mY2q4v7mjg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*m2QO_wbUPA05mY2q4v7mjg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*m2QO_wbUPA05mY2q4v7mjg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*m2QO_wbUPA05mY2q4v7mjg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*m2QO_wbUPA05mY2q4v7mjg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*m2QO_wbUPA05mY2q4v7mjg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*m2QO_wbUPA05mY2q4v7mjg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*m2QO_wbUPA05mY2q4v7mjg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Comparison of object detection algorithms</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">From the above graphs, you can infer that Fast R-CNN is significantly faster in training and testing sessions over R-CNN. When you look at the performance of Fast R-CNN during testing time, including region proposals slows down the algorithm significantly when compared to not using region proposals. Therefore, region proposals become bottlenecks in Fast R-CNN algorithm affecting its performance.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Faster R-CNN</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*pSnVmJCyQIRKHDPt3cfnXA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*pSnVmJCyQIRKHDPt3cfnXA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*pSnVmJCyQIRKHDPt3cfnXA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*pSnVmJCyQIRKHDPt3cfnXA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*pSnVmJCyQIRKHDPt3cfnXA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*pSnVmJCyQIRKHDPt3cfnXA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pSnVmJCyQIRKHDPt3cfnXA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*pSnVmJCyQIRKHDPt3cfnXA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*pSnVmJCyQIRKHDPt3cfnXA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*pSnVmJCyQIRKHDPt3cfnXA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*pSnVmJCyQIRKHDPt3cfnXA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*pSnVmJCyQIRKHDPt3cfnXA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*pSnVmJCyQIRKHDPt3cfnXA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*pSnVmJCyQIRKHDPt3cfnXA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*pSnVmJCyQIRKHDPt3cfnXA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Faster R-CNN</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Both of the above algorithms(R-CNN & Fast R-CNN) uses selective search to find out the region proposals. Selective search is a slow and time-consuming process affecting the performance of the network. Therefore, <a href="https://arxiv.org/pdf/1506.01497.pdf" target="_self">Shaoqing Ren et al</a>. came up with an object detection algorithm that eliminates the selective search algorithm and lets the network learn the region proposals.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Similar to Fast R-CNN, the image is provided as an input to a convolutional network which provides a convolutional feature map. Instead of using selective search algorithm on the feature map to identify the region proposals, a separate network is used to predict the region proposals. The predicted region proposals are then reshaped using a RoI pooling layer which is then used to classify the image within the proposed region and predict the offset values for the bounding boxes.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*4gGddZpKeNIPBoVxYECd5w.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*4gGddZpKeNIPBoVxYECd5w.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*4gGddZpKeNIPBoVxYECd5w.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*4gGddZpKeNIPBoVxYECd5w.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*4gGddZpKeNIPBoVxYECd5w.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*4gGddZpKeNIPBoVxYECd5w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4gGddZpKeNIPBoVxYECd5w.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*4gGddZpKeNIPBoVxYECd5w.png 640w, https://miro.medium.com/v2/resize:fit:720/1*4gGddZpKeNIPBoVxYECd5w.png 720w, https://miro.medium.com/v2/resize:fit:750/1*4gGddZpKeNIPBoVxYECd5w.png 750w, https://miro.medium.com/v2/resize:fit:786/1*4gGddZpKeNIPBoVxYECd5w.png 786w, https://miro.medium.com/v2/resize:fit:828/1*4gGddZpKeNIPBoVxYECd5w.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*4gGddZpKeNIPBoVxYECd5w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*4gGddZpKeNIPBoVxYECd5w.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*4gGddZpKeNIPBoVxYECd5w.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Comparison of test-time speed of object detection algorithms</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">From the above graph, you can see that Faster R-CNN is much faster than it’s predecessors. Therefore, it can even be used for real-time object detection.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">YOLO — You Only Look Once</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:80%;"><iframe  scrolling="auto" width="435.0" height="326.0" frameborder="0" src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fgiphy.com%2Fembed%2F10VZfgNU9YBPpK%2Ftwitter%2Fiframe&url=https%3A%2F%2Fmedia.giphy.com%2Fmedia%2F10VZfgNU9YBPpK%2Fgiphy.mp4&image=https%3A%2F%2Fmedia.giphy.com%2Fmedia%2F10VZfgNU9YBPpK%2Fgiphy.gif&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=giphy" allowfullscreen=""></iframe></div><br><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">All of the previous object detection algorithms use regions to localize the object within the image. The network does not look at the complete image. Instead, parts of the image which have high probabilities of containing the object. YOLO or You Only Look Once is an object detection algorithm much different from the region based algorithms seen above. In YOLO a single convolutional network predicts the bounding boxes and the class probabilities for these boxes.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*JniWRt-ceWLNlkOULjhdpg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*JniWRt-ceWLNlkOULjhdpg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*JniWRt-ceWLNlkOULjhdpg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*JniWRt-ceWLNlkOULjhdpg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*JniWRt-ceWLNlkOULjhdpg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*JniWRt-ceWLNlkOULjhdpg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JniWRt-ceWLNlkOULjhdpg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*JniWRt-ceWLNlkOULjhdpg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*JniWRt-ceWLNlkOULjhdpg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*JniWRt-ceWLNlkOULjhdpg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*JniWRt-ceWLNlkOULjhdpg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*JniWRt-ceWLNlkOULjhdpg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*JniWRt-ceWLNlkOULjhdpg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*JniWRt-ceWLNlkOULjhdpg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*JniWRt-ceWLNlkOULjhdpg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">YOLO</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">How YOLO works is that we take an image and split it into an SxS grid, within each of the grid we take m bounding boxes. For each of the bounding box, the network outputs a class probability and offset values for the bounding box. The bounding boxes having the class probability above a threshold value is selected and used to locate the object within the image.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">YOLO is orders of magnitude faster(45 frames per second) than other object detection algorithms. The limitation of YOLO algorithm is that it struggles with small objects within the image, for example it might have difficulties in detecting a flock of birds. This is due to the spatial constraints of the algorithm.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Conclusion</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Computer vision conferences have been viewing new radical concepts each year and step by step I guess we are moving towards jaw-dropping performances from AI(if not already!). It only gets better. I hope the concepts were made lucid in this article, thank you :)</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:80%;"><iframe  scrolling="auto" width="435.0" height="298.0" frameborder="0" src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fgiphy.com%2Fembed%2FDCHmHrxi4PG92%2Ftwitter%2Fiframe&url=https%3A%2F%2Fmedia.giphy.com%2Fmedia%2FDCHmHrxi4PG92%2Fgiphy.mp4&image=https%3A%2F%2Fmedia.giphy.com%2Fmedia%2FDCHmHrxi4PG92%2Fgiphy.gif&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=giphy" allowfullscreen=""></iframe></div><br><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">References</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/pdf/1311.2524.pdf" target="_self">https://arxiv.org/pdf/1311.2524.pdf</a></li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/pdf/1504.08083.pdf" target="_self">https://arxiv.org/pdf/1504.08083.pdf</a></li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/pdf/1506.01497.pdf" target="_self">https://arxiv.org/pdf/1506.01497.pdf</a></li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/pdf/1506.02640v5.pdf" target="_self">https://arxiv.org/pdf/1506.02640v5.pdf</a></li><li class="ff3" style="font-size:22px;"><a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf" target="_self">http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf</a></li></ol></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>