<!DOCTYPE html>
                <html>
                <head>
                    <title>Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://medium.com/@ageitgey?source=post_page-----c3cffc121d78--------------------------------">Author : Adam Geitgey</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning</h3></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Update:</strong><em> This article is part of a series. Check out the full series: </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471" target="_self">Part 1</a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3" target="_self">Part 2</a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721" target="_self">Part 3</a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78" target="_self">Part 4</a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa" target="_self">Part 5</a><em>, </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a" target="_self">Part 6</a><em>, </em><a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7" target="_self">Part 7</a><em> and </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196" target="_self">Part 8</a><em>! You can also read this article in </em><a href="https://zhuanlan.zhihu.com/p/24567586" target="_self">普通话</a><em>, </em><a href="http://algotravelling.com/ru/%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5-%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5-%D1%8D%D1%82%D0%BE-%D0%B2%D0%B5%D1%81%D0%B5%D0%BB%D0%BE-4/" target="_self">Русский</a><em>, </em><a href="https://medium.com/@jongdae.lim/기계-학습-machine-learning-은-즐겁다-part-4-63ed781eee3c" target="_self">한국어</a><em>, </em><a href="https://medium.com/machina-sapiens/aprendizagem-de-máquina-é-divertido-parte-4-reconhecimento-facial-moderno-com-deep-learning-72525d9684c2" target="_self">Português</a><em>, </em><a href="https://viblo.asia/p/machine-learning-that-thu-vi-4-tu-dong-tag-ten-ban-be-ORNZqPDqK0n" target="_self">Tiếng Việt</a><em>, </em><a href="https://shahaab-co.ir/mag/edu/ml/machine-learning-is-fun-part-4" target="_self">فارسی</a><em> or </em><a href="https://medium.com/botsupply/il-machine-learning-è-divertente-parte-4-c707feee1cf8" target="_self">Italiano</a><em>.</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Giant update:</strong><em> </em><a href="https://www.machinelearningisfun.com/get-the-book/" target="_self">I’ve written a new book based on these articles</a><em>! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects. </em><a href="https://www.machinelearningisfun.com/get-the-book/" target="_self">Check it out now</a><em>!</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Have you noticed that Facebook has developed an uncanny ability to recognize your friends in your photographs? In the old days, Facebook used to make you to tag your friends in photos by clicking on them and typing in their name. Now as soon as you upload a photo, Facebook tags everyone for you <em>like magic</em>:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:67%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 502px" srcset="https://miro.medium.com/v2/resize:fit:640/1*WFi-RK1HU2YrDW3BQXYofQ.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*WFi-RK1HU2YrDW3BQXYofQ.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*WFi-RK1HU2YrDW3BQXYofQ.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*WFi-RK1HU2YrDW3BQXYofQ.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*WFi-RK1HU2YrDW3BQXYofQ.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*WFi-RK1HU2YrDW3BQXYofQ.gif 1100w, https://miro.medium.com/v2/resize:fit:1004/1*WFi-RK1HU2YrDW3BQXYofQ.gif 1004w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 502px" srcset="https://miro.medium.com/v2/resize:fit:640/1*WFi-RK1HU2YrDW3BQXYofQ.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*WFi-RK1HU2YrDW3BQXYofQ.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*WFi-RK1HU2YrDW3BQXYofQ.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*WFi-RK1HU2YrDW3BQXYofQ.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*WFi-RK1HU2YrDW3BQXYofQ.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*WFi-RK1HU2YrDW3BQXYofQ.gif 1100w, https://miro.medium.com/v2/resize:fit:1004/1*WFi-RK1HU2YrDW3BQXYofQ.gif 1004w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/502/1*WFi-RK1HU2YrDW3BQXYofQ.gif"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Facebook automatically tags people in your photos that you have tagged before. I’m not sure if this is helpful or creepy!</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This technology is called face recognition. Facebook’s algorithms are able to recognize your friends’ faces after they have been tagged only a few times. It’s pretty amazing technology — Facebook can recognize faces with <a href="https://research.facebook.com/publications/deepface-closing-the-gap-to-human-level-performance-in-face-verification/" target="_self">98% accuracy</a> which is pretty much as good as humans can do!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Let’s learn how modern face recognition works! But just recognizing your friends would be too easy. We can push this tech to the limit to solve a more challenging problem — telling <a href="https://en.wikipedia.org/wiki/Will_Ferrell" target="_self">Will Ferrell</a> (famous actor) apart from <a href="https://en.wikipedia.org/wiki/Chad_Smith" target="_self">Chad Smith</a> (famous rock musician)!</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*l7zNW_4-afEOfP_mXxs75w.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*l7zNW_4-afEOfP_mXxs75w.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*l7zNW_4-afEOfP_mXxs75w.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*l7zNW_4-afEOfP_mXxs75w.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*l7zNW_4-afEOfP_mXxs75w.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*l7zNW_4-afEOfP_mXxs75w.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l7zNW_4-afEOfP_mXxs75w.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*l7zNW_4-afEOfP_mXxs75w.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*l7zNW_4-afEOfP_mXxs75w.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*l7zNW_4-afEOfP_mXxs75w.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*l7zNW_4-afEOfP_mXxs75w.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*l7zNW_4-afEOfP_mXxs75w.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*l7zNW_4-afEOfP_mXxs75w.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*l7zNW_4-afEOfP_mXxs75w.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*l7zNW_4-afEOfP_mXxs75w.jpeg"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">One of these people is Will Farrell. The other is Chad Smith. I swear they are different people!</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">How to use Machine Learning on a Very Complicated Problem</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">So far in <a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471" target="_self">Part 1</a>, <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3" target="_self">2</a> and <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721" target="_self">3</a>, we’ve used machine learning to solve isolated problems that have only one step — <a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471" target="_self">estimating the price of a house</a>, <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3" target="_self">generating new data based on existing data</a> and <a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721" target="_self">telling if an image contains a certain object</a>. All of those problems can be solved by choosing one machine learning algorithm, feeding in data, and getting the result.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">But face recognition is really a series of several related problems:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;">First, look at a picture and find all the faces in it</li><li class="ff3" style="font-size:22px;">Second, focus on each face and be able to understand that even if a face is turned in a weird direction or in bad lighting, it is still the same person.</li><li class="ff3" style="font-size:22px;">Third, be able to pick out unique features of the face that you can use to tell it apart from other people— like how big the eyes are, how long the face is, etc.</li><li class="ff3" style="font-size:22px;">Finally, compare the unique features of that face to all the people you already know to determine the person’s name.</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">As a human, your brain is wired to do all of this automatically and instantly. In fact, humans are <em>too good</em> at recognizing faces and end up seeing faces in everyday objects:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:78%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 580px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*HcwWLND2du14e-rzJ50tHA.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*HcwWLND2du14e-rzJ50tHA.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*HcwWLND2du14e-rzJ50tHA.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*HcwWLND2du14e-rzJ50tHA.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*HcwWLND2du14e-rzJ50tHA.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*HcwWLND2du14e-rzJ50tHA.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1160/format:webp/1*HcwWLND2du14e-rzJ50tHA.jpeg 1160w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 580px" srcset="https://miro.medium.com/v2/resize:fit:640/1*HcwWLND2du14e-rzJ50tHA.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*HcwWLND2du14e-rzJ50tHA.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*HcwWLND2du14e-rzJ50tHA.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*HcwWLND2du14e-rzJ50tHA.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*HcwWLND2du14e-rzJ50tHA.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*HcwWLND2du14e-rzJ50tHA.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1160/1*HcwWLND2du14e-rzJ50tHA.jpeg 1160w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/580/1*HcwWLND2du14e-rzJ50tHA.jpeg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Computers are not capable of this kind of high-level generalization (<em>at least not yet…</em>), so we have to teach them how to do each step in this process separately.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We need to build a <em>pipeline</em> where we solve each step of face recognition separately and pass the result of the current step to the next step. In other words, we will chain together several machine learning algorithms:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*WxBM1lB5WzDjrDXYfi9gtw.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*WxBM1lB5WzDjrDXYfi9gtw.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*WxBM1lB5WzDjrDXYfi9gtw.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*WxBM1lB5WzDjrDXYfi9gtw.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*WxBM1lB5WzDjrDXYfi9gtw.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*WxBM1lB5WzDjrDXYfi9gtw.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*WxBM1lB5WzDjrDXYfi9gtw.gif 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*WxBM1lB5WzDjrDXYfi9gtw.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*WxBM1lB5WzDjrDXYfi9gtw.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*WxBM1lB5WzDjrDXYfi9gtw.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*WxBM1lB5WzDjrDXYfi9gtw.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*WxBM1lB5WzDjrDXYfi9gtw.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*WxBM1lB5WzDjrDXYfi9gtw.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*WxBM1lB5WzDjrDXYfi9gtw.gif 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*WxBM1lB5WzDjrDXYfi9gtw.gif"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">How a basic pipeline for detecting faces might work</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Face Recognition — Step by Step</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Let’s tackle this problem one step at a time. For each step, we’ll learn about a different machine learning algorithm. I’m not going to explain every single algorithm completely to keep this from turning into a book, but you’ll learn the main ideas behind each one and you’ll learn how you can build your own facial recognition system in Python using <a href="https://cmusatyalab.github.io/openface/" target="_self">OpenFace</a> and <a href="http://dlib.net/" target="_self">dlib</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Step 1: Finding all the Faces</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The first step in our pipeline is <em>face detection</em>. Obviously we need to locate the faces in a photograph before we can try to tell them apart!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you’ve used any camera in the last 10 years, you’ve probably seen face detection in action:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*izQuwClzcsJoCw5ybQC01Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*izQuwClzcsJoCw5ybQC01Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*izQuwClzcsJoCw5ybQC01Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*izQuwClzcsJoCw5ybQC01Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*izQuwClzcsJoCw5ybQC01Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*izQuwClzcsJoCw5ybQC01Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*izQuwClzcsJoCw5ybQC01Q.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*izQuwClzcsJoCw5ybQC01Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*izQuwClzcsJoCw5ybQC01Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*izQuwClzcsJoCw5ybQC01Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*izQuwClzcsJoCw5ybQC01Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*izQuwClzcsJoCw5ybQC01Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*izQuwClzcsJoCw5ybQC01Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*izQuwClzcsJoCw5ybQC01Q.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*izQuwClzcsJoCw5ybQC01Q.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Face detection is a great feature for cameras. When the camera can automatically pick out faces, it can make sure that all the faces are in focus before it takes the picture. But we’ll use it for a different purpose — finding the areas of the image we want to pass on to the next step in our pipeline.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Face detection went mainstream in the early 2000's when Paul Viola and Michael Jones invented a <a href="https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework" target="_self">way to detect faces</a> that was fast enough to run on cheap cameras. However, much more reliable solutions exist now. We’re going to use <a href="http://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf" target="_self">a method invented in 2005</a> called Histogram of Oriented Gradients — or just <strong>HOG</strong> for short.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To find faces in an image, we’ll start by making our image black and white because we don’t need color data to find faces:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:69%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 511px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*osGdB2BNMThhk1rTwo07JA.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*osGdB2BNMThhk1rTwo07JA.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*osGdB2BNMThhk1rTwo07JA.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*osGdB2BNMThhk1rTwo07JA.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*osGdB2BNMThhk1rTwo07JA.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*osGdB2BNMThhk1rTwo07JA.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1022/format:webp/1*osGdB2BNMThhk1rTwo07JA.jpeg 1022w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 511px" srcset="https://miro.medium.com/v2/resize:fit:640/1*osGdB2BNMThhk1rTwo07JA.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*osGdB2BNMThhk1rTwo07JA.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*osGdB2BNMThhk1rTwo07JA.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*osGdB2BNMThhk1rTwo07JA.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*osGdB2BNMThhk1rTwo07JA.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*osGdB2BNMThhk1rTwo07JA.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1022/1*osGdB2BNMThhk1rTwo07JA.jpeg 1022w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/511/1*osGdB2BNMThhk1rTwo07JA.jpeg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Then we’ll look at every single pixel in our image one at a time. For every single pixel, we want to look at the pixels that directly surrounding it:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*RZS05e_5XXQdofdRx1GvPA.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*RZS05e_5XXQdofdRx1GvPA.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*RZS05e_5XXQdofdRx1GvPA.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*RZS05e_5XXQdofdRx1GvPA.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*RZS05e_5XXQdofdRx1GvPA.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*RZS05e_5XXQdofdRx1GvPA.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*RZS05e_5XXQdofdRx1GvPA.gif 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*RZS05e_5XXQdofdRx1GvPA.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*RZS05e_5XXQdofdRx1GvPA.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*RZS05e_5XXQdofdRx1GvPA.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*RZS05e_5XXQdofdRx1GvPA.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*RZS05e_5XXQdofdRx1GvPA.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*RZS05e_5XXQdofdRx1GvPA.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*RZS05e_5XXQdofdRx1GvPA.gif 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*RZS05e_5XXQdofdRx1GvPA.gif"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Our goal is to figure out how dark the current pixel is compared to the pixels directly surrounding it. Then we want to draw an arrow showing in which direction the image is getting darker:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:67%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 500px" srcset="https://miro.medium.com/v2/resize:fit:640/1*WF54tQnH1Hgpoqk-Vtf9Lg.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*WF54tQnH1Hgpoqk-Vtf9Lg.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*WF54tQnH1Hgpoqk-Vtf9Lg.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*WF54tQnH1Hgpoqk-Vtf9Lg.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*WF54tQnH1Hgpoqk-Vtf9Lg.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*WF54tQnH1Hgpoqk-Vtf9Lg.gif 1100w, https://miro.medium.com/v2/resize:fit:1000/1*WF54tQnH1Hgpoqk-Vtf9Lg.gif 1000w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 500px" srcset="https://miro.medium.com/v2/resize:fit:640/1*WF54tQnH1Hgpoqk-Vtf9Lg.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*WF54tQnH1Hgpoqk-Vtf9Lg.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*WF54tQnH1Hgpoqk-Vtf9Lg.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*WF54tQnH1Hgpoqk-Vtf9Lg.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*WF54tQnH1Hgpoqk-Vtf9Lg.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*WF54tQnH1Hgpoqk-Vtf9Lg.gif 1100w, https://miro.medium.com/v2/resize:fit:1000/1*WF54tQnH1Hgpoqk-Vtf9Lg.gif 1000w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/500/1*WF54tQnH1Hgpoqk-Vtf9Lg.gif"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Looking at just this one pixel and the pixels touching it, the image is getting darker towards the upper right.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you repeat that process for <strong>every single pixel</strong> in the image, you end up with every pixel being replaced by an arrow. These arrows are called <em>gradients</em> and they show the flow from light to dark across the entire image:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*oTdaElx_M-_z9c_iAwwqcw.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*oTdaElx_M-_z9c_iAwwqcw.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*oTdaElx_M-_z9c_iAwwqcw.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*oTdaElx_M-_z9c_iAwwqcw.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*oTdaElx_M-_z9c_iAwwqcw.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*oTdaElx_M-_z9c_iAwwqcw.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*oTdaElx_M-_z9c_iAwwqcw.gif 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*oTdaElx_M-_z9c_iAwwqcw.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*oTdaElx_M-_z9c_iAwwqcw.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*oTdaElx_M-_z9c_iAwwqcw.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*oTdaElx_M-_z9c_iAwwqcw.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*oTdaElx_M-_z9c_iAwwqcw.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*oTdaElx_M-_z9c_iAwwqcw.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*oTdaElx_M-_z9c_iAwwqcw.gif 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*oTdaElx_M-_z9c_iAwwqcw.gif"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This might seem like a random thing to do, but there’s a really good reason for replacing the pixels with gradients. If we analyze pixels directly, really dark images and really light images of the same person will have totally different pixel values. But by only considering the <em>direction</em> that brightness changes, both really dark images and really bright images will end up with the same exact representation. That makes the problem a lot easier to solve!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">But saving the gradient for every single pixel gives us way too much detail. We end up <a href="https://en.wiktionary.org/wiki/see_the_forest_for_the_trees" target="_self">missing the forest for the trees</a>. It would be better if we could just see the basic flow of lightness/darkness at a higher level so we could see the basic pattern of the image.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To do this, we’ll break up the image into small squares of 16x16 pixels each. In each square, we’ll count up how many gradients point in each major direction (how many point up, point up-right, point right, etc…). Then we’ll replace that square in the image with the arrow directions that were the strongest.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The end result is we turn the original image into a very simple representation that captures the basic structure of a face in a simple way:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*uHisafuUw0FOsoZA992Jdg.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*uHisafuUw0FOsoZA992Jdg.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*uHisafuUw0FOsoZA992Jdg.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*uHisafuUw0FOsoZA992Jdg.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*uHisafuUw0FOsoZA992Jdg.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*uHisafuUw0FOsoZA992Jdg.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*uHisafuUw0FOsoZA992Jdg.gif 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*uHisafuUw0FOsoZA992Jdg.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*uHisafuUw0FOsoZA992Jdg.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*uHisafuUw0FOsoZA992Jdg.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*uHisafuUw0FOsoZA992Jdg.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*uHisafuUw0FOsoZA992Jdg.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*uHisafuUw0FOsoZA992Jdg.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*uHisafuUw0FOsoZA992Jdg.gif 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*uHisafuUw0FOsoZA992Jdg.gif"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">The original image is turned into a HOG representation that captures the major features of the image regardless of image brightnesss.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To find faces in this HOG image, all we have to do is find the part of our image that looks the most similar to a known HOG pattern that was extracted from a bunch of other training faces:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*6xgev0r-qn4oR88FrW6fiA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*6xgev0r-qn4oR88FrW6fiA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*6xgev0r-qn4oR88FrW6fiA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*6xgev0r-qn4oR88FrW6fiA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*6xgev0r-qn4oR88FrW6fiA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*6xgev0r-qn4oR88FrW6fiA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6xgev0r-qn4oR88FrW6fiA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*6xgev0r-qn4oR88FrW6fiA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*6xgev0r-qn4oR88FrW6fiA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*6xgev0r-qn4oR88FrW6fiA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*6xgev0r-qn4oR88FrW6fiA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*6xgev0r-qn4oR88FrW6fiA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*6xgev0r-qn4oR88FrW6fiA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*6xgev0r-qn4oR88FrW6fiA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*6xgev0r-qn4oR88FrW6fiA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Using this technique, we can now easily find faces in any image:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*dOtP6yl7d4c0oaR6NpfWVg.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*dOtP6yl7d4c0oaR6NpfWVg.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*dOtP6yl7d4c0oaR6NpfWVg.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*dOtP6yl7d4c0oaR6NpfWVg.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*dOtP6yl7d4c0oaR6NpfWVg.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*dOtP6yl7d4c0oaR6NpfWVg.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dOtP6yl7d4c0oaR6NpfWVg.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*dOtP6yl7d4c0oaR6NpfWVg.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*dOtP6yl7d4c0oaR6NpfWVg.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*dOtP6yl7d4c0oaR6NpfWVg.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*dOtP6yl7d4c0oaR6NpfWVg.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*dOtP6yl7d4c0oaR6NpfWVg.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*dOtP6yl7d4c0oaR6NpfWVg.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*dOtP6yl7d4c0oaR6NpfWVg.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*dOtP6yl7d4c0oaR6NpfWVg.jpeg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you want to try this step out yourself using Python and dlib, <a href="https://gist.github.com/ageitgey/1c1cb1c60ace321868f7410d48c228e1" target="_self">here’s code</a> showing how to generate and view HOG representations of images.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Step 2: Posing and Projecting Faces</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Whew, we isolated the faces in our image. But now we have to deal with the problem that faces turned different directions look totally different to a computer:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*x-rg0aSpKOer1JF-TejYUg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*x-rg0aSpKOer1JF-TejYUg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*x-rg0aSpKOer1JF-TejYUg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*x-rg0aSpKOer1JF-TejYUg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*x-rg0aSpKOer1JF-TejYUg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*x-rg0aSpKOer1JF-TejYUg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x-rg0aSpKOer1JF-TejYUg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*x-rg0aSpKOer1JF-TejYUg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*x-rg0aSpKOer1JF-TejYUg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*x-rg0aSpKOer1JF-TejYUg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*x-rg0aSpKOer1JF-TejYUg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*x-rg0aSpKOer1JF-TejYUg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*x-rg0aSpKOer1JF-TejYUg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*x-rg0aSpKOer1JF-TejYUg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*x-rg0aSpKOer1JF-TejYUg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Humans can easily recognize that both images are of Will Ferrell, but computers would see these pictures as two completely different people.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To account for this, we will try to warp each picture so that the eyes and lips are always in the sample place in the image. This will make it a lot easier for us to compare faces in the next steps.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To do this, we are going to use an algorithm called <strong>face landmark estimation</strong>. There are lots of ways to do this, but we are going to use the approach <a href="http://www.csc.kth.se/~vahidk/papers/KazemiCVPR14.pdf" target="_self">invented in 2014 by Vahid Kazemi and Josephine Sullivan.</a></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The basic idea is we will come up with 68 specific points (called <em>landmarks</em>) that exist on every face — the top of the chin, the outside edge of each eye, the inner edge of each eyebrow, etc. Then we will train a machine learning algorithm to be able to find these 68 specific points on any face:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:55%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 414px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*AbEg31EgkbXSQehuNJBlWg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*AbEg31EgkbXSQehuNJBlWg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*AbEg31EgkbXSQehuNJBlWg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*AbEg31EgkbXSQehuNJBlWg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*AbEg31EgkbXSQehuNJBlWg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*AbEg31EgkbXSQehuNJBlWg.png 1100w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*AbEg31EgkbXSQehuNJBlWg.png 828w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 414px" srcset="https://miro.medium.com/v2/resize:fit:640/1*AbEg31EgkbXSQehuNJBlWg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*AbEg31EgkbXSQehuNJBlWg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*AbEg31EgkbXSQehuNJBlWg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*AbEg31EgkbXSQehuNJBlWg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*AbEg31EgkbXSQehuNJBlWg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*AbEg31EgkbXSQehuNJBlWg.png 1100w, https://miro.medium.com/v2/resize:fit:828/1*AbEg31EgkbXSQehuNJBlWg.png 828w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/414/1*AbEg31EgkbXSQehuNJBlWg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">The 68 landmarks we will locate on every face. This image was created by <a href="http://bamos.github.io/" target="_self">Brandon Amos</a> of CMU who works on <a href="https://github.com/cmusatyalab/openface" target="_self">OpenFace</a>.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here’s the result of locating the 68 face landmarks on our test image:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*xBJ4H2lbCMfzIfMrOm9BEQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*xBJ4H2lbCMfzIfMrOm9BEQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*xBJ4H2lbCMfzIfMrOm9BEQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*xBJ4H2lbCMfzIfMrOm9BEQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*xBJ4H2lbCMfzIfMrOm9BEQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*xBJ4H2lbCMfzIfMrOm9BEQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xBJ4H2lbCMfzIfMrOm9BEQ.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*xBJ4H2lbCMfzIfMrOm9BEQ.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*xBJ4H2lbCMfzIfMrOm9BEQ.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*xBJ4H2lbCMfzIfMrOm9BEQ.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*xBJ4H2lbCMfzIfMrOm9BEQ.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*xBJ4H2lbCMfzIfMrOm9BEQ.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*xBJ4H2lbCMfzIfMrOm9BEQ.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*xBJ4H2lbCMfzIfMrOm9BEQ.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*xBJ4H2lbCMfzIfMrOm9BEQ.jpeg"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;"><strong>PROTIP</strong>: You can also use this same technique to implement your own version of Snapchat’s real-time 3d face filters!</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Now that we know were the eyes and mouth are, we’ll simply rotate, scale and <a href="https://en.wikipedia.org/wiki/Shear_mapping#/media/File:VerticalShear_m%3D1.25.svg" target="_self">shear</a> the image so that the eyes and mouth are centered as best as possible. We won’t do any fancy 3d warps because that would introduce distortions into the image. We are only going to use basic image transformations like rotation and scale that preserve parallel lines (called <a href="https://en.wikipedia.org/wiki/Affine_transformation" target="_self">affine transformations</a>):</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*igEzGcFn-tjZb94j15tCNA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*igEzGcFn-tjZb94j15tCNA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*igEzGcFn-tjZb94j15tCNA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*igEzGcFn-tjZb94j15tCNA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*igEzGcFn-tjZb94j15tCNA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*igEzGcFn-tjZb94j15tCNA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*igEzGcFn-tjZb94j15tCNA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*igEzGcFn-tjZb94j15tCNA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*igEzGcFn-tjZb94j15tCNA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*igEzGcFn-tjZb94j15tCNA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*igEzGcFn-tjZb94j15tCNA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*igEzGcFn-tjZb94j15tCNA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*igEzGcFn-tjZb94j15tCNA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*igEzGcFn-tjZb94j15tCNA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*igEzGcFn-tjZb94j15tCNA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Now no matter how the face is turned, we are able to center the eyes and mouth are in roughly the same position in the image. This will make our next step a lot more accurate.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you want to try this step out yourself using Python and dlib, here’s the <a href="https://gist.github.com/ageitgey/ae340db3e493530d5e1f9c15292e5c74" target="_self">code for finding face landmarks</a> and here’s the <a href="https://gist.github.com/ageitgey/82d0ea0fdb56dc93cb9b716e7ceb364b" target="_self">code for transforming the image</a> using those landmarks.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Step 3: Encoding Faces</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Now we are to the meat of the problem — actually telling faces apart. This is where things get really interesting!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The simplest approach to face recognition is to directly compare the unknown face we found in Step 2 with all the pictures we have of people that have already been tagged. When we find a previously tagged face that looks very similar to our unknown face, it must be the same person. Seems like a pretty good idea, right?</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">There’s actually a huge problem with that approach. A site like Facebook with billions of users and a trillion photos can’t possibly loop through every previous-tagged face to compare it to every newly uploaded picture. That would take way too long. They need to be able to recognize faces in milliseconds, not hours.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">What we need is a way to extract a few basic measurements from each face. Then we could measure our unknown face the same way and find the known face with the closest measurements. For example, we might measure the size of each ear, the spacing between the eyes, the length of the nose, etc. If you’ve ever watched a bad crime show like <a href="https://en.wikipedia.org/wiki/CSI:_Crime_Scene_Investigation" target="_self">CSI</a>, you know what I am talking about:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*_GNyjR3JlPoS9grtIVmKFQ.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*_GNyjR3JlPoS9grtIVmKFQ.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*_GNyjR3JlPoS9grtIVmKFQ.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*_GNyjR3JlPoS9grtIVmKFQ.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*_GNyjR3JlPoS9grtIVmKFQ.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*_GNyjR3JlPoS9grtIVmKFQ.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*_GNyjR3JlPoS9grtIVmKFQ.gif 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*_GNyjR3JlPoS9grtIVmKFQ.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*_GNyjR3JlPoS9grtIVmKFQ.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*_GNyjR3JlPoS9grtIVmKFQ.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*_GNyjR3JlPoS9grtIVmKFQ.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*_GNyjR3JlPoS9grtIVmKFQ.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*_GNyjR3JlPoS9grtIVmKFQ.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*_GNyjR3JlPoS9grtIVmKFQ.gif 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*_GNyjR3JlPoS9grtIVmKFQ.gif"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Just like TV! So real! #science</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">The most reliable way to measure a face</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Ok, so which measurements should we collect from each face to build our known face database? Ear size? Nose length? Eye color? Something else?</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">It turns out that the measurements that seem obvious to us humans (like eye color) don’t really make sense to a computer looking at individual pixels in an image. Researchers have discovered that the most accurate approach is to let the computer figure out the measurements to collect itself. Deep learning does a better job than humans at figuring out which parts of a face are important to measure.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The solution is to train a Deep Convolutional Neural Network (<a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721" target="_self">just like we did in Part 3</a>). But instead of training the network to recognize pictures objects like we did last time, we are going to train it to generate 128 measurements for each face.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The training process works by looking at 3 face images at a time:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;">Load a training face image of a known person</li><li class="ff3" style="font-size:22px;">Load another picture of the same known person</li><li class="ff3" style="font-size:22px;">Load a picture of a totally different person</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><mark>Then the algorithm looks at the measurements it is currently generating for each of those three images. It then tweaks the neural network slightly so that it makes sure the measurements it generates for #1 and #2 are slightly closer while making sure the measurements for #2 and #3 are slightly further apart:</mark></p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*n1R8VMyDRw3RNO3JULYBpQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*n1R8VMyDRw3RNO3JULYBpQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*n1R8VMyDRw3RNO3JULYBpQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*n1R8VMyDRw3RNO3JULYBpQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*n1R8VMyDRw3RNO3JULYBpQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*n1R8VMyDRw3RNO3JULYBpQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n1R8VMyDRw3RNO3JULYBpQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*n1R8VMyDRw3RNO3JULYBpQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*n1R8VMyDRw3RNO3JULYBpQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*n1R8VMyDRw3RNO3JULYBpQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*n1R8VMyDRw3RNO3JULYBpQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*n1R8VMyDRw3RNO3JULYBpQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*n1R8VMyDRw3RNO3JULYBpQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*n1R8VMyDRw3RNO3JULYBpQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*n1R8VMyDRw3RNO3JULYBpQ.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After repeating this step millions of times for millions of images of thousands of different people, the neural network learns to reliably generate 128 measurements for each person. Any ten different pictures of the same person should give roughly the same measurements.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Machine learning people call the 128 measurements of each face an <strong>embedding</strong>. The idea of reducing complicated raw data like a picture into a list of computer-generated numbers comes up a lot in machine learning (especially in language translation). The exact approach for faces we are using <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/app/1A_089.pdf" target="_self">was invented in 2015 by researchers at Google</a> but many similar approaches exist.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Encoding our face image</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This process of training a convolutional neural network to output face embeddings requires a lot of data and computer power. Even with an expensive <a href="http://www.nvidia.com/object/tesla-supercomputing-solutions.html" target="_self">NVidia Telsa video card</a>, it takes <a href="https://twitter.com/brandondamos/status/757959518433243136" target="_self">about 24 hours</a> of continuous training to get good accuracy.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">But once the network has been trained, it can generate measurements for any face, even ones it has never seen before! So this step only needs to be done once. Lucky for us, the fine folks at <a href="https://cmusatyalab.github.io/openface/" target="_self">OpenFace</a> already did this and they <a href="https://github.com/cmusatyalab/openface/tree/master/models/openface" target="_self">published several trained networks</a> which we can directly use. Thanks <a href="http://bamos.github.io/" target="_self">Brandon Amos</a> and team!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">So all we need to do ourselves is run our face images through their pre-trained network to get the 128 measurements for each face. Here’s the measurements for our test image:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*6kMMqLt4UBCrN7HtqNHMKw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*6kMMqLt4UBCrN7HtqNHMKw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*6kMMqLt4UBCrN7HtqNHMKw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*6kMMqLt4UBCrN7HtqNHMKw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*6kMMqLt4UBCrN7HtqNHMKw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*6kMMqLt4UBCrN7HtqNHMKw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6kMMqLt4UBCrN7HtqNHMKw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*6kMMqLt4UBCrN7HtqNHMKw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*6kMMqLt4UBCrN7HtqNHMKw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*6kMMqLt4UBCrN7HtqNHMKw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*6kMMqLt4UBCrN7HtqNHMKw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*6kMMqLt4UBCrN7HtqNHMKw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*6kMMqLt4UBCrN7HtqNHMKw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*6kMMqLt4UBCrN7HtqNHMKw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*6kMMqLt4UBCrN7HtqNHMKw.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">So what parts of the face are these 128 numbers measuring exactly? It turns out that we have no idea. It doesn’t really matter to us. All that we care is that the network generates nearly the same numbers when looking at two different pictures of the same person.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you want to try this step yourself, OpenFace <a href="https://github.com/cmusatyalab/openface/blob/master/batch-represent/batch-represent.lua" target="_self">provides a lua script</a> that will generate embeddings all images in a folder and write them to a csv file. You <a href="https://gist.github.com/ageitgey/ddbae3b209b6344a458fa41a3cf75719" target="_self">run it like this</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Step 4: Finding the person’s name from the encoding</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This last step is actually the easiest step in the whole process. All we have to do is find the person in our database of known people who has the closest measurements to our test image.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">You can do that by using any basic machine learning classification algorithm. No fancy deep learning tricks are needed. We’ll use a simple linear <a href="https://en.wikipedia.org/wiki/Support_vector_machine" target="_self">SVM classifier</a>, but lots of classification algorithms could work.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">All we need to do is train a classifier that can take in the measurements from a new test image and tells which known person is the closest match. Running this classifier takes milliseconds. The result of the classifier is the name of the person!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">So let’s try out our system. First, I trained a classifier with the embeddings of about 20 pictures each of Will Ferrell, Chad Smith and Jimmy Falon:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*G6jxtXUxDYGY_orEPNzG9Q.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*G6jxtXUxDYGY_orEPNzG9Q.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*G6jxtXUxDYGY_orEPNzG9Q.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*G6jxtXUxDYGY_orEPNzG9Q.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*G6jxtXUxDYGY_orEPNzG9Q.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*G6jxtXUxDYGY_orEPNzG9Q.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G6jxtXUxDYGY_orEPNzG9Q.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*G6jxtXUxDYGY_orEPNzG9Q.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*G6jxtXUxDYGY_orEPNzG9Q.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*G6jxtXUxDYGY_orEPNzG9Q.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*G6jxtXUxDYGY_orEPNzG9Q.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*G6jxtXUxDYGY_orEPNzG9Q.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*G6jxtXUxDYGY_orEPNzG9Q.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*G6jxtXUxDYGY_orEPNzG9Q.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*G6jxtXUxDYGY_orEPNzG9Q.jpeg"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Sweet, sweet training data!</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Then I ran the classifier on every frame of the famous youtube video of <a href="https://www.youtube.com/watch?v=EsWHyBOk2iQ" target="_self">Will Ferrell and Chad Smith pretending to be each other</a> on the Jimmy Fallon show:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*woPojJbd6lT7CFZ9lHRVDw.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*woPojJbd6lT7CFZ9lHRVDw.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*woPojJbd6lT7CFZ9lHRVDw.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*woPojJbd6lT7CFZ9lHRVDw.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*woPojJbd6lT7CFZ9lHRVDw.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*woPojJbd6lT7CFZ9lHRVDw.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*woPojJbd6lT7CFZ9lHRVDw.gif 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*woPojJbd6lT7CFZ9lHRVDw.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*woPojJbd6lT7CFZ9lHRVDw.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*woPojJbd6lT7CFZ9lHRVDw.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*woPojJbd6lT7CFZ9lHRVDw.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*woPojJbd6lT7CFZ9lHRVDw.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*woPojJbd6lT7CFZ9lHRVDw.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*woPojJbd6lT7CFZ9lHRVDw.gif 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*woPojJbd6lT7CFZ9lHRVDw.gif"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">It works! And look how well it works for faces in different poses — even sideways faces!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Running this Yourself</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Let’s review the steps we followed:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;">Encode a picture using the HOG algorithm to create a simplified version of the image. Using this simplified image, find the part of the image that most looks like a generic HOG encoding of a face.</li><li class="ff3" style="font-size:22px;">Figure out the pose of the face by finding the main landmarks in the face. Once we find those landmarks, use them to warp the image so that the eyes and mouth are centered.</li><li class="ff3" style="font-size:22px;">Pass the centered face image through a neural network that knows how to measure features of the face. Save those 128 measurements.</li><li class="ff3" style="font-size:22px;">Looking at all the faces we’ve measured in the past, see which person has the closest measurements to our face’s measurements. That’s our match!</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Now that you know how this all works, here’s instructions from start-to-finish of how run this entire face recognition pipeline on your own computer:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>UPDATE 4/9/2017: </strong>You can still follow the steps below to use OpenFace. However, I’ve released a new Python-based face recognition library called <a href="https://github.com/ageitgey/face_recognition#face-recognition" target="_self">face_recognition</a> that is much easier to install and use. So I’d recommend trying out <a href="https://github.com/ageitgey/face_recognition#face-recognition" target="_self">face_recognition</a> first instead of continuing below!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I even put together <a href="https://medium.com/@ageitgey/try-deep-learning-in-python-now-with-a-fully-pre-configured-vm-1d97d4c3e9b" target="_self">a pre-configured virtual machine with face_recognition, OpenCV, TensorFlow and lots of other deep learning tools pre-installed</a>. You can download and run it on your computer very easily. Give the virtual machine a shot if you don’t want to install all these libraries yourself!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>Original OpenFace instructions:</em></p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre>## Before you start

Make sure you have python, OpenFace and dlib installed. You can either [install them manually](https://cmusatyalab.github.io/openface/setup/) or use a preconfigured docker image that has everying already installed: 

```bash
docker pull bamos/openface
docker run -p 9000:9000 -p 8000:8000 -t -i bamos/openface /bin/bash
cd /root/openface
```

Pro-tip: If you are using Docker on OSX, you can make your OSX /Users/ folder visible inside a docker image like this:

```
docker run -v /Users:/host/Users -p 9000:9000 -p 8000:8000 -t -i bamos/openface /bin/bash
cd /root/openface
```

Then you can access all your OSX files inside of the docker image at `/host/Users/...`

```
ls /host/Users/
```

## Step 1

Make a folder called `./training-images/` inside the openface folder.

```
mkdir training-images
```

## Step 2 

Make a subfolder for each person you want to recognize. For example:

```
mkdir ./training-images/will-ferrell/
mkdir ./training-images/chad-smith/
mkdir ./training-images/jimmy-fallon/
```

## Step 3 

Copy all your images of each person into the correct sub-folders. Make sure only one face appears in each image. There's no need to crop the image around the face. OpenFace will do that automatically.

## Step 4

Run the openface scripts from inside the openface root directory:

First, do pose detection and alignment:

`./util/align-dlib.py ./training-images/ align outerEyesAndNose ./aligned-images/ --size 96`

This will create a new `./aligned-images/` subfolder with a cropped and aligned version of each of your test images.

Second, generate the representations from the aligned images:

`./batch-represent/main.lua -outDir ./generated-embeddings/ -data ./aligned-images/`

After you run this, the `./generated-embeddings/` sub-folder will contain a csv file with the embeddings for each image.

Third, train your face detection model:

`./demos/classifier.py train ./generated-embeddings/`

This will generate a new file called `./generated-embeddings/classifier.pkl`. This file has the SVM model you'll use to recognize
new faces.

At this point, you should have a working face recognizer!

## Step 5: Recognize faces!

Get a new picture with an unknown face. Pass it to the classifier script like this:

`./demos/classifier.py infer ./generated-embeddings/classifier.pkl your_test_image.jpg`

You should get a prediction that looks like this:

```
=== /test-images/will-ferrel-1.jpg ===
Predict will-ferrell with 0.73 confidence.
```

From here it's up to you to adapt the `./demos/classifier.py` python script to work however you want.

Important notes:
* If you get bad results, try adding a few more pictures of each person in Step 3 (especially picures in different poses).
* This script will _always_ make a prediction even if the face isn't one it knows. In a real application, you would look at the confidence score and throw away predictions with a low confidence since they are most likely wrong.</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you liked this article, please consider signing up for my Machine Learning is Fun! newsletter:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><a href="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fupscri.be%2F947d84%3Fas_embed%3Dtrue&url=https%3A%2F%2Fupscri.be%2F947d84%2F&image=https%3A%2F%2Fupscri.be%2Fmedia%2Fform.jpg&key=d04bfffea46d4aeda930ec88cc64b87c&type=text%2Fhtml&schema=upscri">Click Here!</a></p></div></div></div></section><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">You can also follow me on Twitter at <a href="https://twitter.com/ageitgey" target="_self">@ageitgey</a>, <a href="mailto:ageitgey@gmail.com" target="_self">email me directly</a> or <a href="https://www.linkedin.com/in/ageitgey" target="_self">find me on linkedin</a>. I’d love to hear from you if I can help you or your team with machine learning.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>Now continue on to </em><a href="https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa" target="_self">Machine Learning is Fun Part 5</a><em>!</em></p></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>