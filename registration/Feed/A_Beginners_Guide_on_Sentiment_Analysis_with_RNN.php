<!DOCTYPE html>
                <html>
                <head>
                    <title>A Beginner’s Guide on Sentiment Analysis with RNN</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://towardsdatascience.com/a-beginners-guide-on-sentiment-analysis-with-rnn-9e100627c02e"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://actsusanli.medium.com/?source=post_page-----9e100627c02e--------------------------------">Author : Susan Li</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>A Beginner’s Guide on Sentiment Analysis with RNN</h3></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*TPRgQHHw_0Xg97_XKJy48A.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*TPRgQHHw_0Xg97_XKJy48A.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*TPRgQHHw_0Xg97_XKJy48A.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*TPRgQHHw_0Xg97_XKJy48A.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*TPRgQHHw_0Xg97_XKJy48A.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*TPRgQHHw_0Xg97_XKJy48A.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TPRgQHHw_0Xg97_XKJy48A.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*TPRgQHHw_0Xg97_XKJy48A.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*TPRgQHHw_0Xg97_XKJy48A.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*TPRgQHHw_0Xg97_XKJy48A.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*TPRgQHHw_0Xg97_XKJy48A.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*TPRgQHHw_0Xg97_XKJy48A.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*TPRgQHHw_0Xg97_XKJy48A.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*TPRgQHHw_0Xg97_XKJy48A.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*TPRgQHHw_0Xg97_XKJy48A.jpeg"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Photo Credit: Unsplash</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><a href="https://en.wikipedia.org/wiki/Sentiment_analysis" target="_self">Sentiment analysis</a> probably is one the most common applications in <a href="https://en.wikipedia.org/wiki/Natural-language_processing" target="_self">Natural Language processing</a>. I don’t have to emphasize how important customer service tool sentiment analysis has become. So here we are, we will train a classifier movie reviews in <a href="http://ai.stanford.edu/~amaas/data/sentiment/" target="_self">IMDB data set</a>, using <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network" target="_self">Recurrent Neural Networks</a>. If you want to dive deeper on deep learning for sentiment analysis, this is a good <a href="https://arxiv.org/ftp/arxiv/papers/1801/1801.07883.pdf" target="_self">paper</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">The data</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We will use Recurrent Neural Networks, and in particular <a href="https://en.wikipedia.org/wiki/Long_short-term_memory" target="_self">LSTMs</a>, to perform sentiment analysis in <a href="https://keras.io/" target="_self">Keras</a>. Conveniently, Keras has a built-in IMDb movie reviews data set that we can use.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>from keras.datasets import imdb</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Set the vocabulary size and load in training and test data.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>vocabulary_size = 5000</span><span>(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)<br/>print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Loaded dataset with 25000 training samples, 25000 test samples</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Inspect a sample review and its label.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>print('---review---')<br/>print(X_train[6])<br/>print('---label---')<br/>print(y_train[6])</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*CiWuVTBsDfG6Wo9siICG0Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*CiWuVTBsDfG6Wo9siICG0Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*CiWuVTBsDfG6Wo9siICG0Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*CiWuVTBsDfG6Wo9siICG0Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*CiWuVTBsDfG6Wo9siICG0Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*CiWuVTBsDfG6Wo9siICG0Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CiWuVTBsDfG6Wo9siICG0Q.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*CiWuVTBsDfG6Wo9siICG0Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*CiWuVTBsDfG6Wo9siICG0Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*CiWuVTBsDfG6Wo9siICG0Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*CiWuVTBsDfG6Wo9siICG0Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*CiWuVTBsDfG6Wo9siICG0Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*CiWuVTBsDfG6Wo9siICG0Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*CiWuVTBsDfG6Wo9siICG0Q.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*CiWuVTBsDfG6Wo9siICG0Q.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Figure 1</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Note that the review is stored as a sequence of integers. These are word IDs that have been pre-assigned to individual words, and the label is an integer (0 for negative, 1 for positive).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We can use the dictionary returned by <code>imdb.get_word_index()</code> to map the review back to the original words.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>word2id = imdb.get_word_index()<br/>id2word = {i: word for word, i in word2id.items()}<br/>print('---review with words---')<br/>print([id2word.get(i, ' ') for i in X_train[6]])<br/>print('---label---')<br/>print(y_train[6])</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*3aFMiuQp2xUpYHxKu8qxXQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*3aFMiuQp2xUpYHxKu8qxXQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*3aFMiuQp2xUpYHxKu8qxXQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*3aFMiuQp2xUpYHxKu8qxXQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*3aFMiuQp2xUpYHxKu8qxXQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*3aFMiuQp2xUpYHxKu8qxXQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3aFMiuQp2xUpYHxKu8qxXQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*3aFMiuQp2xUpYHxKu8qxXQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*3aFMiuQp2xUpYHxKu8qxXQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*3aFMiuQp2xUpYHxKu8qxXQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*3aFMiuQp2xUpYHxKu8qxXQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*3aFMiuQp2xUpYHxKu8qxXQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*3aFMiuQp2xUpYHxKu8qxXQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*3aFMiuQp2xUpYHxKu8qxXQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*3aFMiuQp2xUpYHxKu8qxXQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Figure 2</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Maximum review length and minimum review length.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>print('Maximum review length: {}'.format(<br/>len(max((X_train + X_test), key=len))))</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Maximum review length: 2697</strong></p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>print('Minimum review length: {}'.format(<br/>len(min((X_test + X_test), key=len))))</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Minimum review length: 14</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5"><strong>Pad sequences</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In order to feed this data into our RNN, all input documents must have the same length. We will limit the maximum review length to max_words by truncating longer reviews and padding shorter reviews with a null value (0). We can accomplish this using the pad_sequences() function in Keras. For now, set max_words to 500.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>from keras.preprocessing import sequence</span><span>max_words = 500<br/>X_train = sequence.pad_sequences(X_train, maxlen=max_words)<br/>X_test = sequence.pad_sequences(X_test, maxlen=max_words)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5"><strong>Design an RNN model for sentiment analysis</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We start building our model architecture in the code cell below. We have imported some layers from Keras that you might need but feel free to use any other layers / transformations you like.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Remember that our input is a sequence of words (technically, integer word IDs) of maximum length = max_words, and our output is a binary sentiment label (0 or 1).</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>from keras import Sequential<br/>from keras.layers import Embedding, LSTM, Dense, Dropout</span><span>embedding_size=32<br/>model=Sequential()<br/>model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))<br/>model.add(LSTM(100))<br/>model.add(Dense(1, activation='sigmoid'))</span><span>print(model.summary())</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*1ZYMVNcq1ckk3ptfYzAW8A.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*1ZYMVNcq1ckk3ptfYzAW8A.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*1ZYMVNcq1ckk3ptfYzAW8A.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*1ZYMVNcq1ckk3ptfYzAW8A.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*1ZYMVNcq1ckk3ptfYzAW8A.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*1ZYMVNcq1ckk3ptfYzAW8A.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1ZYMVNcq1ckk3ptfYzAW8A.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*1ZYMVNcq1ckk3ptfYzAW8A.png 640w, https://miro.medium.com/v2/resize:fit:720/1*1ZYMVNcq1ckk3ptfYzAW8A.png 720w, https://miro.medium.com/v2/resize:fit:750/1*1ZYMVNcq1ckk3ptfYzAW8A.png 750w, https://miro.medium.com/v2/resize:fit:786/1*1ZYMVNcq1ckk3ptfYzAW8A.png 786w, https://miro.medium.com/v2/resize:fit:828/1*1ZYMVNcq1ckk3ptfYzAW8A.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*1ZYMVNcq1ckk3ptfYzAW8A.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*1ZYMVNcq1ckk3ptfYzAW8A.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*1ZYMVNcq1ckk3ptfYzAW8A.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Figure 3</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To summarize, our model is a simple RNN model with 1 embedding, 1 LSTM and 1 dense layers. 213,301 parameters in total need to be trained.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5"><strong>Train and evaluate our model</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We first need to compile our model by specifying the loss function and optimizer we want to use while training, as well as any evaluation metrics we’d like to measure. Specify the appropriate parameters, including at least one metric ‘accuracy’.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span><mark>model.compile(loss='binary_crossentropy', <br/>             optimizer='adam', <br/>             metrics=['accuracy'])</mark></span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Once compiled, we can kick off the training process. There are two important training parameters that we have to specify — batch size and number of training epochs, which together with our model architecture determine the total training time.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Training may take a while, so grab a cup of coffee, or better, go for a run!</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>batch_size = 64<br/>num_epochs = 3</span><span>X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]<br/>X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]</span><span>model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*KAxtWSwhtp_nVeoUY4MKFA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*KAxtWSwhtp_nVeoUY4MKFA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*KAxtWSwhtp_nVeoUY4MKFA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*KAxtWSwhtp_nVeoUY4MKFA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*KAxtWSwhtp_nVeoUY4MKFA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*KAxtWSwhtp_nVeoUY4MKFA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KAxtWSwhtp_nVeoUY4MKFA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*KAxtWSwhtp_nVeoUY4MKFA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*KAxtWSwhtp_nVeoUY4MKFA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*KAxtWSwhtp_nVeoUY4MKFA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*KAxtWSwhtp_nVeoUY4MKFA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*KAxtWSwhtp_nVeoUY4MKFA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*KAxtWSwhtp_nVeoUY4MKFA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*KAxtWSwhtp_nVeoUY4MKFA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*KAxtWSwhtp_nVeoUY4MKFA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Figure 4</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Once we have trained our model, it’s time to see how well it performs on unseen test data.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">scores[1] will correspond to accuracy if we pass metrics=[‘accuracy’]</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>scores = model.evaluate(X_test, y_test, verbose=0)<br/>print('Test accuracy:', scores[1])</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Test accuracy: 0.86964</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Summary</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">There are several ways in which we can build our model. We can continue trying and improving the accuracy of our model by experimenting with different architectures, layers and parameters. How good can we get without taking prohibitively long to train? How do we prevent overfitting?</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The source code can be found at <a href="https://github.com/susanli2016/NLP-with-Python/blob/master/Sentiment%20Analysis%20with%20RNN.ipynb" target="_self">Github</a>. Look forward to hearing feedback or questions.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Reference: <a href="https://www.udacity.com/course/natural-language-processing-nanodegree--nd892" target="_self">Udacity — NLP</a></p></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>