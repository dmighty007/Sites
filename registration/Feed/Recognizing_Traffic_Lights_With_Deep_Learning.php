<!DOCTYPE html>
                <html>
                <head>
                    <title>Recognizing Traffic Lights With Deep Learning</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://medium.com/free-code-camp/recognizing-traffic-lights-with-deep-learning-23dae23287cc"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://medium.com/@davidbrai?source=post_page-----23dae23287cc--------------------------------">Author : David Brailovsky</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>Recognizing Traffic Lights With Deep Learning</h3></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5 text-muted">How I learned deep learning in 10 weeks and won $5,000</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*0lkLBnzdth7GYBMy--PNnA.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*0lkLBnzdth7GYBMy--PNnA.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*0lkLBnzdth7GYBMy--PNnA.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*0lkLBnzdth7GYBMy--PNnA.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*0lkLBnzdth7GYBMy--PNnA.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*0lkLBnzdth7GYBMy--PNnA.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0lkLBnzdth7GYBMy--PNnA.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*0lkLBnzdth7GYBMy--PNnA.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*0lkLBnzdth7GYBMy--PNnA.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*0lkLBnzdth7GYBMy--PNnA.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*0lkLBnzdth7GYBMy--PNnA.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*0lkLBnzdth7GYBMy--PNnA.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*0lkLBnzdth7GYBMy--PNnA.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*0lkLBnzdth7GYBMy--PNnA.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*0lkLBnzdth7GYBMy--PNnA.jpeg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I recently won first place in the <a href="https://challenge.getnexar.com/challenge-1" target="_self">Nexar Traffic Light Recognition Challenge</a>, computer vision competition organized by a company that’s building an AI dash cam app.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In this post, I’ll describe the solution I used. I’ll also explore approaches that did and did not work in my effort to improve my model.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Don’t worry — you don’t need to be an AI expert to understand this post. I’ll focus on the ideas and methods I used as opposed to the technical implementation.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:62%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 467px" srcset="https://miro.medium.com/v2/resize:fit:640/1*sMLfMnOVmSIKBgCU1Wj9WQ.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*sMLfMnOVmSIKBgCU1Wj9WQ.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*sMLfMnOVmSIKBgCU1Wj9WQ.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*sMLfMnOVmSIKBgCU1Wj9WQ.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*sMLfMnOVmSIKBgCU1Wj9WQ.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*sMLfMnOVmSIKBgCU1Wj9WQ.gif 1100w, https://miro.medium.com/v2/resize:fit:934/1*sMLfMnOVmSIKBgCU1Wj9WQ.gif 934w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 467px" srcset="https://miro.medium.com/v2/resize:fit:640/1*sMLfMnOVmSIKBgCU1Wj9WQ.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*sMLfMnOVmSIKBgCU1Wj9WQ.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*sMLfMnOVmSIKBgCU1Wj9WQ.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*sMLfMnOVmSIKBgCU1Wj9WQ.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*sMLfMnOVmSIKBgCU1Wj9WQ.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*sMLfMnOVmSIKBgCU1Wj9WQ.gif 1100w, https://miro.medium.com/v2/resize:fit:934/1*sMLfMnOVmSIKBgCU1Wj9WQ.gif 934w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/467/1*sMLfMnOVmSIKBgCU1Wj9WQ.gif"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Demo of a deep learning based classifier for recognizing traffic lights</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">The challenge</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The goal of the challenge was to recognize the traffic light state in images taken by drivers using the Nexar app. In any given image, the classifier needed to output whether there was a traffic light in the scene, and whether it was red or green. More specifically, it should only identify traffic lights in the driving direction.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here are a few examples to make it clearer:</p></div></div></div></section><section data-bs-version="5.1" class="gallery3 cid-ttaWMCQCls" id="gallery3-7" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container" style="width:60%;"><div class="row mt-4">
            <div class="item features-image сol-12 col-md-6 col-lg-6">
                <div class="item-wrapper" >
                    <div class="item-img"><img src ="https://miro.medium.com/v2/resize:fit:668/format:webp/1*PbnKd_L7AJqn_RQPzHgcCQ.png" style="height:auto; max-width: 100%;"></div></div></div><br>
            <div class="item features-image сol-12 col-md-6 col-lg-6">
                <div class="item-wrapper" >
                    <div class="item-img"><img src ="https://miro.medium.com/v2/resize:fit:668/format:webp/1*xkKmhlqvnFQwxJqDEKwPtw.png" style="height:auto; max-width: 100%;"></div></div></div><br>
            <div class="item features-image сol-12 col-md-6 col-lg-6">
                <div class="item-wrapper" >
                    <div class="item-img"><img src ="https://miro.medium.com/v2/resize:fit:668/format:webp/1*8_Z5OWAD13ug8o6B7tr_gw.png" style="height:auto; max-width: 100%;"></div></div></div><br><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Source: Nexar challenge</p><br></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The images above are examples of the three possible classes I needed to predict: no traffic light (left), red traffic light (center) and green traffic light (right).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The challenge required the solution to be based on <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" target="_self">Convolutional Neural Networks</a>, a very popular method used in image recognition with deep neural networks. The submissions were scored based on the model’s accuracy along with the model’s size (in megabytes). Smaller models got higher scores. In addition, the minimum accuracy required to win was 95%.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Nexar provided 18,659 labeled images as training data. Each image was labeled with one of the three classes mentioned above (no traffic light / red / green).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Software and hardware</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I used <a href="http://caffe.berkeleyvision.org/" target="_self">Caffe</a> to train the models. The main reason I chose Caffe was because of the large variety of pre-trained models.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Python, NumPy & Jupyter Notebook were used for analyzing results, data exploration and ad-hoc scripts.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Amazon’s GPU instances (g2.2xlarge) were used to train the models. My AWS bill ended up being <strong>$263 </strong>(!). Not cheap. 😑</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The code and files I used to train and run the model are on <a href="https://github.com/davidbrai/deep-learning-traffic-lights" target="_self">GitHub</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">The final classifier</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The final classifier achieved an accuracy of <strong>94.955%</strong> on Nexar’s test set, with a model size of ~<strong>7.84 MB</strong>. To compare, <a href="https://arxiv.org/abs/1409.4842" target="_self">GoogLeNet</a> uses a model size of 41 MB, and <a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/" target="_self">VGG-16</a> uses a model size of 528 MB.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Nexar was kind enough to accept 94.955% as 95% to pass the minimum requirement 😁.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The process of getting higher accuracy involved a LOT of trial and error. Some of it had some logic behind it, and some was just “maybe this will work”. I’ll describe some of the things I tried to improve the model that did and didn’t help. The final classifier details are described right after.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">What worked?</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><a href="http://cs231n.github.io/transfer-learning/" target="_self">Transfer learning</a></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I started off with trying to fine-tune a model which was pre-trained on ImageNet with the <a href="https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet" target="_self">GoogLeNet</a> architecture. Pretty quickly this got me to &gt;90% accuracy! 😯</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Nexar mentioned in the <a href="https://challenge.getnexar.com/challenge-1" target="_self">challenge page</a> that it should be possible to reach 93% by fine-tuning GoogLeNet. Not exactly sure what I did wrong there, I might look into it.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><a href="https://arxiv.org/abs/1602.07360" target="_self">SqueezeNet</a></h4></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size.</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Since the competition rewards solutions that use small models, early on I decided to look for a compact network with as few parameters as possible that can still produce good results. Most of the recently published networks are <em>very</em> deep and have <em>a lot</em> of parameters. <a href="https://arxiv.org/abs/1602.07360" target="_self">SqueezeNet</a> seemed to be a very good fit, and it also had a pre-trained model trained on ImageNet available in <a href="http://caffe.berkeleyvision.org/" target="_self">Caffe</a>’s <a href="https://github.com/BVLC/caffe/wiki/Model-Zoo" target="_self">Model Zoo</a> which came in handy.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:87%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 638px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*r4szCEhY_3DI9exJzdTK6g.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*r4szCEhY_3DI9exJzdTK6g.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*r4szCEhY_3DI9exJzdTK6g.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*r4szCEhY_3DI9exJzdTK6g.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*r4szCEhY_3DI9exJzdTK6g.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*r4szCEhY_3DI9exJzdTK6g.png 1100w, https://miro.medium.com/v2/resize:fit:1276/format:webp/1*r4szCEhY_3DI9exJzdTK6g.png 1276w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 638px" srcset="https://miro.medium.com/v2/resize:fit:640/1*r4szCEhY_3DI9exJzdTK6g.png 640w, https://miro.medium.com/v2/resize:fit:720/1*r4szCEhY_3DI9exJzdTK6g.png 720w, https://miro.medium.com/v2/resize:fit:750/1*r4szCEhY_3DI9exJzdTK6g.png 750w, https://miro.medium.com/v2/resize:fit:786/1*r4szCEhY_3DI9exJzdTK6g.png 786w, https://miro.medium.com/v2/resize:fit:828/1*r4szCEhY_3DI9exJzdTK6g.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*r4szCEhY_3DI9exJzdTK6g.png 1100w, https://miro.medium.com/v2/resize:fit:1276/1*r4szCEhY_3DI9exJzdTK6g.png 1276w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/638/1*r4szCEhY_3DI9exJzdTK6g.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">SqueezeNet network architecture. <a href="http://www.slideshare.net/embeddedvision/techniques-for-efficient-implementation-of-deep-neural-networks-a-presentation-from-stanford" target="_self">Slides</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The network manages to stay compact by:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Using mostly 1x1 convolution filters and some 3x3</li><li class="ff3" style="font-size:22px;">Reducing number of input channels into the 3x3 filters</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For more details, I recommend reading this <a href="https://gab41.lab41.org/lab41-reading-group-squeezenet-9b9d1d754c75#.oprbydtxv" target="_self">blog post</a> by Lab41 or the <a href="https://arxiv.org/abs/1602.07360" target="_self">original paper</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After some back and forth with adjusting the learning rate I was able to fine-tune the pre-trained model as well as training from scratch with good accuracy results: 92%! Very cool! 🙌</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Rotating images</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:61%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 455px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*X7aV0pK2krETntjlmIxQhg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*X7aV0pK2krETntjlmIxQhg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*X7aV0pK2krETntjlmIxQhg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*X7aV0pK2krETntjlmIxQhg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*X7aV0pK2krETntjlmIxQhg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*X7aV0pK2krETntjlmIxQhg.png 1100w, https://miro.medium.com/v2/resize:fit:910/format:webp/1*X7aV0pK2krETntjlmIxQhg.png 910w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 455px" srcset="https://miro.medium.com/v2/resize:fit:640/1*X7aV0pK2krETntjlmIxQhg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*X7aV0pK2krETntjlmIxQhg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*X7aV0pK2krETntjlmIxQhg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*X7aV0pK2krETntjlmIxQhg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*X7aV0pK2krETntjlmIxQhg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*X7aV0pK2krETntjlmIxQhg.png 1100w, https://miro.medium.com/v2/resize:fit:910/1*X7aV0pK2krETntjlmIxQhg.png 910w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/455/1*X7aV0pK2krETntjlmIxQhg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Source: Nexar</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Most of the images were horizontal like the one above, but about 2.4% were vertical, and with all kinds of directions for “up”. See below.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*VXmX-CoyA0LTMzZswPtkHw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*VXmX-CoyA0LTMzZswPtkHw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*VXmX-CoyA0LTMzZswPtkHw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*VXmX-CoyA0LTMzZswPtkHw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*VXmX-CoyA0LTMzZswPtkHw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*VXmX-CoyA0LTMzZswPtkHw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VXmX-CoyA0LTMzZswPtkHw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*VXmX-CoyA0LTMzZswPtkHw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*VXmX-CoyA0LTMzZswPtkHw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*VXmX-CoyA0LTMzZswPtkHw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*VXmX-CoyA0LTMzZswPtkHw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*VXmX-CoyA0LTMzZswPtkHw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*VXmX-CoyA0LTMzZswPtkHw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*VXmX-CoyA0LTMzZswPtkHw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*VXmX-CoyA0LTMzZswPtkHw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Different orientations of vertical images. Source: <a href="https://challenge.getnexar.com/challenge-1" target="_self">Nexar challenge</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Although it’s not a big part of the data-set, I wanted the model to classify them correctly too.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Unfortunately, there was no EXIF data in the jpeg images specifying the orientation. At first I considered doing some heuristic to identify the sky and flip the image accordingly, but that did not seem straightforward.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Instead, I tried to make the model invariant to rotations. My first attempt was to train the network with random rotations of 0°, 90°, 180°, 270°. That didn’t help 🤔. But when averaging the predictions of 4 rotations for each image, there was improvement!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">92% → 92.6% 👍</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To clarify: by “averaging the predictions” I mean averaging the probabilities the model produced of each class across the 4 image variations.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Oversampling crops</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">During training the SqueezeNet network first performed random cropping on the input images by default, and I didn’t change it. This type of data augmentation makes the network generalize better.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Similarly, when generating predictions, I took several crops of the input image and averaged the results. I used 5 crops: 4 corners and a center crop. The implementation was free by using existing <a href="https://github.com/BVLC/caffe/blob/master/python/caffe/classifier.py" target="_self">caffe code</a> for this.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">92% → 92.46% 👌</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Rotating images together with oversampling crops showed very slight improvement.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Additional training with lower learning rate</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">All models were starting to overfit after a certain point. I noticed this by watching the validation-set loss start to rise at some point.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:73%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 545px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*nyz2mHi5-oplgreNXbX6pg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*nyz2mHi5-oplgreNXbX6pg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*nyz2mHi5-oplgreNXbX6pg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*nyz2mHi5-oplgreNXbX6pg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*nyz2mHi5-oplgreNXbX6pg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*nyz2mHi5-oplgreNXbX6pg.png 1100w, https://miro.medium.com/v2/resize:fit:1090/format:webp/1*nyz2mHi5-oplgreNXbX6pg.png 1090w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 545px" srcset="https://miro.medium.com/v2/resize:fit:640/1*nyz2mHi5-oplgreNXbX6pg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*nyz2mHi5-oplgreNXbX6pg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*nyz2mHi5-oplgreNXbX6pg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*nyz2mHi5-oplgreNXbX6pg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*nyz2mHi5-oplgreNXbX6pg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*nyz2mHi5-oplgreNXbX6pg.png 1100w, https://miro.medium.com/v2/resize:fit:1090/1*nyz2mHi5-oplgreNXbX6pg.png 1090w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/545/1*nyz2mHi5-oplgreNXbX6pg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Validation loss rising from around iteration 40,000</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I stopped the training at that point because the model was probably not generalizing any more. This meant that the learning rate didn’t have time to decay all the way to zero. I tried resuming the training process at the point where the model started overfitting with a learning rate 10 times lower than the original one. This usually improved the accuracy by 0-0.5%.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">More training data</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">At first, I split my data into 3 sets: training (64%), validation (16%) & test (20%). After a few days, I thought that giving up 36% of the data might be too much. I merged the training & validations sets and used the test-set to check my results.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I retrained a model with “image rotations” and “additional training at lower rate” and saw improvement:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">92.6% → 93.5% 🤘</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Relabeling mistakes in the training data</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">When analyzing the mistakes the classifier had on the validation set, I noticed that some of the mistakes have very high confidence. In other words, the model is certain it’s one thing (e.g. green light) while the training data says another (e.g. red light).</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:67%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 503px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*bsmdEfIZxNyiumD9-3UPgw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*bsmdEfIZxNyiumD9-3UPgw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*bsmdEfIZxNyiumD9-3UPgw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*bsmdEfIZxNyiumD9-3UPgw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*bsmdEfIZxNyiumD9-3UPgw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*bsmdEfIZxNyiumD9-3UPgw.png 1100w, https://miro.medium.com/v2/resize:fit:1006/format:webp/1*bsmdEfIZxNyiumD9-3UPgw.png 1006w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 503px" srcset="https://miro.medium.com/v2/resize:fit:640/1*bsmdEfIZxNyiumD9-3UPgw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*bsmdEfIZxNyiumD9-3UPgw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*bsmdEfIZxNyiumD9-3UPgw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*bsmdEfIZxNyiumD9-3UPgw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*bsmdEfIZxNyiumD9-3UPgw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*bsmdEfIZxNyiumD9-3UPgw.png 1100w, https://miro.medium.com/v2/resize:fit:1006/1*bsmdEfIZxNyiumD9-3UPgw.png 1006w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/503/1*bsmdEfIZxNyiumD9-3UPgw.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Notice that in the plot above, the right-most bar is pretty high. That means there’s a high number of mistakes with &gt;95% confidence. When examining these cases up close I saw these were usually mistakes in the ground-truth of the training set rather than in the trained model.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I decided to fix these errors in the training set. The reasoning was that these mistakes confuse the model, making it harder for it to generalize. Even if the final testing-set has mistakes in the ground-truth, a more generalized model has a better chance of high accuracy across all the images.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I manually labeled 709 images that one of my models got wrong. This changed the ground-truth for 337 out of the 709 images. It took about an hour of manual work with a <a href="https://github.com/davidbrai/deep-learning-traffic-lights/blob/14749dacf75318842f45fc5a9900c300eb83755f/analysis/label_misses.py" target="_self">python script</a> to help me be efficient.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:67%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 503px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Z1hJaGQtJNZvrdqjbRyAOA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Z1hJaGQtJNZvrdqjbRyAOA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Z1hJaGQtJNZvrdqjbRyAOA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Z1hJaGQtJNZvrdqjbRyAOA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Z1hJaGQtJNZvrdqjbRyAOA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Z1hJaGQtJNZvrdqjbRyAOA.png 1100w, https://miro.medium.com/v2/resize:fit:1006/format:webp/1*Z1hJaGQtJNZvrdqjbRyAOA.png 1006w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 503px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Z1hJaGQtJNZvrdqjbRyAOA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*Z1hJaGQtJNZvrdqjbRyAOA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*Z1hJaGQtJNZvrdqjbRyAOA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*Z1hJaGQtJNZvrdqjbRyAOA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*Z1hJaGQtJNZvrdqjbRyAOA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*Z1hJaGQtJNZvrdqjbRyAOA.png 1100w, https://miro.medium.com/v2/resize:fit:1006/1*Z1hJaGQtJNZvrdqjbRyAOA.png 1006w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/503/1*Z1hJaGQtJNZvrdqjbRyAOA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Above is the same plot after re-labeling and retraining the model. Looks better!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This improved the previous model by:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">93.5% → 94.1% ✌️</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Ensemble of models</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Using several models together and averaging their results improved the accuracy as well. I experimented with different kinds of modifications in the training process of the models involved in the ensemble. A noticeable improvement was achieved by using a model trained from scratch even though it had lower accuracy on its own together with the models that were fine-tuned on pre-trained models. Perhaps this is because this model learned different features than the ones that were fine-tuned on pre-trained models.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The ensemble used 3 models with accuracies of 94.1%, 94.2% and 92.9% and together got an accuracy of 94.8%. 👾</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">What didn’t work?</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Lots of things! 🤕 Hopefully some of these ideas can be useful in other settings.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Combatting overfitting</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">While trying to deal with overfitting I tried several things, none of which produced significant improvements:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">increasing the dropout ratio in the network</li><li class="ff3" style="font-size:22px;">more data augmentation (random shifts, zooms, skews)</li><li class="ff3" style="font-size:22px;">training on more data: using 90/10 split instead of 80/20</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Balancing the dataset</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The dataset wasn’t very balanced:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">19% of images were labeled with no traffic light</li><li class="ff3" style="font-size:22px;">53% red light</li><li class="ff3" style="font-size:22px;">28% green light.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I tried balancing the dataset by oversampling the less common classes but didn’t notice any improvement.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Separating day & night</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">My intuition was that recognizing traffic lights in daylight and nighttime is very different. I thought maybe I could help the model by separating it into two simpler problems.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">It was fairly easy to separate the images to day and night by looking at their average pixel intensity:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:75%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 555px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*s7Oqnhres1qlVdlKpQzDew.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*s7Oqnhres1qlVdlKpQzDew.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*s7Oqnhres1qlVdlKpQzDew.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*s7Oqnhres1qlVdlKpQzDew.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*s7Oqnhres1qlVdlKpQzDew.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*s7Oqnhres1qlVdlKpQzDew.png 1100w, https://miro.medium.com/v2/resize:fit:1110/format:webp/1*s7Oqnhres1qlVdlKpQzDew.png 1110w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 555px" srcset="https://miro.medium.com/v2/resize:fit:640/1*s7Oqnhres1qlVdlKpQzDew.png 640w, https://miro.medium.com/v2/resize:fit:720/1*s7Oqnhres1qlVdlKpQzDew.png 720w, https://miro.medium.com/v2/resize:fit:750/1*s7Oqnhres1qlVdlKpQzDew.png 750w, https://miro.medium.com/v2/resize:fit:786/1*s7Oqnhres1qlVdlKpQzDew.png 786w, https://miro.medium.com/v2/resize:fit:828/1*s7Oqnhres1qlVdlKpQzDew.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*s7Oqnhres1qlVdlKpQzDew.png 1100w, https://miro.medium.com/v2/resize:fit:1110/1*s7Oqnhres1qlVdlKpQzDew.png 1110w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/555/1*s7Oqnhres1qlVdlKpQzDew.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">You can see a very natural separation of images with low average values, i.e. dark images, taken at nighttime, and bright images, taken at daytime.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I tried two approaches, both didn’t improve the results:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Training two separate models for day images and night images</li><li class="ff3" style="font-size:22px;">Training the network to predict 6 classes instead of 3 by also predicting whether it’s day or night</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Using better variants of SqueezeNet</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I experimented a little bit with two improved variants of SqueezeNet. The first used <a href="https://github.com/songhan/SqueezeNet-Residual" target="_self">residual connections</a> and the second was trained with <a href="https://github.com/songhan/SqueezeNet-DSD-Training" target="_self">dense→sparse→dense</a> training (more details in the paper). No luck. 😕</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Localization of traffic lights</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After reading a great <a href="http://deepsense.io/deep-learning-right-whale-recognition-kaggle/" target="_self">post</a> by deepsense.io on how they won the whale recognition challenge, I tried to train a localizer, i.e. identify the location of the traffic light in the image first, and then identify the traffic light state on a small region of the image.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I used <a href="http://sloth.readthedocs.io/en/latest/" target="_self">sloth</a> to annotate about 2,000 images which took a few hours. When trying to train a model, it was overfitting very quickly, probably because there was not enough labeled data. Perhaps this could work if I had annotated a lot more images.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Training a classifier on the hard cases</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I chose 30% of the “harder” images by selecting images which my classifier was less than 97% confident about. I then tried to train classifier just on these images. No improvement. 😑</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Different optimization algorithm</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I experimented very shortly with using Caffe’s Adam solver instead of SGD with linearly decreasing learning rate but didn’t see any improvement. 🤔</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Adding more models to ensemble</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Since the ensemble method proved helpful, I tried to double-down on it. I tried changing different parameters to produce different models and add them to the ensemble: initial seed, dropout rate, different training data (different split), different checkpoint in the training. None of these made any significant improvement. 😞</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Final classifier details</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The classifier uses an ensemble of 3 separately trained networks. A weighted average of the probabilities they give to each class is used as the output. All three networks were using the <a href="https://arxiv.org/abs/1602.07360" target="_self">SqueezeNet</a> network but each one was trained differently.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Model #1 — Pre-trained network with oversampling</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Trained on the re-labeled training set (after fixing the ground-truth mistakes). The model was fine-tuned based on a pre-trained model of SqueezeNet trained on ImageNet.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Data augmentation during training:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Random horizontal mirroring</li><li class="ff3" style="font-size:22px;">Randomly cropping patches of size 227 x 227 before feeding into the network</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">At test time, the predictions of 10 variations of each image were averaged to calculate the final prediction. The 10 variations were made of:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">5 crops of size 227 x 227: 1 for each corner and 1 in the center of the image</li><li class="ff3" style="font-size:22px;">for each crop, a horizontally mirrored version was also used</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Model accuracy on validation set: 94.21%<br></br>Model size: ~2.6 MB</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Model #2 — Adding rotation invariance</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Very similar to Model #1, with the addition of image rotations. During training time, images were randomly rotated by 90°, 180°, 270° or not at all. At test-time, each one of the 10 variations described in Model #1 created three more variations by rotating it by 90°, 180° and 270°. A total of 40 variations were classified by our model and averaged together.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Model accuracy on validation set: 94.1%<br></br>Model size: ~2.6 MB</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Model #3 — Trained from scratch</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This model was not fine-tuned, but instead <em>trained from scratch</em>. The rationale behind it was that even though it achieves lower accuracy, it learns different features on the training set than the previous two models, which could be useful when used in an ensemble.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Data augmentation during training and testing are the same as Model #1: mirroring and cropping.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Model accuracy on validation set: 92.92%<br></br>Model size: ~2.6 MB</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Combining the models together</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Each model output three values, representing the probability that the image belongs to each one of the three classes. We averaged their outputs with the following weights:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Model #1: 0.28</li><li class="ff3" style="font-size:22px;">Model #2: 0.49</li><li class="ff3" style="font-size:22px;">Model #3: 0.23</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The values for the weights were found by doing a grid-search over possible values and testing it on the validation set. They are probably a little overfitted to the validation set, but perhaps not too much since this is a very simple operation.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Model accuracy on validation set: 94.83%<br></br>Model size: ~7.84 MB<br></br>Model accuracy on Nexar’s test set: 94.955% 🎉</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Examples of the model mistakes</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:61%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 455px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*i08IJ3KHwfd220O9Kf7MQQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*i08IJ3KHwfd220O9Kf7MQQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*i08IJ3KHwfd220O9Kf7MQQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*i08IJ3KHwfd220O9Kf7MQQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*i08IJ3KHwfd220O9Kf7MQQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*i08IJ3KHwfd220O9Kf7MQQ.png 1100w, https://miro.medium.com/v2/resize:fit:910/format:webp/1*i08IJ3KHwfd220O9Kf7MQQ.png 910w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 455px" srcset="https://miro.medium.com/v2/resize:fit:640/1*i08IJ3KHwfd220O9Kf7MQQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*i08IJ3KHwfd220O9Kf7MQQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*i08IJ3KHwfd220O9Kf7MQQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*i08IJ3KHwfd220O9Kf7MQQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*i08IJ3KHwfd220O9Kf7MQQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*i08IJ3KHwfd220O9Kf7MQQ.png 1100w, https://miro.medium.com/v2/resize:fit:910/1*i08IJ3KHwfd220O9Kf7MQQ.png 910w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/455/1*i08IJ3KHwfd220O9Kf7MQQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Source: Nexar</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The green dot in the palm tree produced by the glare probably made the model predict there’s a green light by mistake.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:61%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 455px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*OhT3o6u7sZSxt1KfjIpReQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*OhT3o6u7sZSxt1KfjIpReQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*OhT3o6u7sZSxt1KfjIpReQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*OhT3o6u7sZSxt1KfjIpReQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*OhT3o6u7sZSxt1KfjIpReQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*OhT3o6u7sZSxt1KfjIpReQ.png 1100w, https://miro.medium.com/v2/resize:fit:910/format:webp/1*OhT3o6u7sZSxt1KfjIpReQ.png 910w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 455px" srcset="https://miro.medium.com/v2/resize:fit:640/1*OhT3o6u7sZSxt1KfjIpReQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*OhT3o6u7sZSxt1KfjIpReQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*OhT3o6u7sZSxt1KfjIpReQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*OhT3o6u7sZSxt1KfjIpReQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*OhT3o6u7sZSxt1KfjIpReQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*OhT3o6u7sZSxt1KfjIpReQ.png 1100w, https://miro.medium.com/v2/resize:fit:910/1*OhT3o6u7sZSxt1KfjIpReQ.png 910w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/455/1*OhT3o6u7sZSxt1KfjIpReQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Source: Nexar</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The model predicted red instead of green. Tricky case when there is more than one traffic light in the scene.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:61%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 455px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*dqWcTNFW6NRYXPnSLvfb8g.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*dqWcTNFW6NRYXPnSLvfb8g.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*dqWcTNFW6NRYXPnSLvfb8g.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*dqWcTNFW6NRYXPnSLvfb8g.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*dqWcTNFW6NRYXPnSLvfb8g.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*dqWcTNFW6NRYXPnSLvfb8g.png 1100w, https://miro.medium.com/v2/resize:fit:910/format:webp/1*dqWcTNFW6NRYXPnSLvfb8g.png 910w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 455px" srcset="https://miro.medium.com/v2/resize:fit:640/1*dqWcTNFW6NRYXPnSLvfb8g.png 640w, https://miro.medium.com/v2/resize:fit:720/1*dqWcTNFW6NRYXPnSLvfb8g.png 720w, https://miro.medium.com/v2/resize:fit:750/1*dqWcTNFW6NRYXPnSLvfb8g.png 750w, https://miro.medium.com/v2/resize:fit:786/1*dqWcTNFW6NRYXPnSLvfb8g.png 786w, https://miro.medium.com/v2/resize:fit:828/1*dqWcTNFW6NRYXPnSLvfb8g.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*dqWcTNFW6NRYXPnSLvfb8g.png 1100w, https://miro.medium.com/v2/resize:fit:910/1*dqWcTNFW6NRYXPnSLvfb8g.png 910w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/455/1*dqWcTNFW6NRYXPnSLvfb8g.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The model said there’s no traffic light while there’s a green traffic light ahead.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Conclusion</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This was the first time I applied deep learning on a real problem! I was happy to see it worked so well. I learned a LOT during the process and will probably write another post that will hopefully help newcomers waste less time on some of the mistakes and technical challenges I had.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I want to thank Nexar for providing this great challenge and hope they organize more of these in the future! 🙌</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>If you enjoyed reading this post, please tap </em><strong>♥ </strong><em>below!</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>Would love to get your feedback and questions below!</em></p></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>