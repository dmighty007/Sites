<!DOCTYPE html>
                <html>
                <head>
                    <title>Report on Text Classification using CNN, RNN & HAN</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://medium.com/@aks3d76?source=post_page-----f0e887214d5f--------------------------------">Author : Akshat Maheshwari</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>Report on Text Classification using CNN, RNN & HAN</h3></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5"><strong>Introduction</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Hello World!! I recently joined <a href="https://www.jatana.ai/" target="_self">Jatana.ai</a><strong> </strong>as <a href="https://en.wikipedia.org/wiki/Natural_language_processing" target="_self">NLP</a> Researcher (Intern üòá) and I was asked to work on the text classification use cases using Deep learning models.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In this article I will share my experiences and learnings while experimenting with various neural networks architectures.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I will cover 3 main algorithms such as:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;"><strong>Convolutional Neural Network (CNN)</strong></li><li class="ff3" style="font-size:22px;"><strong>Recurrent Neural Network (RNN)</strong></li><li class="ff3" style="font-size:22px;"><strong>Hierarchical Attention Network (HAN)</strong></li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Text classification was performed on datasets having Danish, Italian, German, English and Turkish languages.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Let‚Äôs get to it. ‚úÖ</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5"><strong>About Natural </strong>Language Processing (NLP)</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">One of the widely used <em>Natural Language Processing & Supervised Machine Learning (ML)</em> task in different business problems is <strong>‚ÄúText Classification‚Äù</strong>, it‚Äôs an example of Supervised Machine Learning task since a labelled dataset containing text documents and their labels is used for training a classifier.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The goal of text classification is to automatically classify the text documents into one or more predefined categories.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Some examples of text classification are:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Understanding audience sentiment (üòÅ üòê üò•) from social media</li><li class="ff3" style="font-size:22px;">Detection of spam & non-spam emails</li><li class="ff3" style="font-size:22px;"><mark>Auto tagging of customer queries</mark></li><li class="ff3" style="font-size:22px;">Categorisation of news articles üì∞ into predefined topics</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Text Classification is a very active research area both in academia üìö and industry. In this post, I will try to present a few different approaches and compare their performances, where implementation is based on <a href="https://keras.io/" target="_self">Keras</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">All the source code and the results of experiments can be found in jatana_research <a href="https://github.com/jatana-research/Text-Classification" target="_self">repository.</a></p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:80%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 590px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*M8gKrj7I7LNTB-G3winTng.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*M8gKrj7I7LNTB-G3winTng.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*M8gKrj7I7LNTB-G3winTng.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*M8gKrj7I7LNTB-G3winTng.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*M8gKrj7I7LNTB-G3winTng.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*M8gKrj7I7LNTB-G3winTng.png 1100w, https://miro.medium.com/v2/resize:fit:1180/format:webp/1*M8gKrj7I7LNTB-G3winTng.png 1180w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 590px" srcset="https://miro.medium.com/v2/resize:fit:640/1*M8gKrj7I7LNTB-G3winTng.png 640w, https://miro.medium.com/v2/resize:fit:720/1*M8gKrj7I7LNTB-G3winTng.png 720w, https://miro.medium.com/v2/resize:fit:750/1*M8gKrj7I7LNTB-G3winTng.png 750w, https://miro.medium.com/v2/resize:fit:786/1*M8gKrj7I7LNTB-G3winTng.png 786w, https://miro.medium.com/v2/resize:fit:828/1*M8gKrj7I7LNTB-G3winTng.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*M8gKrj7I7LNTB-G3winTng.png 1100w, https://miro.medium.com/v2/resize:fit:1180/1*M8gKrj7I7LNTB-G3winTng.png 1180w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/590/1*M8gKrj7I7LNTB-G3winTng.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">An end-to-end text classification pipeline is composed of following components:</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;"><strong>Training text: </strong>It is the input text through which our supervised learning model is able to learn and predict the required class.</li><li class="ff3" style="font-size:22px;"><strong>Feature Vector:</strong> A feature vector is a vector that contains information describing the characteristics of the input data.</li><li class="ff3" style="font-size:22px;"><strong>Labels:</strong> These are the predefined categories/classes that our model will predict</li><li class="ff3" style="font-size:22px;"><strong>ML Algo:</strong> It is the algorithm through which our model is able to deal with text classification (In our case : CNN, RNN, HAN)</li><li class="ff3" style="font-size:22px;"><strong>Predictive Model:</strong> A model which is trained on the historical dataset which can perform label predictions.</li></ol></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:86%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 635px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*xuvsmqxSjgvqJKs1j_Bo8w.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*xuvsmqxSjgvqJKs1j_Bo8w.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*xuvsmqxSjgvqJKs1j_Bo8w.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*xuvsmqxSjgvqJKs1j_Bo8w.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*xuvsmqxSjgvqJKs1j_Bo8w.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*xuvsmqxSjgvqJKs1j_Bo8w.png 1100w, https://miro.medium.com/v2/resize:fit:1270/format:webp/1*xuvsmqxSjgvqJKs1j_Bo8w.png 1270w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 635px" srcset="https://miro.medium.com/v2/resize:fit:640/1*xuvsmqxSjgvqJKs1j_Bo8w.png 640w, https://miro.medium.com/v2/resize:fit:720/1*xuvsmqxSjgvqJKs1j_Bo8w.png 720w, https://miro.medium.com/v2/resize:fit:750/1*xuvsmqxSjgvqJKs1j_Bo8w.png 750w, https://miro.medium.com/v2/resize:fit:786/1*xuvsmqxSjgvqJKs1j_Bo8w.png 786w, https://miro.medium.com/v2/resize:fit:828/1*xuvsmqxSjgvqJKs1j_Bo8w.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*xuvsmqxSjgvqJKs1j_Bo8w.png 1100w, https://miro.medium.com/v2/resize:fit:1270/1*xuvsmqxSjgvqJKs1j_Bo8w.png 1270w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/635/1*xuvsmqxSjgvqJKs1j_Bo8w.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Analysing Our Data :</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We are using 3 types of dataset with various classes as shown in table below:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:86%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 634px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*iXN2zDwgjAWQXnuwWm7rGw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*iXN2zDwgjAWQXnuwWm7rGw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*iXN2zDwgjAWQXnuwWm7rGw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*iXN2zDwgjAWQXnuwWm7rGw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*iXN2zDwgjAWQXnuwWm7rGw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*iXN2zDwgjAWQXnuwWm7rGw.png 1100w, https://miro.medium.com/v2/resize:fit:1268/format:webp/1*iXN2zDwgjAWQXnuwWm7rGw.png 1268w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 634px" srcset="https://miro.medium.com/v2/resize:fit:640/1*iXN2zDwgjAWQXnuwWm7rGw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*iXN2zDwgjAWQXnuwWm7rGw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*iXN2zDwgjAWQXnuwWm7rGw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*iXN2zDwgjAWQXnuwWm7rGw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*iXN2zDwgjAWQXnuwWm7rGw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*iXN2zDwgjAWQXnuwWm7rGw.png 1100w, https://miro.medium.com/v2/resize:fit:1268/1*iXN2zDwgjAWQXnuwWm7rGw.png 1268w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/634/1*iXN2zDwgjAWQXnuwWm7rGw.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5"><strong>Text Classification Using </strong>Convolutional Neural Network (CNN) :</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" target="_self">CNN</a> is a class of deep, feed-forward artificial neural networks ( where connections between nodes do <em>not</em> form a cycle) & use a variation of multilayer perceptrons designed to require minimal preprocessing. These are inspired by animal visual cortex.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I have taken reference from Yoon Kim <a href="https://arxiv.org/abs/1408.5882" target="_self">paper</a> and this <a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/" target="_self">blog</a> by Denny Britz.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">CNNs are generally used in computer vision, however they‚Äôve recently been applied to various NLP tasks and the <strong>results were promising </strong>üôå<strong> .</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Let‚Äôs briefly see what happens when we use CNN on text data through a diagram.The result of each convolution will fire when a special pattern is detected. By varying the size of the kernels and concatenating their outputs, you‚Äôre allowing yourself to detect patterns of multiples sizes (2, 3, or 5 adjacent words).Patterns could be expressions (word ngrams?) like ‚ÄúI hate‚Äù, ‚Äúvery good‚Äù and therefore CNNs can identify them in the sentence regardless of their position.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*0efgxnFIaLTZ2qkY 640w, https://miro.medium.com/v2/resize:fit:720/0*0efgxnFIaLTZ2qkY 720w, https://miro.medium.com/v2/resize:fit:750/0*0efgxnFIaLTZ2qkY 750w, https://miro.medium.com/v2/resize:fit:786/0*0efgxnFIaLTZ2qkY 786w, https://miro.medium.com/v2/resize:fit:828/0*0efgxnFIaLTZ2qkY 828w, https://miro.medium.com/v2/resize:fit:1100/0*0efgxnFIaLTZ2qkY 1100w, https://miro.medium.com/v2/resize:fit:1400/0*0efgxnFIaLTZ2qkY 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*0efgxnFIaLTZ2qkY 640w, https://miro.medium.com/v2/resize:fit:720/0*0efgxnFIaLTZ2qkY 720w, https://miro.medium.com/v2/resize:fit:750/0*0efgxnFIaLTZ2qkY 750w, https://miro.medium.com/v2/resize:fit:786/0*0efgxnFIaLTZ2qkY 786w, https://miro.medium.com/v2/resize:fit:828/0*0efgxnFIaLTZ2qkY 828w, https://miro.medium.com/v2/resize:fit:1100/0*0efgxnFIaLTZ2qkY 1100w, https://miro.medium.com/v2/resize:fit:1400/0*0efgxnFIaLTZ2qkY 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*0efgxnFIaLTZ2qkY"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Image Reference : <a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/" target="_self">http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In this section, I have used a simplified CNN to build a classifier. So first use Beautiful Soup in order to remove some HTML tags and some unwanted characters.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>def clean_str(string):<br/>    string = re.sub(r"\\", "", string)    <br/>    string = re.sub(r"\'", "", string)    <br/>    string = re.sub(r"\"", "", string)    <br/>    return string.strip().lower()<br/><br/>texts = [];labels = []<br/><br/>for i in range(df.message.shape[0]):<br/>    text = BeautifulSoup(df.message[i])<br/>    texts.append(clean_str(str(text.get_text().encode())))<br/><br/>for i in df['class']:<br/>    labels.append(i)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here I have used <a href="http://nlp.stanford.edu/projects/glove/" target="_self">Google Glove 6B vector 100d.</a> Its Official documentation :</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>‚Äò‚Äò‚Äò</strong><em> GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. </em><strong>‚Äô‚Äô‚Äô</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For an unknown word, the following code will just randomise its vector. Below is a very simple Convolutional Architecture, using a total of 128 filters with size 5 and max pooling of 5 and 35, following the sample from <a href="https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html" target="_self">this blog</a>.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')<br/>embedded_sequences = embedding_layer(sequence_input)<br/>l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)<br/>l_pool1 = MaxPooling1D(5)(l_cov1)<br/>l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)<br/>l_pool2 = MaxPooling1D(5)(l_cov2)<br/>l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)<br/>l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling<br/>l_flat = Flatten()(l_pool3)<br/>l_dense = Dense(128, activation='relu')(l_flat)<br/>preds = Dense(len(macronum), activation='softmax')(l_dense)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Here is the architecture of the CNN Model.</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:68%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 507px" srcset="https://miro.medium.com/v2/resize:fit:640/0*-16u65Ixd2U601ch 640w, https://miro.medium.com/v2/resize:fit:720/0*-16u65Ixd2U601ch 720w, https://miro.medium.com/v2/resize:fit:750/0*-16u65Ixd2U601ch 750w, https://miro.medium.com/v2/resize:fit:786/0*-16u65Ixd2U601ch 786w, https://miro.medium.com/v2/resize:fit:828/0*-16u65Ixd2U601ch 828w, https://miro.medium.com/v2/resize:fit:1100/0*-16u65Ixd2U601ch 1100w, https://miro.medium.com/v2/resize:fit:1014/0*-16u65Ixd2U601ch 1014w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 507px" srcset="https://miro.medium.com/v2/resize:fit:640/0*-16u65Ixd2U601ch 640w, https://miro.medium.com/v2/resize:fit:720/0*-16u65Ixd2U601ch 720w, https://miro.medium.com/v2/resize:fit:750/0*-16u65Ixd2U601ch 750w, https://miro.medium.com/v2/resize:fit:786/0*-16u65Ixd2U601ch 786w, https://miro.medium.com/v2/resize:fit:828/0*-16u65Ixd2U601ch 828w, https://miro.medium.com/v2/resize:fit:1100/0*-16u65Ixd2U601ch 1100w, https://miro.medium.com/v2/resize:fit:1014/0*-16u65Ixd2U601ch 1014w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/507/0*-16u65Ixd2U601ch"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5"><strong>Text Classification Using </strong>Recurrent Neural Network <strong>(RNN) :</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">A <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network" target="_self">recurrent neural network (RNN)</a> is a class of artificial neural network where connections between nodes form a directed graph along a sequence. This allows it to exhibit dynamic temporal behavior for a time sequence.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Using the knowledge from an external embedding can enhance the precision of your RNN because it integrates new information (lexical and semantic) about the words, an information that has been trained and distilled on a very large corpus of data.The pre-trained embedding we‚Äôll be using is <a href="https://nlp.stanford.edu/projects/glove/" target="_self">GloVe</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">RNNs may look scary üò± . Although they‚Äôre complex to understand, they‚Äôre quite interesting. They encapsulate a very beautiful design that overcomes traditional neural networks‚Äô shortcomings that arise when dealing with sequence data: text, time series, videos, DNA sequences, etc.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">RNN is a sequence of neural network blocks that are linked to each others like a chain. Each one is passing a message to a successor. Again if you want to dive into the internal mechanics, I highly recommend Colah‚Äôs <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_self">blog</a>.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:87%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 637px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*ungLVaw-HBfP39vH-WEt_A.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*ungLVaw-HBfP39vH-WEt_A.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*ungLVaw-HBfP39vH-WEt_A.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*ungLVaw-HBfP39vH-WEt_A.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*ungLVaw-HBfP39vH-WEt_A.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ungLVaw-HBfP39vH-WEt_A.png 1100w, https://miro.medium.com/v2/resize:fit:1274/format:webp/1*ungLVaw-HBfP39vH-WEt_A.png 1274w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 637px" srcset="https://miro.medium.com/v2/resize:fit:640/1*ungLVaw-HBfP39vH-WEt_A.png 640w, https://miro.medium.com/v2/resize:fit:720/1*ungLVaw-HBfP39vH-WEt_A.png 720w, https://miro.medium.com/v2/resize:fit:750/1*ungLVaw-HBfP39vH-WEt_A.png 750w, https://miro.medium.com/v2/resize:fit:786/1*ungLVaw-HBfP39vH-WEt_A.png 786w, https://miro.medium.com/v2/resize:fit:828/1*ungLVaw-HBfP39vH-WEt_A.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*ungLVaw-HBfP39vH-WEt_A.png 1100w, https://miro.medium.com/v2/resize:fit:1274/1*ungLVaw-HBfP39vH-WEt_A.png 1274w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/637/1*ungLVaw-HBfP39vH-WEt_A.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Image Reference : <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_self">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Same preprocessing is also done here using Beautiful Soup. We will process text data, which is a sequence type. The order of words is very important to the meaning. Hopefully RNNs take care of this and can capture long-term dependencies.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To use Keras on text data, we first have to preprocess it. For this, we can use Keras‚Äô Tokenizer class. This object takes as argument <strong>num_words</strong> which is the maximum number of words kept after tokenization based on their word frequency.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>MAX_NB_WORDS = 20000<br/>tokenizer = Tokenizer (num_words=MAX_NB_WORDS) tokenizer.fit_on_texts(texts)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Once the tokenizer is fitted on the data, we can use it to convert text strings to sequences of numbers. These numbers represent the position of each word in the dictionary (think of it as mapping).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">In this section, I will try to tackle the problem by using recurrent neural network and attention based LSTM encoder.</li><li class="ff3" style="font-size:22px;">By using LSTM encoder, we intent to encode all the information of text in the last output of Recurrent Neural Network before running feed forward network for classification.</li><li class="ff3" style="font-size:22px;">This is very similar to neural translation machine and sequence to sequence learning. Following is the figure from <a href="https://arxiv.org/pdf/1506.01057v2.pdf" target="_self">A Hierarchical Neural Autoencoder for Paragraphs and Documents</a>.</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:86%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 636px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*XLDP5QsiD8Fs6KN0xexSyA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*XLDP5QsiD8Fs6KN0xexSyA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*XLDP5QsiD8Fs6KN0xexSyA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*XLDP5QsiD8Fs6KN0xexSyA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*XLDP5QsiD8Fs6KN0xexSyA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*XLDP5QsiD8Fs6KN0xexSyA.png 1100w, https://miro.medium.com/v2/resize:fit:1272/format:webp/1*XLDP5QsiD8Fs6KN0xexSyA.png 1272w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 636px" srcset="https://miro.medium.com/v2/resize:fit:640/1*XLDP5QsiD8Fs6KN0xexSyA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*XLDP5QsiD8Fs6KN0xexSyA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*XLDP5QsiD8Fs6KN0xexSyA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*XLDP5QsiD8Fs6KN0xexSyA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*XLDP5QsiD8Fs6KN0xexSyA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*XLDP5QsiD8Fs6KN0xexSyA.png 1100w, https://miro.medium.com/v2/resize:fit:1272/1*XLDP5QsiD8Fs6KN0xexSyA.png 1272w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/636/1*XLDP5QsiD8Fs6KN0xexSyA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Image Reference : <a href="https://arxiv.org/pdf/1506.01057v2.pdf" target="_self">https://arxiv.org/pdf/1506.01057v2.pdf</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">I‚Äôm using LSTM layer in Keras to implement this. Other than forward LSTM, here I have used bidirectional LSTM and concatenate both last output of LSTM outputs.</li><li class="ff3" style="font-size:22px;">Keras has provide a very nice wrapper called <strong>bidirectional</strong>, which will make this coding exercise effortless. You can see the sample code <a href="https://github.com/fchollet/keras/blob/master/examples/imdb_bidirectional_lstm.py" target="_self">here</a></li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')<br/>embedded_sequences = embedding_layer(sequence_input)<br/>l_lstm = Bidirectional(LSTM(100))(embedded_sequences)<br/>preds = Dense(len(macronum), activation='softmax')(l_lstm)<br/>model = Model(sequence_input, preds)<br/>model.compile(loss='categorical_crossentropy',optimizer='rmsprop',  metrics=['acc'])</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Here is the architecture of the RNN Model.</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:82%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 607px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*VGtBedNuZyX9E-07gnm2Yg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*VGtBedNuZyX9E-07gnm2Yg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*VGtBedNuZyX9E-07gnm2Yg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*VGtBedNuZyX9E-07gnm2Yg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*VGtBedNuZyX9E-07gnm2Yg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*VGtBedNuZyX9E-07gnm2Yg.png 1100w, https://miro.medium.com/v2/resize:fit:1214/format:webp/1*VGtBedNuZyX9E-07gnm2Yg.png 1214w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 607px" srcset="https://miro.medium.com/v2/resize:fit:640/1*VGtBedNuZyX9E-07gnm2Yg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*VGtBedNuZyX9E-07gnm2Yg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*VGtBedNuZyX9E-07gnm2Yg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*VGtBedNuZyX9E-07gnm2Yg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*VGtBedNuZyX9E-07gnm2Yg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*VGtBedNuZyX9E-07gnm2Yg.png 1100w, https://miro.medium.com/v2/resize:fit:1214/1*VGtBedNuZyX9E-07gnm2Yg.png 1214w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/607/1*VGtBedNuZyX9E-07gnm2Yg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Text Classification Using Hierarchical Attention Network<strong> (HAN) :</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I have taken reference from this research paper <a href="https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf" target="_self">Hierarchical Attention Networks for Document Classification</a>. It can be a great guide for Document Classification using HAN. Same pre-processing is also done here using Beautiful Soup. The pre-trained embedding we‚Äôll be using is<a href="https://nlp.stanford.edu/projects/glove/" target="_self"> GloVe</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Here I am building a Hierarchical LSTM network. I have to construct the data input as 3D rather than 2D as in above two sections.</li><li class="ff3" style="font-size:22px;">So the input tensor would be [# of reviews each batch, # of sentences, # of words in each sentence].</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)<br/>tokenizer.fit_on_texts(texts)<br/>data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')</span><span>for i, sentences in enumerate(reviews):<br/>    for j, sent in enumerate(sentences):<br/>        if j&lt; MAX_SENTS:<br/>            wordTokens = text_to_word_sequence(sent)<br/>            k=0<br/>            for _, word in enumerate(wordTokens):<br/>                if(k&lt;MAX_SENT_LENGTH and tokenizer.word_index[word]&lt;MAX_NB_WORDS):<br/>                    data[i,j,k] = tokenizer.word_index[word]<br/>                    k=k+1</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After this we can use Keras magic function TimeDistributed to construct the Hierarchical input layers as following. We can also refer to this <a href="https://offbit.github.io/how-to-read/" target="_self">post</a>.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>embedding_layer=Embedding(len(word_index)+1,EMBEDDING_DIM,weights=[embedding_matrix],</span><span>input_length=MAX_SENT_LENGTH,trainable=True)</span><span>sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')<br/>embedded_sequences = embedding_layer(sentence_input)<br/>l_lstm = Bidirectional(LSTM(100))(embedded_sequences)<br/>sentEncoder = Model(sentence_input, l_lstm)<br/><br/>review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')<br/>review_encoder = TimeDistributed(sentEncoder)(review_input)<br/>l_lstm_sent = Bidirectional(LSTM(100))(review_encoder)<br/>preds = Dense(len(macronum), activation='softmax')(l_lstm_sent)<br/>model = Model(review_input, preds)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Here is the architecture of the HAN Model.</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:78%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 580px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*XCXW1_cmPcD4JMTmz98dzA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*XCXW1_cmPcD4JMTmz98dzA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*XCXW1_cmPcD4JMTmz98dzA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*XCXW1_cmPcD4JMTmz98dzA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*XCXW1_cmPcD4JMTmz98dzA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*XCXW1_cmPcD4JMTmz98dzA.png 1100w, https://miro.medium.com/v2/resize:fit:1160/format:webp/1*XCXW1_cmPcD4JMTmz98dzA.png 1160w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 580px" srcset="https://miro.medium.com/v2/resize:fit:640/1*XCXW1_cmPcD4JMTmz98dzA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*XCXW1_cmPcD4JMTmz98dzA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*XCXW1_cmPcD4JMTmz98dzA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*XCXW1_cmPcD4JMTmz98dzA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*XCXW1_cmPcD4JMTmz98dzA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*XCXW1_cmPcD4JMTmz98dzA.png 1100w, https://miro.medium.com/v2/resize:fit:1160/1*XCXW1_cmPcD4JMTmz98dzA.png 1160w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/580/1*XCXW1_cmPcD4JMTmz98dzA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Results</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:86%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 635px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*ePn3MBTv_orTGt-G_9IeMg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*ePn3MBTv_orTGt-G_9IeMg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*ePn3MBTv_orTGt-G_9IeMg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*ePn3MBTv_orTGt-G_9IeMg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*ePn3MBTv_orTGt-G_9IeMg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ePn3MBTv_orTGt-G_9IeMg.png 1100w, https://miro.medium.com/v2/resize:fit:1270/format:webp/1*ePn3MBTv_orTGt-G_9IeMg.png 1270w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 635px" srcset="https://miro.medium.com/v2/resize:fit:640/1*ePn3MBTv_orTGt-G_9IeMg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*ePn3MBTv_orTGt-G_9IeMg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*ePn3MBTv_orTGt-G_9IeMg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*ePn3MBTv_orTGt-G_9IeMg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*ePn3MBTv_orTGt-G_9IeMg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*ePn3MBTv_orTGt-G_9IeMg.png 1100w, https://miro.medium.com/v2/resize:fit:1270/1*ePn3MBTv_orTGt-G_9IeMg.png 1270w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/635/1*ePn3MBTv_orTGt-G_9IeMg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Here are the plots for Accuracy üìà and Loss üìâ</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*PksLGd953Rk1T2cXmJmMRw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*PksLGd953Rk1T2cXmJmMRw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*PksLGd953Rk1T2cXmJmMRw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*PksLGd953Rk1T2cXmJmMRw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*PksLGd953Rk1T2cXmJmMRw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*PksLGd953Rk1T2cXmJmMRw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PksLGd953Rk1T2cXmJmMRw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*PksLGd953Rk1T2cXmJmMRw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*PksLGd953Rk1T2cXmJmMRw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*PksLGd953Rk1T2cXmJmMRw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*PksLGd953Rk1T2cXmJmMRw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*PksLGd953Rk1T2cXmJmMRw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*PksLGd953Rk1T2cXmJmMRw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*PksLGd953Rk1T2cXmJmMRw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*PksLGd953Rk1T2cXmJmMRw.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*mt4gXJI6hlQWPucMyUY5lw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*mt4gXJI6hlQWPucMyUY5lw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*mt4gXJI6hlQWPucMyUY5lw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*mt4gXJI6hlQWPucMyUY5lw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*mt4gXJI6hlQWPucMyUY5lw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*mt4gXJI6hlQWPucMyUY5lw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mt4gXJI6hlQWPucMyUY5lw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*mt4gXJI6hlQWPucMyUY5lw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*mt4gXJI6hlQWPucMyUY5lw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*mt4gXJI6hlQWPucMyUY5lw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*mt4gXJI6hlQWPucMyUY5lw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*mt4gXJI6hlQWPucMyUY5lw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*mt4gXJI6hlQWPucMyUY5lw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*mt4gXJI6hlQWPucMyUY5lw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*mt4gXJI6hlQWPucMyUY5lw.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5"><strong>Observations üëá :</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Based on the above plots, CNN has achieved good validation accuracy with high consistency, also RNN & HAN have achieved high accuracy but they are not that consistent throughout all the datasets.</li><li class="ff3" style="font-size:22px;">RNN was found to be the worst architecture to implement for production ready scenarios.</li><li class="ff3" style="font-size:22px;">CNN model has outperformed the other two models (RNN & HAN) in terms of training time, however HAN can perform better than CNN and RNN if we have a huge dataset.</li><li class="ff3" style="font-size:22px;">For dataset 1 and dataset 2 where the training samples are more, HAN has achieved the best validation accuracy while when the training samples are very low, then HAN has not performed that good (dataset 3).</li><li class="ff3" style="font-size:22px;">When training samples are less (dataset 3) CNN has achieved the best validation accuracy.</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*yhLvkAeX4COs32Ubp90vGg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*yhLvkAeX4COs32Ubp90vGg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*yhLvkAeX4COs32Ubp90vGg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*yhLvkAeX4COs32Ubp90vGg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*yhLvkAeX4COs32Ubp90vGg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*yhLvkAeX4COs32Ubp90vGg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yhLvkAeX4COs32Ubp90vGg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*yhLvkAeX4COs32Ubp90vGg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*yhLvkAeX4COs32Ubp90vGg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*yhLvkAeX4COs32Ubp90vGg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*yhLvkAeX4COs32Ubp90vGg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*yhLvkAeX4COs32Ubp90vGg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*yhLvkAeX4COs32Ubp90vGg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*yhLvkAeX4COs32Ubp90vGg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*yhLvkAeX4COs32Ubp90vGg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*mLQi98T5wdXnpDjnWV_7-Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*mLQi98T5wdXnpDjnWV_7-Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*mLQi98T5wdXnpDjnWV_7-Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*mLQi98T5wdXnpDjnWV_7-Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*mLQi98T5wdXnpDjnWV_7-Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*mLQi98T5wdXnpDjnWV_7-Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mLQi98T5wdXnpDjnWV_7-Q.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*mLQi98T5wdXnpDjnWV_7-Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*mLQi98T5wdXnpDjnWV_7-Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*mLQi98T5wdXnpDjnWV_7-Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*mLQi98T5wdXnpDjnWV_7-Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*mLQi98T5wdXnpDjnWV_7-Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*mLQi98T5wdXnpDjnWV_7-Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*mLQi98T5wdXnpDjnWV_7-Q.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*mLQi98T5wdXnpDjnWV_7-Q.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5"><strong>Performance Improvements :</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To achieve the best performances üòâ, we may:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;"><strong>Fine Tune Hyper-Parameters : </strong>Hyper-parameters are the variables which are set before training and determine the network structure & how the network is trained. (eg : learning rate, batch size, number of epochs). Fine tuning can be done by : <em>Manual Search, Grid Search, Random Search‚Ä¶</em></li><li class="ff3" style="font-size:22px;"><strong>Improve Text Pre-Processing : </strong>Better pre-processing of input data can be done as per the need of your dataset like removing some special symbols, numbers, stopwords and so on ‚Ä¶</li><li class="ff3" style="font-size:22px;"><strong>Use </strong><a href="https://keras.io/layers/core/#dropout" target="_self">Dropout Layer</a><strong> : </strong>Dropout is regularization technique to avoid overfitting (increase the validation accuracy) thus increasing the generalizing power.</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Infrastructure setup:</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">All the above experiments were performed on 8 core vCPU‚Äôs with <strong>Nvidia Tesla K80 GPU.</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Further all the experiments were performed under the guidance of <a style="color:green" href="https://medium.com/u/fb54c1c7a2a1?source=post_page-----f0e887214d5f--------------------------------" target="_self">Rahul Kumar</a> üòé.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Also I would like to thanks <a href="https://www.jatana.ai/" target="_self">Jatana.ai</a><strong> </strong>for providing me a very good infrastructure and full support throughout my journey üòÉ.</p></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>