<!DOCTYPE html>
                <html>
                <head>
                    <title>Background removal with deep learning</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://towardsdatascience.com/background-removal-with-deep-learning-c4f2104b3157"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://gidishperber.medium.com/?source=post_page-----c4f2104b3157--------------------------------">Author : Gidi Shperber</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>Background removal with deep learning</h3></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*33XKEatpsF875NCc.jpg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*33XKEatpsF875NCc.jpg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*33XKEatpsF875NCc.jpg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*33XKEatpsF875NCc.jpg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*33XKEatpsF875NCc.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*33XKEatpsF875NCc.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*33XKEatpsF875NCc.jpg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*33XKEatpsF875NCc.jpg 640w, https://miro.medium.com/v2/resize:fit:720/0*33XKEatpsF875NCc.jpg 720w, https://miro.medium.com/v2/resize:fit:750/0*33XKEatpsF875NCc.jpg 750w, https://miro.medium.com/v2/resize:fit:786/0*33XKEatpsF875NCc.jpg 786w, https://miro.medium.com/v2/resize:fit:828/0*33XKEatpsF875NCc.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/0*33XKEatpsF875NCc.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/0*33XKEatpsF875NCc.jpg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*33XKEatpsF875NCc.jpg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4">This post describes our work and research on the <a href="http://bgr.shibumi-ai.com" target="_self">greenScreen.AI</a>. We’ll be happy to hear thoughts and comments -On <a href="https://twitter.com/shgidi" target="_self">Twitter</a>, <a href="https://www.linkedin.com/in/gidi-shperber-9623a916/" target="_self">Linkedin</a></p></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4">Also check out our website — <a href="https://www.shibumi-ai.com/" target="_self">www.shibumi-ai.com</a></p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Intro</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Throughout the last few years in machine learning, I’ve always wanted to build real machine learning products.<br></br>A few months ago, after taking the great <a href="http://www.fast.ai/" target="_self">Fast.AI</a> deep learning course, it seemed like the stars aligned, and I have the opportunity: The advances in deep learning technology permitted doing many things that weren’t possible before, and new tools were developed and made the deployment process more accessible than ever.<br></br>In the aforementioned course, I’ve met <a href="https://medium.com/@burgalon/35648f9dc5fb" target="_self">Alon Burg</a>, who is an experienced web developer, an we’ve partnered up to pursue this goal. Together, we’ve set ourselves the following goals:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;">Improving our deep learning skills</li><li class="ff3" style="font-size:22px;">Improving our AI product deployment skills</li><li class="ff3" style="font-size:22px;">Making a useful product, with a market need</li><li class="ff3" style="font-size:22px;">Having fun (for us and for our users)</li><li class="ff3" style="font-size:22px;">Sharing our experience</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Considering the above, we were exploring ideas which:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;">Haven't been done yet (or haven't been done properly)</li><li class="ff3" style="font-size:22px;">Will be not too hard to plan and implement — our plan was 2–3 months of work, with a load of 1 weekly work day.</li><li class="ff3" style="font-size:22px;">Will have an easy and appealing user interface — we wanted to do a product that people will use, not only for demonstration purposes.</li><li class="ff3" style="font-size:22px;">Will have training data readily available — as any machine learning practitioner knows, sometimes the data is more expensive than the algorithm.</li><li class="ff3" style="font-size:22px;">Will use cutting edge deep learning techniques (which were still not commoditized by Google, Amazon and friends in their cloud platforms) but not too cutting edge (so we will be able to find some examples online)</li><li class="ff3" style="font-size:22px;">Will have the potential to achieve “production ready” result.</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Our early thoughts were to take on some medical project, since this field is very close to our hearts, and we felt (and still feel) that there is an enormous number of low hanging fruits for deep learning in the medical field. However, we realized that we are going to stumble upon issues with data collection and perhaps legality and regulation, which was a contradiction with our will to keep it simple. Our second choice was a <strong>background removal </strong>product.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Background removal is a task that is quite easy to do manually, or semi manually (Photoshop, and even Power Point has such tools) if you use some kind of a “marker” and edge detection, see <a href="https://clippingmagic.com/" target="_self">here</a> an example. However, fully automated background removal is quite a challenging task, and as far as we know, there is still no product that has satisfactory results with it, although some do <a href="http://www.logitech.com/en-us/product/c922-pro-stream-webcam" target="_self">try</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">What background will we remove? This turned out to be an important question, since the more specific a model is in terms of objects, angle, etc. the higher quality the separation will be. When starting our work, we thought big: a general background remover that will automatically identify the foreground and background in every type of image. But after training our first model, we understood that it will be better to focus our efforts in a specific set of images. Therefore, we decided to focus on selfies and human portraits.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*QPmjt_wQiEBEheUler07zw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*QPmjt_wQiEBEheUler07zw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*QPmjt_wQiEBEheUler07zw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*QPmjt_wQiEBEheUler07zw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*QPmjt_wQiEBEheUler07zw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*QPmjt_wQiEBEheUler07zw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QPmjt_wQiEBEheUler07zw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*QPmjt_wQiEBEheUler07zw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*QPmjt_wQiEBEheUler07zw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*QPmjt_wQiEBEheUler07zw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*QPmjt_wQiEBEheUler07zw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*QPmjt_wQiEBEheUler07zw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*QPmjt_wQiEBEheUler07zw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*QPmjt_wQiEBEheUler07zw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*QPmjt_wQiEBEheUler07zw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Background removal of (almost) human portrait</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">A selfie is an image with a salient and focused foreground (one or more “<em>persons</em>”) guarantees us a good separation between the object (face+upper body) and the background, along with quite an constant angle, and always the same object (<em>person</em>).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">With these assumptions in mind, we embarked on a journey of research, implementation and hours of training to create a one click easy to use background removal service.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The main part of our work was training the model, but we couldn't underestimate the importance of proper deployment. Good segmentation models are still not compact as the classification model (e.g <a href="https://arxiv.org/abs/1602.07360" target="_self">SqueezeNet</a>) and we actively examined both server and browser deployment options.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you want to read more details about the deployment process(es) of our product, your are welcomed to check out our posts on <a href="https://medium.com/@burgalon/35648f9dc5fb" target="_self">server side</a> and <a href="https://medium.com/@burgalon/2e5a29589ad8" target="_self">client side</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you want to read about the model and it’s training process, keep going.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Semantic Segmentation</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">When examining deep learning and computer vision tasks which resemble ours, it is easy to see that our best option is the <em>semantic segmentation </em>task.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Other strategies, like <a href="https://arxiv.org/pdf/1507.06821.pdf" target="_self">separation by depth detection</a> also exist, but didn’t seem ripe enough for our purposes.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Semantic segmentation</strong> is a well known computer vision task, one of the top three, along with classification and object detection. The segmentation is actually a classification task, in the sense of classifying every pixel to a class. Unlike image classification or detection, segmentation model really shows some “understanding” of the images, in not only saying “there is a cat in this image”, but pointing where and what is the cat, on a pixel level.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">So how does the segmentation work? To better understand, we will have to examine some of the early works in this field.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The earliest idea was to adopt some of the early classification networks such as <strong>VGG</strong> and <strong>Alexnet</strong>. <strong>VGG</strong> was the state of the art model back in 2014 for image classification, and is very useful nowadays because of it’s simple and straightforward architecture. When examining VGG early layers, it may be noticed that there are high activation around the item to classify. Deeper layers have even stronger activation, however they are coarse in their nature since the repetitive pooling action. With these understandings in mind, it was hypothesized that classification training can also be used with some tweaks to finding/segmenting the object.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Early results for semantic segmentation emerged along with the classification algorithms. In this <a href="http://warmspringwinds.github.io/tensorflow/tf-slim/2016/11/22/upsampling-and-image-segmentation-with-tensorflow-and-tf-slim/" target="_self">post</a>, you can see some rough segmentation results that come from using the <strong>VGG</strong>:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:44%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 341px" srcset="https://miro.medium.com/v2/resize:fit:640/0*muU9JtcPG8ahHY3a. 640w, https://miro.medium.com/v2/resize:fit:720/0*muU9JtcPG8ahHY3a. 720w, https://miro.medium.com/v2/resize:fit:750/0*muU9JtcPG8ahHY3a. 750w, https://miro.medium.com/v2/resize:fit:786/0*muU9JtcPG8ahHY3a. 786w, https://miro.medium.com/v2/resize:fit:828/0*muU9JtcPG8ahHY3a. 828w, https://miro.medium.com/v2/resize:fit:1100/0*muU9JtcPG8ahHY3a. 1100w, https://miro.medium.com/v2/resize:fit:682/0*muU9JtcPG8ahHY3a. 682w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 341px" srcset="https://miro.medium.com/v2/resize:fit:640/0*muU9JtcPG8ahHY3a. 640w, https://miro.medium.com/v2/resize:fit:720/0*muU9JtcPG8ahHY3a. 720w, https://miro.medium.com/v2/resize:fit:750/0*muU9JtcPG8ahHY3a. 750w, https://miro.medium.com/v2/resize:fit:786/0*muU9JtcPG8ahHY3a. 786w, https://miro.medium.com/v2/resize:fit:828/0*muU9JtcPG8ahHY3a. 828w, https://miro.medium.com/v2/resize:fit:1100/0*muU9JtcPG8ahHY3a. 1100w, https://miro.medium.com/v2/resize:fit:682/0*muU9JtcPG8ahHY3a. 682w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/341/0*muU9JtcPG8ahHY3a."></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">late layer results:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*_TMVftWCnWDmPXlS. 640w, https://miro.medium.com/v2/resize:fit:720/0*_TMVftWCnWDmPXlS. 720w, https://miro.medium.com/v2/resize:fit:750/0*_TMVftWCnWDmPXlS. 750w, https://miro.medium.com/v2/resize:fit:786/0*_TMVftWCnWDmPXlS. 786w, https://miro.medium.com/v2/resize:fit:828/0*_TMVftWCnWDmPXlS. 828w, https://miro.medium.com/v2/resize:fit:1100/0*_TMVftWCnWDmPXlS. 1100w, https://miro.medium.com/v2/resize:fit:1400/0*_TMVftWCnWDmPXlS. 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*_TMVftWCnWDmPXlS. 640w, https://miro.medium.com/v2/resize:fit:720/0*_TMVftWCnWDmPXlS. 720w, https://miro.medium.com/v2/resize:fit:750/0*_TMVftWCnWDmPXlS. 750w, https://miro.medium.com/v2/resize:fit:786/0*_TMVftWCnWDmPXlS. 786w, https://miro.medium.com/v2/resize:fit:828/0*_TMVftWCnWDmPXlS. 828w, https://miro.medium.com/v2/resize:fit:1100/0*_TMVftWCnWDmPXlS. 1100w, https://miro.medium.com/v2/resize:fit:1400/0*_TMVftWCnWDmPXlS. 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*_TMVftWCnWDmPXlS."></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Segmentation of the buss image, light purple (29) is school bus class</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">after bilinear upsampling:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*ZBBaIisvp4nG1JAtJ4eIkA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*ZBBaIisvp4nG1JAtJ4eIkA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*ZBBaIisvp4nG1JAtJ4eIkA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*ZBBaIisvp4nG1JAtJ4eIkA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*ZBBaIisvp4nG1JAtJ4eIkA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ZBBaIisvp4nG1JAtJ4eIkA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZBBaIisvp4nG1JAtJ4eIkA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*ZBBaIisvp4nG1JAtJ4eIkA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*ZBBaIisvp4nG1JAtJ4eIkA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*ZBBaIisvp4nG1JAtJ4eIkA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*ZBBaIisvp4nG1JAtJ4eIkA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*ZBBaIisvp4nG1JAtJ4eIkA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*ZBBaIisvp4nG1JAtJ4eIkA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*ZBBaIisvp4nG1JAtJ4eIkA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*ZBBaIisvp4nG1JAtJ4eIkA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">These results comes from merely converting (or maintaining) the fully connected layer into it’s original shape, maintaining its spatial features, getting a fully convolutional network. In the example above, we feed a 768*1024 image into the VGG, and get a layer of 24*32*1000. the 24*32 is the pooled version of the image (by 32) and the 1000 is the image-net class count, from which we can derive the segmentation above.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To smooth the prediction, the researchers used a naive bilienar up-sampling layer.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In the <a href="https://arxiv.org/abs/1411.4038" target="_self">FCN</a> paper, the researchers improved the idea above. They connected some layers along the way, to allow a richer interpretations, which were named FCN-32, FCN-16 and FCN-8, according the up-sampling rate:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*AyBHxDfXDVVkbena. 640w, https://miro.medium.com/v2/resize:fit:720/0*AyBHxDfXDVVkbena. 720w, https://miro.medium.com/v2/resize:fit:750/0*AyBHxDfXDVVkbena. 750w, https://miro.medium.com/v2/resize:fit:786/0*AyBHxDfXDVVkbena. 786w, https://miro.medium.com/v2/resize:fit:828/0*AyBHxDfXDVVkbena. 828w, https://miro.medium.com/v2/resize:fit:1100/0*AyBHxDfXDVVkbena. 1100w, https://miro.medium.com/v2/resize:fit:1400/0*AyBHxDfXDVVkbena. 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*AyBHxDfXDVVkbena. 640w, https://miro.medium.com/v2/resize:fit:720/0*AyBHxDfXDVVkbena. 720w, https://miro.medium.com/v2/resize:fit:750/0*AyBHxDfXDVVkbena. 750w, https://miro.medium.com/v2/resize:fit:786/0*AyBHxDfXDVVkbena. 786w, https://miro.medium.com/v2/resize:fit:828/0*AyBHxDfXDVVkbena. 828w, https://miro.medium.com/v2/resize:fit:1100/0*AyBHxDfXDVVkbena. 1100w, https://miro.medium.com/v2/resize:fit:1400/0*AyBHxDfXDVVkbena. 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*AyBHxDfXDVVkbena."></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Adding some skip connections between the layers allowed the prediction to encode finer details from the original image. Further training improved the results even more.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">this technique showed itself as not so bad as might have been thought, and proved there is indeed potential in semantic segmentation with deep learning.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*CGgz2q_jGpBnbcig. 640w, https://miro.medium.com/v2/resize:fit:720/0*CGgz2q_jGpBnbcig. 720w, https://miro.medium.com/v2/resize:fit:750/0*CGgz2q_jGpBnbcig. 750w, https://miro.medium.com/v2/resize:fit:786/0*CGgz2q_jGpBnbcig. 786w, https://miro.medium.com/v2/resize:fit:828/0*CGgz2q_jGpBnbcig. 828w, https://miro.medium.com/v2/resize:fit:1100/0*CGgz2q_jGpBnbcig. 1100w, https://miro.medium.com/v2/resize:fit:1400/0*CGgz2q_jGpBnbcig. 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*CGgz2q_jGpBnbcig. 640w, https://miro.medium.com/v2/resize:fit:720/0*CGgz2q_jGpBnbcig. 720w, https://miro.medium.com/v2/resize:fit:750/0*CGgz2q_jGpBnbcig. 750w, https://miro.medium.com/v2/resize:fit:786/0*CGgz2q_jGpBnbcig. 786w, https://miro.medium.com/v2/resize:fit:828/0*CGgz2q_jGpBnbcig. 828w, https://miro.medium.com/v2/resize:fit:1100/0*CGgz2q_jGpBnbcig. 1100w, https://miro.medium.com/v2/resize:fit:1400/0*CGgz2q_jGpBnbcig. 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*CGgz2q_jGpBnbcig."></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">FCN results from the paper</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The FCN unlocked the concept of segmentation, and researchers tried different architectures for this task. <mark>The main idea stays similar: using known architectures, up-sampling, and using skip connections are still prominent at the newer models.</mark></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">You can read about advances in this field in a few good posts: <a href="http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review" target="_self">here</a>, <a href="https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4" target="_self">here</a> and <a href="https://meetshah1995.github.io/semantic-segmentation/deep-learning/pytorch/visdom/2017/06/01/semantic-segmentation-over-the-years.html" target="_self">here</a>. You can also see that most architectures keep the encoder- decoder architecture.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Back to our project</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After doing some research, we settled on three models, which were available to us: the FCN, <a href="https://arxiv.org/pdf/1505.04597.pdf" target="_self">Unet</a> and <a href="https://arxiv.org/abs/1611.09326" target="_self">Tiramisu</a> — very deep encoder-decoder architecture. We also had some thoughts about the mask-RCNN, but implementing it seemed outside of our projects scope.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>FCN</strong> didn’t seem relevant since its results weren’t as good as we would have like (even as a starting point), but the 2 other models we’ve mentioned showed results that were not bad: the tiramisu on the <a href="http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/" target="_self">CamVid</a> dataset, and the <a href="https://arxiv.org/abs/1505.04597" target="_self">Unet</a> main advantage was it’s compactness and speed. In terms of implementations, the Unet is quite straightforward to implement (we used keras) and the <strong>Tiramisu</strong> was also implementable. To get us started, we’ve used a good <a href="http://files.fast.ai/part2/lesson14/" target="_self">implementation</a> of Tiramisu at the last lesson of <em>Jeremy Howard’s</em> great deep learning <a href="https://www.usfca.edu/data-institute/certificates/deep-learning-part-two" target="_self">course</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">With these two models, we went ahead and started training on some data-sets. I must say that after we first tried the <strong>Tiramisu</strong>, we saw that its results had much more potential for us, since it had the ability to capture sharp edges in an image. from the other hand, the unet seemed not fine enough, and the results seemed a bit blobbish.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*cyPpwf_y-81QgUEntLbLLg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*cyPpwf_y-81QgUEntLbLLg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*cyPpwf_y-81QgUEntLbLLg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*cyPpwf_y-81QgUEntLbLLg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*cyPpwf_y-81QgUEntLbLLg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*cyPpwf_y-81QgUEntLbLLg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cyPpwf_y-81QgUEntLbLLg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*cyPpwf_y-81QgUEntLbLLg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*cyPpwf_y-81QgUEntLbLLg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*cyPpwf_y-81QgUEntLbLLg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*cyPpwf_y-81QgUEntLbLLg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*cyPpwf_y-81QgUEntLbLLg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*cyPpwf_y-81QgUEntLbLLg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*cyPpwf_y-81QgUEntLbLLg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*cyPpwf_y-81QgUEntLbLLg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Unet blobbiness</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">The data</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After having our general direction set with the model, we started looking for proper datasets. Segmentation data is not as common as classification or even detection. Additionally, manual tagging is not really a possibility. The most common datasets for segmentation were the <a href="http://mscoco.org/" target="_self">COCO</a> dataset, which includes around 80K images with 90 categories, the <a href="http://host.robots.ox.ac.uk/pascal/VOC/" target="_self">VOC pascal</a> dataset with 11K images and 20 classes, and the newer <a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/" target="_self">ADE20K</a> datasets.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We chose to work with the COCO dataset, since it includes much more images with the class “<em>person</em>” which was our class of interest.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Considering our task, we pondered if we’ll use only the images that are super relevant for us, or use more general dataset. On one hand, using a more general dataset with more images and classes will allow the model to deal with more scenerios and challenges. On the other hand, on overnight training session allowed us going over ~150K images. If we’ll introduce the model with the entire COCO dataset, we will end with the model seeing each image twice (on average) therefore trimming it a little bit will be beneficial. additionally, it will results in a more focused model for our purposes.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">One more thing that is worth mentioning — the <strong>Tiramisu</strong> model was originally trained on the <a href="http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/" target="_self">CamVid</a> dataset, which has some flaws, but most importantly it’s images are very monotonous: all images are road pics from a car. As you can easily understand, learning from such dataset (even though it contains people) had no benefit for our task, so after a short trial, we moved ahead.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*OwksnSnYpEukpOx5stja3w.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*OwksnSnYpEukpOx5stja3w.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*OwksnSnYpEukpOx5stja3w.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*OwksnSnYpEukpOx5stja3w.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*OwksnSnYpEukpOx5stja3w.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*OwksnSnYpEukpOx5stja3w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OwksnSnYpEukpOx5stja3w.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*OwksnSnYpEukpOx5stja3w.png 640w, https://miro.medium.com/v2/resize:fit:720/1*OwksnSnYpEukpOx5stja3w.png 720w, https://miro.medium.com/v2/resize:fit:750/1*OwksnSnYpEukpOx5stja3w.png 750w, https://miro.medium.com/v2/resize:fit:786/1*OwksnSnYpEukpOx5stja3w.png 786w, https://miro.medium.com/v2/resize:fit:828/1*OwksnSnYpEukpOx5stja3w.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*OwksnSnYpEukpOx5stja3w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*OwksnSnYpEukpOx5stja3w.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*OwksnSnYpEukpOx5stja3w.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Images from CamVid dataset</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The COCO dataset ships with pretty straight-forward <a href="https://github.com/pdollar/coco" target="_self">API</a> which allowed us to know exactly what objects are at each image (according th 90 predefined classes)</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After some experimenting, we’ve decided to dilute the dataset: first we filtered only the images with a person in them, leaving us with 40K images. Then, we dropped all the images with many people in them, and left with only 1 or 2, since this is what our product should find. Finally, we left only the images where 20%-70% of the image are tagged as the person, removing the images with a very small person in the background, or some kind of weird monstrosity (unfortunately not all of them). Our final dataset consisted of 11K images, which we felt was enough at this stage.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*QJiLnyjNtyS9ujrMFCnUGA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*QJiLnyjNtyS9ujrMFCnUGA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*QJiLnyjNtyS9ujrMFCnUGA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*QJiLnyjNtyS9ujrMFCnUGA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*QJiLnyjNtyS9ujrMFCnUGA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*QJiLnyjNtyS9ujrMFCnUGA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QJiLnyjNtyS9ujrMFCnUGA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*QJiLnyjNtyS9ujrMFCnUGA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*QJiLnyjNtyS9ujrMFCnUGA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*QJiLnyjNtyS9ujrMFCnUGA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*QJiLnyjNtyS9ujrMFCnUGA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*QJiLnyjNtyS9ujrMFCnUGA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*QJiLnyjNtyS9ujrMFCnUGA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*QJiLnyjNtyS9ujrMFCnUGA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*QJiLnyjNtyS9ujrMFCnUGA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Left: good image ___ Center: too many characters ___ Right: Objective is too small</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">The Tiramisu model</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">As said, we were introduced with the <a href="https://arxiv.org/abs/1611.09326" target="_self">Tiramisu</a> model in Jeremy Howard’s course. Though it’s full name “100 layers Tiramisu” implies a gigantic model, it is actually quite economical, with only 9M parameters. The VGG16 for comparison, has more than 130M parameters.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The <strong>Tiramisu</strong> model was based on the <strong>DensNet, </strong>a recent image classification model where all layers are interconnected. Moreover, Tiramisu adds skip connections to the up-sampling layers, like the Unet.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you recall, this architecture is congruent with the idea presented in FCN: using classification architecture, up-sampling, and adding skip connections for refinement.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:45%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 347px" srcset="https://miro.medium.com/v2/resize:fit:640/0*8y3DsK9cGoW9tpne. 640w, https://miro.medium.com/v2/resize:fit:720/0*8y3DsK9cGoW9tpne. 720w, https://miro.medium.com/v2/resize:fit:750/0*8y3DsK9cGoW9tpne. 750w, https://miro.medium.com/v2/resize:fit:786/0*8y3DsK9cGoW9tpne. 786w, https://miro.medium.com/v2/resize:fit:828/0*8y3DsK9cGoW9tpne. 828w, https://miro.medium.com/v2/resize:fit:1100/0*8y3DsK9cGoW9tpne. 1100w, https://miro.medium.com/v2/resize:fit:694/0*8y3DsK9cGoW9tpne. 694w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 347px" srcset="https://miro.medium.com/v2/resize:fit:640/0*8y3DsK9cGoW9tpne. 640w, https://miro.medium.com/v2/resize:fit:720/0*8y3DsK9cGoW9tpne. 720w, https://miro.medium.com/v2/resize:fit:750/0*8y3DsK9cGoW9tpne. 750w, https://miro.medium.com/v2/resize:fit:786/0*8y3DsK9cGoW9tpne. 786w, https://miro.medium.com/v2/resize:fit:828/0*8y3DsK9cGoW9tpne. 828w, https://miro.medium.com/v2/resize:fit:1100/0*8y3DsK9cGoW9tpne. 1100w, https://miro.medium.com/v2/resize:fit:694/0*8y3DsK9cGoW9tpne. 694w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/347/0*8y3DsK9cGoW9tpne."></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Tiramisu Architecture in general</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The <a href="https://arxiv.org/pdf/1608.06993.pdf" target="_self">DenseNet</a> model can be seen as a natural evolution of the <strong>Resnet</strong> model, but instead of “remembering” every layer only until the next layer, the <strong>DenseNet</strong> remembers all layers throughout the model. These connections are called highway connections. It causes an inflation of the filter numbers, which is defined as the “growth rate”. The Tiramisu has growth rate of 16, therefore with each layer we add 16 new filters until we reach layers of 1072 filters. You might expect 1600 layers because it’s 100 layer tiramisu, however, the up-sampling layers drop some filters.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:44%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 340px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*-Gd_J_qqorbl1PPyAw9KQQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*-Gd_J_qqorbl1PPyAw9KQQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*-Gd_J_qqorbl1PPyAw9KQQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*-Gd_J_qqorbl1PPyAw9KQQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*-Gd_J_qqorbl1PPyAw9KQQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*-Gd_J_qqorbl1PPyAw9KQQ.png 1100w, https://miro.medium.com/v2/resize:fit:680/format:webp/1*-Gd_J_qqorbl1PPyAw9KQQ.png 680w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 340px" srcset="https://miro.medium.com/v2/resize:fit:640/1*-Gd_J_qqorbl1PPyAw9KQQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*-Gd_J_qqorbl1PPyAw9KQQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*-Gd_J_qqorbl1PPyAw9KQQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*-Gd_J_qqorbl1PPyAw9KQQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*-Gd_J_qqorbl1PPyAw9KQQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*-Gd_J_qqorbl1PPyAw9KQQ.png 1100w, https://miro.medium.com/v2/resize:fit:680/1*-Gd_J_qqorbl1PPyAw9KQQ.png 680w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/340/1*-Gd_J_qqorbl1PPyAw9KQQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Densenet model sketch — early filters are stacked throughout the model</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Training</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We trained our model with schedule as described in the original paper: standard cross entropy loss, RMSProp optimizer with 1e-3 learning rate and small decay. We split our 11K images into 70% training, 20% validation, 10% test. All images below are taken from our test set.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To keep our training schedule aligned with the original paper, we set the epoch size on 500 images. This also allowed us to save the model periodically with every improvement in results, since we trained it on much more data (the CamVid dataset which was used in the article contains less than 1K images)</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Additionally, we trained it on only 2 classes: background and <em>person</em>, while the paper had 12 classes. We first tried to train on some of coco’s classes, however we saw that this doesn’t add to much to our training.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Data Issues</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">some dataset flaws hindered our score:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><strong>Animals</strong> — Our model sometimes segmented animals. this of course leads to a low IOU. adding animals to our task in the same main class or as anther, would probably removed our results</li><li class="ff3" style="font-size:22px;"><strong>Body parts</strong> — since we filtered our dataset programatically, we had no way to tell if the person class is actually a person or some body part like hand or foot. these images were not in our scope, but still emerged here and there.</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:34%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 270px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*NfXdLh8c0nEbJcwv9zkh8w.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*NfXdLh8c0nEbJcwv9zkh8w.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*NfXdLh8c0nEbJcwv9zkh8w.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*NfXdLh8c0nEbJcwv9zkh8w.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*NfXdLh8c0nEbJcwv9zkh8w.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*NfXdLh8c0nEbJcwv9zkh8w.png 1100w, https://miro.medium.com/v2/resize:fit:540/format:webp/1*NfXdLh8c0nEbJcwv9zkh8w.png 540w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 270px" srcset="https://miro.medium.com/v2/resize:fit:640/1*NfXdLh8c0nEbJcwv9zkh8w.png 640w, https://miro.medium.com/v2/resize:fit:720/1*NfXdLh8c0nEbJcwv9zkh8w.png 720w, https://miro.medium.com/v2/resize:fit:750/1*NfXdLh8c0nEbJcwv9zkh8w.png 750w, https://miro.medium.com/v2/resize:fit:786/1*NfXdLh8c0nEbJcwv9zkh8w.png 786w, https://miro.medium.com/v2/resize:fit:828/1*NfXdLh8c0nEbJcwv9zkh8w.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*NfXdLh8c0nEbJcwv9zkh8w.png 1100w, https://miro.medium.com/v2/resize:fit:540/1*NfXdLh8c0nEbJcwv9zkh8w.png 540w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/270/1*NfXdLh8c0nEbJcwv9zkh8w.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Animal, Body part, hand held object</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><strong>Handheld Objects -</strong> many Images in the dataset are sports related. Baseball bats, tennis rackets and snowboards where everywhere. Our model was somehow confused how should it segment them. As in the animal case, adding them as part of the main class or as separate class would help the performance of the model in our opinion.</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:31%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 250px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*OwgNMXBGYgmYcgX1gaz-oA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*OwgNMXBGYgmYcgX1gaz-oA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*OwgNMXBGYgmYcgX1gaz-oA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*OwgNMXBGYgmYcgX1gaz-oA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*OwgNMXBGYgmYcgX1gaz-oA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*OwgNMXBGYgmYcgX1gaz-oA.png 1100w, https://miro.medium.com/v2/resize:fit:500/format:webp/1*OwgNMXBGYgmYcgX1gaz-oA.png 500w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 250px" srcset="https://miro.medium.com/v2/resize:fit:640/1*OwgNMXBGYgmYcgX1gaz-oA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*OwgNMXBGYgmYcgX1gaz-oA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*OwgNMXBGYgmYcgX1gaz-oA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*OwgNMXBGYgmYcgX1gaz-oA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*OwgNMXBGYgmYcgX1gaz-oA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*OwgNMXBGYgmYcgX1gaz-oA.png 1100w, https://miro.medium.com/v2/resize:fit:500/1*OwgNMXBGYgmYcgX1gaz-oA.png 500w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/250/1*OwgNMXBGYgmYcgX1gaz-oA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Sporting image with an object</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><strong>Coarse ground truth</strong> — the coco dataset was not annotated pixel by pixel, but with polygons. Sometimes it’s good enough, but other times the ground truth is very coarse, which possible hinders the model from learning subtleties</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:84%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 619px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*P4wf6d8oi-zcIQJFG9464Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*P4wf6d8oi-zcIQJFG9464Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*P4wf6d8oi-zcIQJFG9464Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*P4wf6d8oi-zcIQJFG9464Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*P4wf6d8oi-zcIQJFG9464Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*P4wf6d8oi-zcIQJFG9464Q.png 1100w, https://miro.medium.com/v2/resize:fit:1238/format:webp/1*P4wf6d8oi-zcIQJFG9464Q.png 1238w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 619px" srcset="https://miro.medium.com/v2/resize:fit:640/1*P4wf6d8oi-zcIQJFG9464Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*P4wf6d8oi-zcIQJFG9464Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*P4wf6d8oi-zcIQJFG9464Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*P4wf6d8oi-zcIQJFG9464Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*P4wf6d8oi-zcIQJFG9464Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*P4wf6d8oi-zcIQJFG9464Q.png 1100w, https://miro.medium.com/v2/resize:fit:1238/1*P4wf6d8oi-zcIQJFG9464Q.png 1238w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/619/1*P4wf6d8oi-zcIQJFG9464Q.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Image and (very) Coarse ground truth</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Results</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Our results were satisfying, tough not perfect: we have reached IoU of 84.6 on our test set, while current state of the art is 85. This number is tricky though: it fluctuates throughout different datasets and classes. there are classes which are inherently easier to segment e.g houses, roads, where most models easily reach results of 90 IoU. Other more challenging classes are trees and humans, on which most models reach results of around 60 IoU. To gauge this difficulty, we helped our network focus on a single class, and limited type of photos.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We still not feel our work is “production ready” as we would want it to be, but we think it’s a good time to stop and discuss our results, since around 50% of the photos will give good results.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here are some good examples to give you a feel of the app capabilities:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*0VxtBPnfXUHDoTA5I_1UDw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*0VxtBPnfXUHDoTA5I_1UDw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*0VxtBPnfXUHDoTA5I_1UDw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*0VxtBPnfXUHDoTA5I_1UDw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*0VxtBPnfXUHDoTA5I_1UDw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*0VxtBPnfXUHDoTA5I_1UDw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0VxtBPnfXUHDoTA5I_1UDw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*0VxtBPnfXUHDoTA5I_1UDw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*0VxtBPnfXUHDoTA5I_1UDw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*0VxtBPnfXUHDoTA5I_1UDw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*0VxtBPnfXUHDoTA5I_1UDw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*0VxtBPnfXUHDoTA5I_1UDw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*0VxtBPnfXUHDoTA5I_1UDw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*0VxtBPnfXUHDoTA5I_1UDw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*0VxtBPnfXUHDoTA5I_1UDw.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*FCxtsyd-_amEi13NLZaxTg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*FCxtsyd-_amEi13NLZaxTg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*FCxtsyd-_amEi13NLZaxTg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*FCxtsyd-_amEi13NLZaxTg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*FCxtsyd-_amEi13NLZaxTg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*FCxtsyd-_amEi13NLZaxTg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FCxtsyd-_amEi13NLZaxTg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*FCxtsyd-_amEi13NLZaxTg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*FCxtsyd-_amEi13NLZaxTg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*FCxtsyd-_amEi13NLZaxTg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*FCxtsyd-_amEi13NLZaxTg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*FCxtsyd-_amEi13NLZaxTg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*FCxtsyd-_amEi13NLZaxTg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*FCxtsyd-_amEi13NLZaxTg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*FCxtsyd-_amEi13NLZaxTg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*-k6qGz9GfiW08Ipr8r9pnA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*-k6qGz9GfiW08Ipr8r9pnA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*-k6qGz9GfiW08Ipr8r9pnA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*-k6qGz9GfiW08Ipr8r9pnA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*-k6qGz9GfiW08Ipr8r9pnA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*-k6qGz9GfiW08Ipr8r9pnA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-k6qGz9GfiW08Ipr8r9pnA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*-k6qGz9GfiW08Ipr8r9pnA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*-k6qGz9GfiW08Ipr8r9pnA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*-k6qGz9GfiW08Ipr8r9pnA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*-k6qGz9GfiW08Ipr8r9pnA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*-k6qGz9GfiW08Ipr8r9pnA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*-k6qGz9GfiW08Ipr8r9pnA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*-k6qGz9GfiW08Ipr8r9pnA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*-k6qGz9GfiW08Ipr8r9pnA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Image, Ground truth, our result (from our test set)</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Debugging and logging</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">A very important part of training neural networks is the debugging. When starting our work, it was very tempting to get right to it, grab the data and the network, start the training, and see what comes out. However, we found out that it extremely important to track every move, and making tools for ourselves for being able to examine results at each step.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here are the common challenges, and what we did about them:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;"><strong>Early problems </strong>— The model might not be training. It may be because some inherent problem, or because of some kind of pre-processing error, like forgetting to normalize some chunk of the data. Anyhow, simple visualization of results may be very helpful. Here is a good <a href="https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607" target="_self">post</a> about this subject.</li><li class="ff3" style="font-size:22px;"><strong>Debugging the network itself</strong> — after making sure there are no crucial issues, the training starts, with the predefined loss and metrics. In segmentation, the main measure is the <a href="http://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/" target="_self">IoU</a> — intersect over union. It took us a few sessions to start using the IoU as a main measure for our models (and not the cross entropy loss). Another helpful practice was showing some predictions of our model at every epoch. Here is a good <a href="https://hackernoon.com/how-to-debug-neural-networks-manual-dc2a200f10f2" target="_self">post</a> about debugging machine learning models. Take note that IoU is not a standard metric/loss in keras, but you can easily find it online, e.g <a href="https://github.com/udacity/self-driving-car/blob/master/vehicle-detection/u-net/README.md" target="_self">here</a>. We also used this <a href="https://gist.github.com/shgidi/7c3fcf69b1eb3eaf4bac1112d10967ca" target="_self">gist</a> for plotting the loss and some predictions at every epoch.</li><li class="ff3" style="font-size:22px;"><strong>Machine learning version control</strong> — when training a model, there are many parameters, some of them are tricky to follow. I must say we still haven't found the perfect method, except from fervently writing up our configurations (and auto-saving best models with keras callback, see below).</li><li class="ff3" style="font-size:22px;"><strong>Debugging tool</strong> — after doing all the above got us into a point the we can examine our work at every step, but not seamlessly. therefore, the most important step was combining the steps above together, and creating an Jupyter notebook which allowed us to seamlessly load every model and every image, and quickly examine it’s results. This way we could easily see differences between models, pitfalls and other issues.</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here are and example of the improvement of our model, throughout tweaking of parameters and extra training:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:61%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 461px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*V_LDY3_yXxXL-WRv5h9JiA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*V_LDY3_yXxXL-WRv5h9JiA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*V_LDY3_yXxXL-WRv5h9JiA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*V_LDY3_yXxXL-WRv5h9JiA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*V_LDY3_yXxXL-WRv5h9JiA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*V_LDY3_yXxXL-WRv5h9JiA.png 1100w, https://miro.medium.com/v2/resize:fit:922/format:webp/1*V_LDY3_yXxXL-WRv5h9JiA.png 922w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 461px" srcset="https://miro.medium.com/v2/resize:fit:640/1*V_LDY3_yXxXL-WRv5h9JiA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*V_LDY3_yXxXL-WRv5h9JiA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*V_LDY3_yXxXL-WRv5h9JiA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*V_LDY3_yXxXL-WRv5h9JiA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*V_LDY3_yXxXL-WRv5h9JiA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*V_LDY3_yXxXL-WRv5h9JiA.png 1100w, https://miro.medium.com/v2/resize:fit:922/1*V_LDY3_yXxXL-WRv5h9JiA.png 922w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/461/1*V_LDY3_yXxXL-WRv5h9JiA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">for saving model with best validation IoU until now: (Keras Provides a very nice <a href="https://keras.io/callbacks/" target="_self">callbacks</a> to make these things easier)</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>callbacks = [keras.callbacks.ModelCheckpoint(hist_model, verbose=1,save_best_only =True, monitor= ’val_IOU_calc_loss’), plot_losses]</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In addition to the normal debugging of possible code errors, we’ve noticed that model errors are “predictable”, like “cutting” body parts that seem out of the general body counter, “bites” on large segments, unnecessarily continuing extending body parts, poor lighting, poor quality, and many details. Some of this caveats were treated in adding specific images from different datasets, but others are still remain challenges to be dealt with. To improve results for the next version, we will use augmentation specifically on “hard” images for out model.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We already mentioned this issue above, with the data set issues. Now lets see some of our model difficulties:es:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;">Cloths — very dark or very light clothing tends sometimes to be interpreted as background</li><li class="ff3" style="font-size:22px;">“Bites” — otherwise good results, had some bites in them</li></ol></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*2UtlGC0NEcC0FRDpkIOEww.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*2UtlGC0NEcC0FRDpkIOEww.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*2UtlGC0NEcC0FRDpkIOEww.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*2UtlGC0NEcC0FRDpkIOEww.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*2UtlGC0NEcC0FRDpkIOEww.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*2UtlGC0NEcC0FRDpkIOEww.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2UtlGC0NEcC0FRDpkIOEww.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*2UtlGC0NEcC0FRDpkIOEww.png 640w, https://miro.medium.com/v2/resize:fit:720/1*2UtlGC0NEcC0FRDpkIOEww.png 720w, https://miro.medium.com/v2/resize:fit:750/1*2UtlGC0NEcC0FRDpkIOEww.png 750w, https://miro.medium.com/v2/resize:fit:786/1*2UtlGC0NEcC0FRDpkIOEww.png 786w, https://miro.medium.com/v2/resize:fit:828/1*2UtlGC0NEcC0FRDpkIOEww.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*2UtlGC0NEcC0FRDpkIOEww.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*2UtlGC0NEcC0FRDpkIOEww.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*2UtlGC0NEcC0FRDpkIOEww.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Clothing and bite</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">3. lighting -poor lightning and obscurity is common in images, however not in COCO dataset. therefore, apart from the standard difficulty of models to deals with these things, ours haven’t been even prepared fro harder images. This can be improved with getting more data, and additional, with data augmentation. meanwhile, it is better not to try our app at night :)</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*w-zn5F-ZK6CRfuW9vfN-gQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*w-zn5F-ZK6CRfuW9vfN-gQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*w-zn5F-ZK6CRfuW9vfN-gQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*w-zn5F-ZK6CRfuW9vfN-gQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*w-zn5F-ZK6CRfuW9vfN-gQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*w-zn5F-ZK6CRfuW9vfN-gQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w-zn5F-ZK6CRfuW9vfN-gQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*w-zn5F-ZK6CRfuW9vfN-gQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*w-zn5F-ZK6CRfuW9vfN-gQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*w-zn5F-ZK6CRfuW9vfN-gQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*w-zn5F-ZK6CRfuW9vfN-gQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*w-zn5F-ZK6CRfuW9vfN-gQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*w-zn5F-ZK6CRfuW9vfN-gQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*w-zn5F-ZK6CRfuW9vfN-gQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*w-zn5F-ZK6CRfuW9vfN-gQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">poor lighting example</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Further progress options</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><strong>Further training</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Our production results come after training ~300 epochs over our training data. After this period, the model started over-fitting. We’ve reached these results very close to the release, therefore we haven't had the chance to apply the basic practice of data augmentation.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We’ve trained the model after resizing the images to 224X224. Further training with more data and larger images (original size of COCO images is around 600X1000) would also expected to improve results.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">CRF and other enhancements</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">At some stages, we saw that our results are a bit noisy at the edges. A model that may refine this is the CRF. In this <a href="http://warmspringwinds.github.io/tensorflow/tf-slim/2016/12/18/image-segmentation-with-tensorflow-using-cnns-and-conditional-random-fields/" target="_self">blogpost</a>, the author shows slightly naive example for using CRF.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">However, it wasn’t very useful for our work, perhaps since it generally helps when results are coarser.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><strong>Matting</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Even with our current results, the segmentation is not perfect. Hair, delicate clothes, tree branches and other fine objects will never be segmented perfectly, even because the ground truth segmentation does not contain these subtleties. The task of separating such delicate segmentation is called matting, and defines a different challenge. Here is an example of state of the art matting, <a href="https://news.developer.nvidia.com/ai-software-automatically-removes-the-background-from-images/" target="_self">published</a> earlier this year in NVIDIA conference.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:85%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 625px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*9AVzzGTj6F2k5vRuuGSGEw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*9AVzzGTj6F2k5vRuuGSGEw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*9AVzzGTj6F2k5vRuuGSGEw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*9AVzzGTj6F2k5vRuuGSGEw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*9AVzzGTj6F2k5vRuuGSGEw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*9AVzzGTj6F2k5vRuuGSGEw.png 1100w, https://miro.medium.com/v2/resize:fit:1250/format:webp/1*9AVzzGTj6F2k5vRuuGSGEw.png 1250w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 625px" srcset="https://miro.medium.com/v2/resize:fit:640/1*9AVzzGTj6F2k5vRuuGSGEw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*9AVzzGTj6F2k5vRuuGSGEw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*9AVzzGTj6F2k5vRuuGSGEw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*9AVzzGTj6F2k5vRuuGSGEw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*9AVzzGTj6F2k5vRuuGSGEw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*9AVzzGTj6F2k5vRuuGSGEw.png 1100w, https://miro.medium.com/v2/resize:fit:1250/1*9AVzzGTj6F2k5vRuuGSGEw.png 1250w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/625/1*9AVzzGTj6F2k5vRuuGSGEw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Matting example — the input includes the trimap as well</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The matting task is different from other image related tasks, since it’s input includes not only an image, but also a <strong>trimap — </strong>an outline of the edges of the images, what makes it a “semi supervised” problem.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We experimented with matting a little bit, using our segmentation as the trimap, however we did not reach significant results.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">one more issue was the lack of a proper dataset to train on.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Summary</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">As said in the beginning, our goal was to build a significant deep learning product. As you can see in <a href="https://medium.com/@burgalon" target="_self">Alon</a>’s posts, deployment becomes easier and faster all the time. Training a model on the other hand, is tricky — training, especially when done overnight, requires careful planning, debugging, and recording of results.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">It is also not easy to balance between research and trying new things, and the mundane training and improving. Since we use deep learning, we always have the feeling that the best model, or the exact model we need, is just around the corner, and another google search or article will lead us to it. But in practice, our actual improvements came from simply “squeezing” more and more from our original model. And as said above, we still feel there is much more to squeeze out of it.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To conclude, we had a lot of fun doing this work, which a few months ago seemed to us like science fiction. we’ll be glad to discuss and answer any questions, and looking forward to see you on our website :)</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4">Enjoyed the article? Want learn more? visit <a href="https://www.shibumi-ai.com/" target="_self">www.shibumi-ai.com</a></p></div></div></section><?php include_once 'Elemental/footer.php'; ?>