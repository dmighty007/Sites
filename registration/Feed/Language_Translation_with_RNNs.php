<!DOCTYPE html>
                <html>
                <head>
                    <title>Language Translation with RNNs</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://towardsdatascience.com/language-translation-with-rnns-d84d43b40571"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://medium.com/@thomastracey?source=post_page-----d84d43b40571--------------------------------">Author : Thomas Tracey</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>Language Translation with RNNs</h3></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5 text-muted">Build a recurrent neural network that translates English to French</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>Aug 29, 2018 by </em><a href="https://ttracey.com/" target="_self">Thomas Tracey</a><em>, originally </em><a href="https://github.com/tommytracey/AIND-Capstone/blob/master/README.md" target="_self">posted on Github</a></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><a href="https://www.ibidem-translations.com/edu/rnn-machine-translation/" target="_self">Versión en español </a><em>— A Spanish translation of this post is available </em><a href="https://www.ibidem-translations.com/edu/rnn-machine-translation/" target="_self">here</a><em>, courtesy of the </em><a href="https://www.ibidem-translations.com/spanish.php" target="_self">Ibidem Group</a><em>. Muchísimas gracias </em><a href="https://www.linkedin.com/in/josemariabescos/" target="_self">Chema Bescós</a><em>!</em></p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*KGcqYRXgidCzctW_.jpg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*KGcqYRXgidCzctW_.jpg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*KGcqYRXgidCzctW_.jpg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*KGcqYRXgidCzctW_.jpg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*KGcqYRXgidCzctW_.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*KGcqYRXgidCzctW_.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KGcqYRXgidCzctW_.jpg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*KGcqYRXgidCzctW_.jpg 640w, https://miro.medium.com/v2/resize:fit:720/0*KGcqYRXgidCzctW_.jpg 720w, https://miro.medium.com/v2/resize:fit:750/0*KGcqYRXgidCzctW_.jpg 750w, https://miro.medium.com/v2/resize:fit:786/0*KGcqYRXgidCzctW_.jpg 786w, https://miro.medium.com/v2/resize:fit:828/0*KGcqYRXgidCzctW_.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/0*KGcqYRXgidCzctW_.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/0*KGcqYRXgidCzctW_.jpg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*KGcqYRXgidCzctW_.jpg"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Image Credit: Peter Booth and Alexandra Booth / iStock</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This post explores my work on the <a href="https://github.com/tommytracey/AIND-Capstone" target="_self">final project</a> of the <a href="https://www.udacity.com/course/ai-artificial-intelligence-nanodegree--nd898" target="_self">Udacity Artificial Intelligence Nanodegree</a> program. <strong>My goal is to help other students and professionals who are in the early phases of building their intuition in machine learning (ML) and artificial intelligence (AI).</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">With that said, please keep in mind that I am a product manager by trade (not an engineer or data scientist). So, what follows is meant to be a semi-technical yet approachable explanation of the ML concepts and algorithms in this project. If anything covered below is inaccurate or if you have <em>constructive</em> feedback, I’d love to hear from you.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">My Github repo for this project can be found <a href="https://github.com/tommytracey/AIND-Capstone" target="_self">here</a>. The original Udacity source repo for this project is located <a href="https://github.com/udacity/aind2-nlp-capstone" target="_self">here</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Project Goal</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In this project, I build a deep neural network that functions as part of a machine translation pipeline. The pipeline accepts English text as input and returns the French translation. The goal is to achieve the highest translation accuracy possible.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Why Machine Translation Matters</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The ability to communicate with one another is a fundamental part of being human. There are nearly 7,000 different languages worldwide. As our world becomes increasingly connected, language translation provides a critical cultural and economic bridge between people from different countries and ethnic groups. Some of the more obvious use-cases include:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><strong>business</strong>: international trade, investment, contracts, finance</li><li class="ff3" style="font-size:22px;"><strong>commerce</strong>: travel, purchase of foreign goods and services, customer support</li><li class="ff3" style="font-size:22px;"><strong>media</strong>: accessing information via search, sharing information via social networks, localization of content and advertising</li><li class="ff3" style="font-size:22px;"><strong>education</strong>: sharing of ideas, collaboration, translation of research papers</li><li class="ff3" style="font-size:22px;"><strong>government</strong>: foreign relations, negotiation</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To meet these needs, technology companies are investing heavily in machine translation. This investment and recent advancements in deep learning have yielded major improvements in translation quality. According to Google, <a href="https://www.washingtonpost.com/news/innovations/wp/2016/10/03/google-translate-is-getting-really-really-accurate" target="_self">switching to deep learning produced a 60% increase in translation accuracy</a> compared to the phrase-based approach previously used in <a href="https://translate.google.com/" target="_self">Google Translate</a>. Today, Google and Microsoft can translate over 100 different languages and are approaching human-level accuracy for many of them.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">However, while machine translation has made lots of progress, it’s still not perfect. 😬</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*KA769S6-5theK6kA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*KA769S6-5theK6kA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*KA769S6-5theK6kA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*KA769S6-5theK6kA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*KA769S6-5theK6kA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*KA769S6-5theK6kA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KA769S6-5theK6kA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*KA769S6-5theK6kA.png 640w, https://miro.medium.com/v2/resize:fit:720/0*KA769S6-5theK6kA.png 720w, https://miro.medium.com/v2/resize:fit:750/0*KA769S6-5theK6kA.png 750w, https://miro.medium.com/v2/resize:fit:786/0*KA769S6-5theK6kA.png 786w, https://miro.medium.com/v2/resize:fit:828/0*KA769S6-5theK6kA.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*KA769S6-5theK6kA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/0*KA769S6-5theK6kA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*KA769S6-5theK6kA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Bad translation or extreme carnivorism?</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Approach for This Project</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To translate a corpus of English text to French, we need to build a recurrent neural network (RNN). Before diving into the implementation, let’s first build some intuition of RNNs and why they’re useful for NLP tasks.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">RNN Overview</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><mark>RNNs are designed to take sequences of text as inputs or return sequences of text as outputs, or both.</mark> They’re called recurrent because the network’s hidden layers have a loop in which the output and cell state from each time step become inputs at the next time step. This recurrence serves as a form of memory. It allows contextual information to flow through the network so that relevant outputs from previous time steps can be applied to network operations at the current time step.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This is analogous to how we read. As you read this post, you’re storing important pieces of information from previous words and sentences and using it as context to understand each new word and sentence.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Other types of neural networks can’t do this (yet). Imagine you’re using a convolutional neural network (CNN) to perform object detection in a movie. Currently, there’s no way for information from objects detected in previous scenes to inform the model’s detection of objects in the current scene. For example, if a courtroom and judge were detected in a previous scene, that information could help correctly classify the judge’s gavel in the current scene, instead of misclassifying it as a hammer or mallet. But CNNs don’t allow this type of time-series context to flow through the network like RNNs do.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">RNN Setup</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Depending on the use-case, you’ll want to set up your RNN to handle inputs and outputs differently. For this project, we’ll use a many-to-many process where the input is a sequence of English words and the output is a sequence of French words (fourth from the left in the diagram below).</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*sm-7smnbyLioThPQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*sm-7smnbyLioThPQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*sm-7smnbyLioThPQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*sm-7smnbyLioThPQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*sm-7smnbyLioThPQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*sm-7smnbyLioThPQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sm-7smnbyLioThPQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*sm-7smnbyLioThPQ.png 640w, https://miro.medium.com/v2/resize:fit:720/0*sm-7smnbyLioThPQ.png 720w, https://miro.medium.com/v2/resize:fit:750/0*sm-7smnbyLioThPQ.png 750w, https://miro.medium.com/v2/resize:fit:786/0*sm-7smnbyLioThPQ.png 786w, https://miro.medium.com/v2/resize:fit:828/0*sm-7smnbyLioThPQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*sm-7smnbyLioThPQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/0*sm-7smnbyLioThPQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*sm-7smnbyLioThPQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Diagram of different RNN sequence types. Credit: <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_self">Andrej Karpathy</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4">Each rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNN’s state (more on this soon).From left to right: (1) Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification). (2) Sequence output (e.g. image captioning takes an image and outputs a sentence of words). (3) Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment). (4) Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French). (5) Synced sequence input and output (e.g. video classification where we wish to label each frame of the video). Notice that in every case are no pre-specified constraints on the lengths sequences because the recurrent transformation (green) is fixed and can be applied as many times as we like.— Andrej Karpathy, <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_self">The Unreasonable Effectiveness of Recurrent Neural Networks</a></p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Building the Pipeline</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Below is a summary of the various preprocessing and modeling steps. The high-level steps include:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;"><strong>Preprocessing</strong>: load and examine data, cleaning, tokenization, padding</li><li class="ff3" style="font-size:22px;"><strong>Modeling</strong>: build, train, and test the model</li><li class="ff3" style="font-size:22px;"><strong>Prediction</strong>: generate specific translations of English to French, and compare the output translations to the ground truth translations</li><li class="ff3" style="font-size:22px;"><strong>Iteration</strong>: iterate on the model, experimenting with different architectures</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For a more detailed walkthrough including the source code, check out the <a href="https://github.com/tommytracey/AIND-Capstone/blob/master/machine_translation.ipynb" target="_self">Jupyter notebook in the project repo</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Frameworks</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We use Keras for the frontend and TensorFlow for the backend in this project. I prefer using Keras on top of TensorFlow because the syntax is simpler, which makes building the model layers more intuitive. However, there is a trade-off with Keras as you lose the ability to do fine-grained customizations. But this won’t affect the models we’re building in this project.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Preprocessing</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><strong>Load & Examine Data</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here is a sample of the data. The inputs are sentences in English; the outputs are the corresponding translations in French.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*KiND8hSnTJTUh21G.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*KiND8hSnTJTUh21G.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*KiND8hSnTJTUh21G.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*KiND8hSnTJTUh21G.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*KiND8hSnTJTUh21G.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*KiND8hSnTJTUh21G.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KiND8hSnTJTUh21G.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*KiND8hSnTJTUh21G.png 640w, https://miro.medium.com/v2/resize:fit:720/0*KiND8hSnTJTUh21G.png 720w, https://miro.medium.com/v2/resize:fit:750/0*KiND8hSnTJTUh21G.png 750w, https://miro.medium.com/v2/resize:fit:786/0*KiND8hSnTJTUh21G.png 786w, https://miro.medium.com/v2/resize:fit:828/0*KiND8hSnTJTUh21G.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*KiND8hSnTJTUh21G.png 1100w, https://miro.medium.com/v2/resize:fit:1400/0*KiND8hSnTJTUh21G.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*KiND8hSnTJTUh21G.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">When we run a word count, we can see that the vocabulary for the dataset is quite small. This was by design for this project. This allows us to train the models in a reasonable time.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*3k_iK2ZmbNsMNlMH.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*3k_iK2ZmbNsMNlMH.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*3k_iK2ZmbNsMNlMH.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*3k_iK2ZmbNsMNlMH.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*3k_iK2ZmbNsMNlMH.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*3k_iK2ZmbNsMNlMH.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3k_iK2ZmbNsMNlMH.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*3k_iK2ZmbNsMNlMH.png 640w, https://miro.medium.com/v2/resize:fit:720/0*3k_iK2ZmbNsMNlMH.png 720w, https://miro.medium.com/v2/resize:fit:750/0*3k_iK2ZmbNsMNlMH.png 750w, https://miro.medium.com/v2/resize:fit:786/0*3k_iK2ZmbNsMNlMH.png 786w, https://miro.medium.com/v2/resize:fit:828/0*3k_iK2ZmbNsMNlMH.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*3k_iK2ZmbNsMNlMH.png 1100w, https://miro.medium.com/v2/resize:fit:1400/0*3k_iK2ZmbNsMNlMH.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*3k_iK2ZmbNsMNlMH.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Cleaning</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">No additional cleaning needs to be done at this point. The data has already been converted to lowercase and split so that there are spaces between all words and punctuation.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Note</strong>: For other NLP projects you may need to perform additional steps such as: remove HTML tags, remove stop words, remove punctuation or convert to tag representations, label the parts of speech, or perform entity extraction.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Tokenization</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Next, we need to tokenize the data — i.e., convert the text to numerical values. This allows the neural network to perform operations on the input data. For this project, each word and punctuation mark will be given a unique ID. (For other NLP projects, it might make sense to assign each character a unique ID.)</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">When we run the tokenizer, it creates a word index, which is then used to convert each sentence to a vector.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*0IlrP7F4irUXJGb_.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*0IlrP7F4irUXJGb_.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*0IlrP7F4irUXJGb_.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*0IlrP7F4irUXJGb_.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*0IlrP7F4irUXJGb_.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*0IlrP7F4irUXJGb_.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0IlrP7F4irUXJGb_.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*0IlrP7F4irUXJGb_.png 640w, https://miro.medium.com/v2/resize:fit:720/0*0IlrP7F4irUXJGb_.png 720w, https://miro.medium.com/v2/resize:fit:750/0*0IlrP7F4irUXJGb_.png 750w, https://miro.medium.com/v2/resize:fit:786/0*0IlrP7F4irUXJGb_.png 786w, https://miro.medium.com/v2/resize:fit:828/0*0IlrP7F4irUXJGb_.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*0IlrP7F4irUXJGb_.png 1100w, https://miro.medium.com/v2/resize:fit:1400/0*0IlrP7F4irUXJGb_.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*0IlrP7F4irUXJGb_.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Padding</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">When we feed our sequences of word IDs into the model, each sequence needs to be the same length. To achieve this, padding is added to any sequence that is shorter than the max length (i.e. shorter than the longest sentence).</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*6jZTOE0P7_i7N8pn.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*6jZTOE0P7_i7N8pn.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*6jZTOE0P7_i7N8pn.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*6jZTOE0P7_i7N8pn.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*6jZTOE0P7_i7N8pn.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*6jZTOE0P7_i7N8pn.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6jZTOE0P7_i7N8pn.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*6jZTOE0P7_i7N8pn.png 640w, https://miro.medium.com/v2/resize:fit:720/0*6jZTOE0P7_i7N8pn.png 720w, https://miro.medium.com/v2/resize:fit:750/0*6jZTOE0P7_i7N8pn.png 750w, https://miro.medium.com/v2/resize:fit:786/0*6jZTOE0P7_i7N8pn.png 786w, https://miro.medium.com/v2/resize:fit:828/0*6jZTOE0P7_i7N8pn.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*6jZTOE0P7_i7N8pn.png 1100w, https://miro.medium.com/v2/resize:fit:1400/0*6jZTOE0P7_i7N8pn.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*6jZTOE0P7_i7N8pn.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">One-Hot Encoding (not used)</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In this project, our input sequences will be a vector containing a series of integers. Each integer represents an English word (as seen above). However, in other projects, sometimes an additional step is performed to convert each integer into a one-hot encoded vector. We don’t use one-hot encoding (OHE) in this project, but you’ll see references to it in certain diagrams (like the one below). I just didn’t want you to get confused.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*buTCMmdernlfWqs0.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*buTCMmdernlfWqs0.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*buTCMmdernlfWqs0.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*buTCMmdernlfWqs0.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*buTCMmdernlfWqs0.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*buTCMmdernlfWqs0.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*buTCMmdernlfWqs0.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*buTCMmdernlfWqs0.png 640w, https://miro.medium.com/v2/resize:fit:720/0*buTCMmdernlfWqs0.png 720w, https://miro.medium.com/v2/resize:fit:750/0*buTCMmdernlfWqs0.png 750w, https://miro.medium.com/v2/resize:fit:786/0*buTCMmdernlfWqs0.png 786w, https://miro.medium.com/v2/resize:fit:828/0*buTCMmdernlfWqs0.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*buTCMmdernlfWqs0.png 1100w, https://miro.medium.com/v2/resize:fit:1400/0*buTCMmdernlfWqs0.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*buTCMmdernlfWqs0.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">One of the advantages of OHE is efficiency since it can <a href="https://en.wikipedia.org/wiki/One-hot#cite_note-2" target="_self">run at a faster clock rate than other encodings</a>. The other advantage is that OHE better represents categorical data where there is no ordinal relationship between different values. For example, let’s say we’re classifying animals as either a mammal, reptile, fish, or bird. If we encode them as 1, 2, 3, 4 respectively, our model may assume there is a natural ordering between them, which there isn’t. It’s not useful to structure our data such that mammal comes before reptile and so forth. This can mislead our model and cause poor results. However, if we then apply one-hot encoding to these integers, changing them to binary representations — 1000, 0100, 0010, 0001 respectively — then no ordinal relationship can be inferred by the model.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">But, one of the drawbacks of OHE is that the vectors can get very long and sparse. The length of the vector is determined by the vocabulary, i.e. the number of unique words in your text corpus. As we saw in the data examination step above, our vocabulary for this project is very small — only 227 English words and 355 French words. By comparison, the <a href="https://en.oxforddictionaries.com/explore/how-many-words-are-there-in-the-english-language/" target="_self">Oxford English Dictionary has 172,000 words</a>. But, if we include various proper nouns, words tenses, and slang there could be millions of words in each language. For example, <a href="http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/" target="_self">Google’s word2vec</a> is trained on a vocabulary of 3 million unique words. If we used OHE on this vocabulary, the vector for each word would include one positive value (1) surrounded by 2,999,999 zeros!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">And, since we’re using embeddings (in the next step) to further encode the word representations, we don’t need to bother with OHE. Any efficiency gains aren’t worth it on a data set this small.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Modeling</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">First, let’s breakdown the architecture of an RNN at a high level. Referring to the diagram above, there are a few parts of the model we need to be aware of:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;"><strong>Inputs</strong>. Input sequences are fed into the model with one word for every time step. Each word is encoded as a unique integer or one-hot encoded vector that maps to the English dataset vocabulary.</li><li class="ff3" style="font-size:22px;"><strong>Embedding Layers</strong>. Embeddings are used to convert each word to a vector. The size of the vector depends on the complexity of the vocabulary.</li><li class="ff3" style="font-size:22px;"><strong>Recurrent Layers (Encoder)</strong>. This is where the context from word vectors in previous time steps is applied to the current word vector.</li><li class="ff3" style="font-size:22px;"><strong>Dense Layers (Decoder)</strong>. These are typical fully connected layers used to decode the encoded input into the correct translation sequence.</li><li class="ff3" style="font-size:22px;"><strong>Outputs</strong>. The outputs are returned as a sequence of integers or one-hot encoded vectors which can then be mapped to the French dataset vocabulary.</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Embeddings</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Embeddings allow us to capture more precise syntactic and semantic word relationships. This is achieved by projecting each word into n-dimensional space. Words with similar meanings occupy similar regions of this space; the closer two words are, the more similar they are. And often the vectors between words represent useful relationships, such as gender, verb tense, or even geopolitical relationships.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*j09oav_Svr3EpZ3n.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*j09oav_Svr3EpZ3n.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*j09oav_Svr3EpZ3n.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*j09oav_Svr3EpZ3n.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*j09oav_Svr3EpZ3n.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*j09oav_Svr3EpZ3n.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*j09oav_Svr3EpZ3n.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*j09oav_Svr3EpZ3n.png 640w, https://miro.medium.com/v2/resize:fit:720/0*j09oav_Svr3EpZ3n.png 720w, https://miro.medium.com/v2/resize:fit:750/0*j09oav_Svr3EpZ3n.png 750w, https://miro.medium.com/v2/resize:fit:786/0*j09oav_Svr3EpZ3n.png 786w, https://miro.medium.com/v2/resize:fit:828/0*j09oav_Svr3EpZ3n.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*j09oav_Svr3EpZ3n.png 1100w, https://miro.medium.com/v2/resize:fit:1400/0*j09oav_Svr3EpZ3n.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*j09oav_Svr3EpZ3n.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Photo credit: <a href="https://cbail.github.io/textasdata/word2vec/rmarkdown/word2vec.html" target="_self">Chris Bail</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Training embeddings on a large dataset from scratch requires a huge amount of data and computation. So, instead of doing it ourselves, we normally use a pre-trained embeddings package such as <a href="https://nlp.stanford.edu/projects/glove/" target="_self">GloVe</a> or <a href="https://mubaris.com/2017/12/14/word2vec/" target="_self">word2vec</a>. When used this way, embeddings are a form of <a href="https://machinelearningmastery.com/transfer-learning-for-deep-learning/" target="_self">transfer learning</a>. However, since our dataset for this project has a small vocabulary and low syntactic variation, we’ll use Keras to train the embeddings ourselves.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Encoder & Decoder</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Our sequence-to-sequence model links two recurrent networks: an encoder and decoder. The encoder summarizes the input into a context variable, also called the state. This context is then decoded and the output sequence is generated.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*vqCzgED6tPGQShQf.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*vqCzgED6tPGQShQf.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*vqCzgED6tPGQShQf.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*vqCzgED6tPGQShQf.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*vqCzgED6tPGQShQf.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*vqCzgED6tPGQShQf.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vqCzgED6tPGQShQf.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*vqCzgED6tPGQShQf.png 640w, https://miro.medium.com/v2/resize:fit:720/0*vqCzgED6tPGQShQf.png 720w, https://miro.medium.com/v2/resize:fit:750/0*vqCzgED6tPGQShQf.png 750w, https://miro.medium.com/v2/resize:fit:786/0*vqCzgED6tPGQShQf.png 786w, https://miro.medium.com/v2/resize:fit:828/0*vqCzgED6tPGQShQf.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*vqCzgED6tPGQShQf.png 1100w, https://miro.medium.com/v2/resize:fit:1400/0*vqCzgED6tPGQShQf.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*vqCzgED6tPGQShQf.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Image credit: <a href="https://classroom.udacity.com/nanodegrees/nd101/parts/4f636f4e-f9e8-4d52-931f-a49a0c26b710/modules/c1558ffb-9afd-48fa-bf12-b8f29dcb18b0/lessons/43ccf91e-7055-4833-8acc-0e2cf77696e8/concepts/be468484-4bd5-4fb0-82d6-5f5697af07da" target="_self">Udacity</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Since both the encoder and decoder are recurrent, they have loops which process each part of the sequence at different time steps. To picture this, it’s best to unroll the network so we can see what’s happening at each time step.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In the example below, it takes four timesteps to encode the entire input sequence. At each time step, the encoder “reads” the input word and performs a transformation on its hidden state. Then it passes that hidden state to the next time step. Keep in mind that the hidden state represents the relevant context flowing through the network. The bigger the hidden state, the greater the learning capacity of the model, but also the greater the computation requirements. We’ll talk more about the transformations within the hidden state when we cover gated recurrent units (GRU).</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*hZ4r1yyTEf2CqoNZ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*hZ4r1yyTEf2CqoNZ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*hZ4r1yyTEf2CqoNZ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*hZ4r1yyTEf2CqoNZ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*hZ4r1yyTEf2CqoNZ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*hZ4r1yyTEf2CqoNZ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hZ4r1yyTEf2CqoNZ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*hZ4r1yyTEf2CqoNZ.png 640w, https://miro.medium.com/v2/resize:fit:720/0*hZ4r1yyTEf2CqoNZ.png 720w, https://miro.medium.com/v2/resize:fit:750/0*hZ4r1yyTEf2CqoNZ.png 750w, https://miro.medium.com/v2/resize:fit:786/0*hZ4r1yyTEf2CqoNZ.png 786w, https://miro.medium.com/v2/resize:fit:828/0*hZ4r1yyTEf2CqoNZ.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*hZ4r1yyTEf2CqoNZ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/0*hZ4r1yyTEf2CqoNZ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*hZ4r1yyTEf2CqoNZ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Image credit: modified version from <a href="https://classroom.udacity.com/nanodegrees/nd101/parts/4f636f4e-f9e8-4d52-931f-a49a0c26b710/modules/c1558ffb-9afd-48fa-bf12-b8f29dcb18b0/lessons/43ccf91e-7055-4833-8acc-0e2cf77696e8/concepts/f999d8f6-b4c1-4cd0-811e-4767b127ae50" target="_self">Udacity</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For now, notice that for each time step after the first word in the sequence there are two inputs: the hidden state and a word from the sequence. For the encoder, it’s the <em>next</em> word in the input sequence. For the decoder, it’s the <em>previous</em> word from the output sequence.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Also, remember that when we refer to a “word,” we really mean the <em>vector representation</em> of the word which comes from the embedding layer.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here’s another way to visualize the encoder and decoder, except with a Mandarin input sequence.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/0*9KfkdXqz-ZHxe3eF.gif 640w, https://miro.medium.com/v2/0*9KfkdXqz-ZHxe3eF.gif 720w, https://miro.medium.com/v2/0*9KfkdXqz-ZHxe3eF.gif 750w, https://miro.medium.com/v2/0*9KfkdXqz-ZHxe3eF.gif 786w, https://miro.medium.com/v2/0*9KfkdXqz-ZHxe3eF.gif 828w, https://miro.medium.com/v2/0*9KfkdXqz-ZHxe3eF.gif 1100w, https://miro.medium.com/v2/0*9KfkdXqz-ZHxe3eF.gif 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/0*9KfkdXqz-ZHxe3eF.gif 640w, https://miro.medium.com/v2/0*9KfkdXqz-ZHxe3eF.gif 720w, https://miro.medium.com/v2/0*9KfkdXqz-ZHxe3eF.gif 750w, https://miro.medium.com/v2/0*9KfkdXqz-ZHxe3eF.gif 786w, https://miro.medium.com/v2/0*9KfkdXqz-ZHxe3eF.gif 828w, https://miro.medium.com/v2/0*9KfkdXqz-ZHxe3eF.gif 1100w, https://miro.medium.com/v2/0*9KfkdXqz-ZHxe3eF.gif 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/proxy/0*9KfkdXqz-ZHxe3eF.gif"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Image credit: <a href="https://xiandong79.github.io/seq2seq-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86" target="_self">xiandong79.github.io</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Bidirectional Layer</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Now that we understand how context flows through the network via the hidden state, let’s take it a step further by allowing that context to flow in both directions. This is what a bidirectional layer does.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:53%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 400px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*DHjnkS0rueJik4R0.jpg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*DHjnkS0rueJik4R0.jpg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*DHjnkS0rueJik4R0.jpg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*DHjnkS0rueJik4R0.jpg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*DHjnkS0rueJik4R0.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*DHjnkS0rueJik4R0.jpg 1100w, https://miro.medium.com/v2/resize:fit:800/format:webp/0*DHjnkS0rueJik4R0.jpg 800w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 400px" srcset="https://miro.medium.com/v2/resize:fit:640/0*DHjnkS0rueJik4R0.jpg 640w, https://miro.medium.com/v2/resize:fit:720/0*DHjnkS0rueJik4R0.jpg 720w, https://miro.medium.com/v2/resize:fit:750/0*DHjnkS0rueJik4R0.jpg 750w, https://miro.medium.com/v2/resize:fit:786/0*DHjnkS0rueJik4R0.jpg 786w, https://miro.medium.com/v2/resize:fit:828/0*DHjnkS0rueJik4R0.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/0*DHjnkS0rueJik4R0.jpg 1100w, https://miro.medium.com/v2/resize:fit:800/0*DHjnkS0rueJik4R0.jpg 800w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/400/0*DHjnkS0rueJik4R0.jpg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In the example above, the encoder only has historical context. But, providing future context can result in better model performance. This may seem counterintuitive to the way humans process language since we only read in one direction. However, humans often require future context to interpret what is being said. In other words, sometimes we don’t understand a sentence until an important word or phrase is provided at the end. Happens this does whenever Yoda speaks. 😑 🙏</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To implement this, we train two RNN layers simultaneously. The first layer is fed the input sequence as-is and the second is fed a reversed copy.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*IYLXY8Dxvw8rbd8E.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*IYLXY8Dxvw8rbd8E.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*IYLXY8Dxvw8rbd8E.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*IYLXY8Dxvw8rbd8E.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*IYLXY8Dxvw8rbd8E.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*IYLXY8Dxvw8rbd8E.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IYLXY8Dxvw8rbd8E.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*IYLXY8Dxvw8rbd8E.png 640w, https://miro.medium.com/v2/resize:fit:720/0*IYLXY8Dxvw8rbd8E.png 720w, https://miro.medium.com/v2/resize:fit:750/0*IYLXY8Dxvw8rbd8E.png 750w, https://miro.medium.com/v2/resize:fit:786/0*IYLXY8Dxvw8rbd8E.png 786w, https://miro.medium.com/v2/resize:fit:828/0*IYLXY8Dxvw8rbd8E.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*IYLXY8Dxvw8rbd8E.png 1100w, https://miro.medium.com/v2/resize:fit:1400/0*IYLXY8Dxvw8rbd8E.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*IYLXY8Dxvw8rbd8E.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Image credit: Udacity</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Hidden Layer with Gated Recurrent Unit (GRU)</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Now let’s make our RNN a little bit smarter. Instead of allowing <em>all</em> of the information from the hidden state to flow through the network, what if we could be more selective? Perhaps some of the information is more relevant, while other information should be discarded. This is essentially what a gated recurrent unit (GRU) does.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">There are two gates in a GRU: an update gate and reset gate. <a href="https://medium.com/understanding-gru-networks-2ef37df6c9be" target="_self">This article</a> by Simeon Kostadinov, explains these in detail. To summarize, the <strong>update gate </strong>(z) helps the model determine how much information from previous time steps needs to be passed along to the future. Meanwhile, the <strong>reset gate</strong> (r) decides how much of the past information to forget.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*FvPLILqWS2FWD-98.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*FvPLILqWS2FWD-98.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*FvPLILqWS2FWD-98.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*FvPLILqWS2FWD-98.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*FvPLILqWS2FWD-98.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*FvPLILqWS2FWD-98.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FvPLILqWS2FWD-98.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*FvPLILqWS2FWD-98.png 640w, https://miro.medium.com/v2/resize:fit:720/0*FvPLILqWS2FWD-98.png 720w, https://miro.medium.com/v2/resize:fit:750/0*FvPLILqWS2FWD-98.png 750w, https://miro.medium.com/v2/resize:fit:786/0*FvPLILqWS2FWD-98.png 786w, https://miro.medium.com/v2/resize:fit:828/0*FvPLILqWS2FWD-98.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*FvPLILqWS2FWD-98.png 1100w, https://miro.medium.com/v2/resize:fit:1400/0*FvPLILqWS2FWD-98.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*FvPLILqWS2FWD-98.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Image Credit: <a href="https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/gru/" target="_self">analyticsvidhya.com</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Final Model</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Now that we’ve discussed the various parts of our model, let’s take a look at the code. Again, all of the source code is available <a href="https://github.com/tommytracey/AIND-Capstone/blob/master/machine_translation.ipynb" target="_self">here in the notebook</a> (<a href="https://tommytracey.github.io/AIND-Capstone/machine_translation.html" target="_self">.html version</a>).</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre>def  model_final (input_shape, output_sequence_length, english_vocab_size, french_vocab_size):
    """
    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN
    :param input_shape: Tuple of input shape
    :param output_sequence_length: Length of output sequence
    :param english_vocab_size: Number of unique English words in the dataset
    :param french_vocab_size: Number of unique French words in the dataset
    :return: Keras model built, but not trained
    """
    # Hyperparameters
    learning_rate = 0.003

    # Build the layers    
    model = Sequential()
    # Embedding
    model.add(Embedding(english_vocab_size, 128, input_length=input_shape[1],
                         input_shape=input_shape[1:]))
    # Encoder
    model.add(Bidirectional(GRU(128)))
    model.add(RepeatVector(output_sequence_length))
    # Decoder
    model.add(Bidirectional(GRU(128, return_sequences=True)))
    model.add(TimeDistributed(Dense(512, activation='relu')))
    model.add(Dropout(0.5))
    model.add(TimeDistributed(Dense(french_vocab_size, activation='softmax')))
    model.compile(loss=sparse_categorical_crossentropy,
                  optimizer=Adam(learning_rate),
                  metrics=['accuracy'])
    return model</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Results</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The results from the final model can be found in cell 20 of the <a href="https://tommytracey.github.io/AIND-Capstone/machine_translation.html" target="_self">notebook</a>.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*MkdtK5JXvYNFHFXHCPU3aw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*MkdtK5JXvYNFHFXHCPU3aw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*MkdtK5JXvYNFHFXHCPU3aw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*MkdtK5JXvYNFHFXHCPU3aw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*MkdtK5JXvYNFHFXHCPU3aw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*MkdtK5JXvYNFHFXHCPU3aw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MkdtK5JXvYNFHFXHCPU3aw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*MkdtK5JXvYNFHFXHCPU3aw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*MkdtK5JXvYNFHFXHCPU3aw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*MkdtK5JXvYNFHFXHCPU3aw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*MkdtK5JXvYNFHFXHCPU3aw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*MkdtK5JXvYNFHFXHCPU3aw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*MkdtK5JXvYNFHFXHCPU3aw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*MkdtK5JXvYNFHFXHCPU3aw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*MkdtK5JXvYNFHFXHCPU3aw.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Validation accuracy: 97.5%</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Training time: 23 epochs</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Future Improvements</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;"><strong>Do proper data split (training, validation, test)</strong>. Currently, there is no test set, only training and validation. Obviously, this doesn’t follow best practices.</li><li class="ff3" style="font-size:22px;"><strong>LSTM + attention</strong>. This has been the de facto architecture for RNNs over the past few years, although there are <a href="https://medium.com/the-fall-of-rnn-lstm-2d1594c74ce0" target="_self">some limitations</a>. I didn’t use LSTM because I’d <a href="https://github.com/tommytracey/udacity/tree/master/deep-learning-nano/projects/4-language-translation#build-the-neural-network" target="_self">already implemented it in TensorFlow in another project</a>, and I wanted to experiment with GRU + Keras for this project.</li><li class="ff3" style="font-size:22px;"><strong>Train on a larger and more diverse text corpus</strong>. The text corpus and vocabulary for this project are quite small with little variation in syntax. As a result, the model is very brittle. To create a model that generalizes better, you’ll need to train on a larger dataset with more variability in grammar and sentence structure.</li><li class="ff3" style="font-size:22px;"><strong>Residual layers</strong>. You could add residual layers to a deep LSTM RNN, as described in <a href="https://arxiv.org/abs/1701.03360" target="_self">this paper</a>. Or, use residual layers as an alternative to LSTM and GRU, as described <a href="http://www.mdpi.com/2078-2489/9/3/56/pdf" target="_self">here</a>.</li><li class="ff3" style="font-size:22px;"><strong>Embeddings</strong>. If you’re training on a larger dataset, you should definitely use a pre-trained set of embeddings such as <a href="https://mubaris.com/2017/12/14/word2vec/" target="_self">word2vec</a> or <a href="https://nlp.stanford.edu/projects/glove/" target="_self">GloVe</a>. Even better, use ELMo or BERT.</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><strong>Embedding Language Model (ELMo)</strong>. One of the biggest advances in <a href="https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a" target="_self">universal embeddings</a> in 2018 was <a href="https://allennlp.org/elmo" target="_self">ELMo</a>, developed by the <a href="https://allennlp.org" target="_self">Allen Institute for AI</a>. One of the major advantages of ELMo is that it addresses the problem of polysemy, in which a single word has multiple meanings. ELMo is context-based (not word-based), so different meanings for a word occupy different vectors within the embedding space. With GloVe and word2vec, each word has only one representation in the embedding space. For example, the word “queen” could refer to the matriarch of a royal family, a bee, a chess piece, or the 1970s rock band. With traditional embeddings, all of these meanings are tied to a single vector for the word <em>queen</em>. With ELMO, these are four distinct vectors, each with a unique set of context words occupying the same region of the embedding space. For example, we’d expect to see words like <em>queen</em>, <em>rook</em>, and <em>pawn</em> in a similar vector space related to the game of chess. And we’d expect to see <em>queen</em>, <em>hive</em>, and <em>honey</em> in a different vector space related to bees. This provides a significant boost in semantic encoding.</li><li class="ff3" style="font-size:22px;"><strong>Bidirectional Encoder Representations from </strong><a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" target="_self">Transformers</a><strong> (BERT)</strong>. So far in 2019, the biggest advancement in bidirectional embeddings has been <a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" target="_self">BERT</a>, which was open-sourced by Google. How is BERT different?</li></ul></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4">Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary. For example, the word “bank” would have the same context-free representation in “bank account” and “bank of the river.” Contextual models instead generate a representation of each word that is based on the other words in the sentence. For example, in the sentence “I accessed the bank account,” a unidirectional contextual model would represent “bank” based on “I accessed the” but not “account.” However, BERT represents “bank” using both its previous and next context — “I accessed the … account” — starting from the very bottom of a deep neural network, making it deeply bidirectional.<br></br> — Jacob Devlin and Ming-Wei Chang, <a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" target="_self">Google AI Blog</a></p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Contact</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I hope you found this useful. Again, if you have any feedback, I’d love to hear it. Feel free to post in the comments.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you’d like to discuss other collaboration or career opportunities you can find me <a href="https://linkedin.com/in/thomastracey" target="_self">here on LinkedIn</a> or view <a href="https://ttracey.com" target="_self">my portfolio here</a>.</p></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>