<!DOCTYPE html>
                <html>
                <head>
                    <title>Deep Learning for Object Detection: A Comprehensive Review</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://towardsdatascience.com/deep-learning-for-object-detection-a-comprehensive-review-73930816d8d9"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://medium.com/@joycex99?source=post_page-----73930816d8d9--------------------------------">Author : Joyce Xu</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>Deep Learning for Object Detection: A Comprehensive Review</h3></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*ftTEVgsx0jfvUSFB6X5mQg.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*ftTEVgsx0jfvUSFB6X5mQg.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*ftTEVgsx0jfvUSFB6X5mQg.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*ftTEVgsx0jfvUSFB6X5mQg.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*ftTEVgsx0jfvUSFB6X5mQg.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ftTEVgsx0jfvUSFB6X5mQg.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ftTEVgsx0jfvUSFB6X5mQg.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*ftTEVgsx0jfvUSFB6X5mQg.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*ftTEVgsx0jfvUSFB6X5mQg.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*ftTEVgsx0jfvUSFB6X5mQg.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*ftTEVgsx0jfvUSFB6X5mQg.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*ftTEVgsx0jfvUSFB6X5mQg.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*ftTEVgsx0jfvUSFB6X5mQg.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*ftTEVgsx0jfvUSFB6X5mQg.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*ftTEVgsx0jfvUSFB6X5mQg.jpeg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">With the rise of autonomous vehicles, smart video surveillance, facial detection and various people counting applications, fast and accurate object detection systems are rising in demand. These systems involve not only recognizing and classifying every object in an image, but <em>localizing</em> each one by drawing the appropriate bounding box around it. This makes object detection a significantly harder task than its traditional computer vision predecessor, image classification.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Fortunately, however, the most successful approaches to object detection are currently extensions of image classification models. A few months ago, Google released a <a href="https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html" target="_self">new object detection API</a> for Tensorflow. With this release came the pre-built architectures and weights for a <a href="https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md" target="_self">few specific models</a>:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/abs/1512.02325" target="_self">Single Shot Multibox Detector</a> (SSD) with <a href="http://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html" target="_self">MobileNets</a></li><li class="ff3" style="font-size:22px;">SSD with <a href="https://arxiv.org/abs/1512.00567" target="_self">Inception V2</a></li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/abs/1605.06409" target="_self">Region-Based Fully Convolutional Networks</a> (R-FCN) with <a href="https://arxiv.org/abs/1512.03385" target="_self">Resnet 101</a></li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/abs/1506.01497" target="_self">Faster RCNN</a> with Resnet 101</li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/abs/1506.01497" target="_self">Faster RCNN</a> with <a href="https://arxiv.org/abs/1602.07261" target="_self">Inception Resnet v2</a></li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In my <a href="https://medium.com/towards-data-science/an-intuitive-guide-to-deep-network-architectures-65fdc477db41" target="_self">last blog post</a>, I covered the intuition behind the three base network architectures listed above: MobileNets, Inception, and ResNet. This time around, I want to do the same for Tensorflow’s object detection models: Faster R-CNN, R-FCN, and SSD. By the end of this post, we will hopefully have gained an understanding of how deep learning is applied to object detection, and how these object detection models both inspire and diverge from one another.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Faster R-CNN</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Faster R-CNN is now a canonical model for deep learning-based object detection. It helped inspire many detection and segmentation models that came after it, including the two others we’re going to examine today. Unfortunately, we can’t really begin to understand Faster R-CNN without understanding its own predecessors, R-CNN and Fast R-CNN, so let’s take a quick dive into its ancestry.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">R-CNN</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">R-CNN is the grand-daddy of Faster R-CNN. In other words, R-CNN <em>really</em> kicked things off.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">R-CNN, or <strong>R</strong>egion-based <strong>C</strong>onvolutional <strong>N</strong>eural <strong>N</strong>etwork, consisted of 3 simple steps:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;">Scan the input image for possible objects using an algorithm called Selective Search, generating ~2000 <strong>region proposals</strong></li><li class="ff3" style="font-size:22px;">Run a convolutional neural net (<strong>CNN</strong>) on top of each of these region proposals</li><li class="ff3" style="font-size:22px;">Take the output of each <strong>CNN</strong> and feed it into a) an SVM to classify the region and b) a linear regressor to tighten the bounding box of the object, if such an object exists.</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">These 3 steps are illustrated in the image below:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*D2sFqL329qKKx4Tvl31IhQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*D2sFqL329qKKx4Tvl31IhQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*D2sFqL329qKKx4Tvl31IhQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*D2sFqL329qKKx4Tvl31IhQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*D2sFqL329qKKx4Tvl31IhQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*D2sFqL329qKKx4Tvl31IhQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D2sFqL329qKKx4Tvl31IhQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*D2sFqL329qKKx4Tvl31IhQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*D2sFqL329qKKx4Tvl31IhQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*D2sFqL329qKKx4Tvl31IhQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*D2sFqL329qKKx4Tvl31IhQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*D2sFqL329qKKx4Tvl31IhQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*D2sFqL329qKKx4Tvl31IhQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*D2sFqL329qKKx4Tvl31IhQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*D2sFqL329qKKx4Tvl31IhQ.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In other words, we first propose regions, then extract features, and then classify those regions based on their features. In essence, we have turned object detection into an image classification problem. R-CNN was very intuitive, but very slow.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Fast R-CNN</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">R-CNN’s immediate descendant was Fast-R-CNN. Fast R-CNN resembled the original in many ways, but improved on its detection speed through two main augmentations:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;">Performing feature extraction over the image <strong>before</strong> proposing regions, thus only running one CNN over the entire image instead of 2000 CNN’s over 2000 overlapping regions</li><li class="ff3" style="font-size:22px;">Replacing the SVM with a softmax layer, thus extending the neural network for predictions instead of creating a new model</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The new model looked something like this:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*iWyUwIPO-5kA2ECAfaaPSg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*iWyUwIPO-5kA2ECAfaaPSg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*iWyUwIPO-5kA2ECAfaaPSg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*iWyUwIPO-5kA2ECAfaaPSg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*iWyUwIPO-5kA2ECAfaaPSg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*iWyUwIPO-5kA2ECAfaaPSg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iWyUwIPO-5kA2ECAfaaPSg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*iWyUwIPO-5kA2ECAfaaPSg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*iWyUwIPO-5kA2ECAfaaPSg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*iWyUwIPO-5kA2ECAfaaPSg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*iWyUwIPO-5kA2ECAfaaPSg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*iWyUwIPO-5kA2ECAfaaPSg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*iWyUwIPO-5kA2ECAfaaPSg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*iWyUwIPO-5kA2ECAfaaPSg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*iWyUwIPO-5kA2ECAfaaPSg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">As we can see from the image, we are now generating region proposals based on the last feature map of the network, not from the original image itself. As a result, we can train just <strong>one</strong> CNN for the entire image.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In addition, instead of training many different SVM’s to classify each object class, there is a single softmax layer that outputs the class probabilities directly. Now we only have one neural net to train, as opposed to one neural net and many SVM’s.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Fast R-CNN performed much better in terms of speed. There was just one big bottleneck remaining: the selective search algorithm for generating region proposals.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Faster R-CNN</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">At this point, we’re back to our original target: Faster R-CNN. The main insight of Faster R-CNN was to replace the slow selective search algorithm with a fast neural net. Specifically, it introduced the <strong>region proposal network</strong> (RPN).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here’s how the RPN worked:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">At the last layer of an initial CNN, a 3x3 sliding window moves across the feature map and maps it to a lower dimension (e.g. 256-d)</li><li class="ff3" style="font-size:22px;">For each sliding-window location, it generates <em>multiple</em> possible regions based on <em>k</em> fixed-ratio <strong>anchor boxes </strong>(default bounding boxes)</li><li class="ff3" style="font-size:22px;">Each region proposal consists of a) an “objectness” score for that region and b) 4 coordinates representing the bounding box of the region</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In other words, we look at each location in our last feature map and consider <em>k</em> different boxes centered around it: a tall box, a wide box, a large box, etc. For each of those boxes, we output whether or not we think it contains an object, and what the coordinates for that box are. This is what it looks like at one sliding window location:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*7heX-no7cdqllky-GwGBfQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*7heX-no7cdqllky-GwGBfQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*7heX-no7cdqllky-GwGBfQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*7heX-no7cdqllky-GwGBfQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*7heX-no7cdqllky-GwGBfQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*7heX-no7cdqllky-GwGBfQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7heX-no7cdqllky-GwGBfQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*7heX-no7cdqllky-GwGBfQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*7heX-no7cdqllky-GwGBfQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*7heX-no7cdqllky-GwGBfQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*7heX-no7cdqllky-GwGBfQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*7heX-no7cdqllky-GwGBfQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*7heX-no7cdqllky-GwGBfQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*7heX-no7cdqllky-GwGBfQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*7heX-no7cdqllky-GwGBfQ.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The 2<em>k</em> scores represent the softmax probability of each of the <em>k</em> bounding boxes being on “object.” Notice that although the RPN outputs bounding box coordinates, it does not try to classify any potential objects: its sole job is still proposing object regions. If an anchor box has an “objectness” score above a certain threshold, that box’s coordinates get passed forward as a region proposal.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Once we have our region proposals, we feed them straight into what is essentially a Fast R-CNN. We add a pooling layer, some fully-connected layers, and finally a softmax classification layer and bounding box regressor. In a sense, <strong>Faster R-CNN = RPN + Fast R-CNN.</strong></p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*LHk_CCzzfP9mzw280kG70w.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*LHk_CCzzfP9mzw280kG70w.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*LHk_CCzzfP9mzw280kG70w.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*LHk_CCzzfP9mzw280kG70w.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*LHk_CCzzfP9mzw280kG70w.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*LHk_CCzzfP9mzw280kG70w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LHk_CCzzfP9mzw280kG70w.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*LHk_CCzzfP9mzw280kG70w.png 640w, https://miro.medium.com/v2/resize:fit:720/1*LHk_CCzzfP9mzw280kG70w.png 720w, https://miro.medium.com/v2/resize:fit:750/1*LHk_CCzzfP9mzw280kG70w.png 750w, https://miro.medium.com/v2/resize:fit:786/1*LHk_CCzzfP9mzw280kG70w.png 786w, https://miro.medium.com/v2/resize:fit:828/1*LHk_CCzzfP9mzw280kG70w.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*LHk_CCzzfP9mzw280kG70w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*LHk_CCzzfP9mzw280kG70w.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*LHk_CCzzfP9mzw280kG70w.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Altogether, Faster R-CNN achieved much better speeds and a state-of-the-art accuracy. It is worth noting that although future models did a lot to increase detection speeds, few models managed to outperform Faster R-CNN by a significant margin. In other words, Faster R-CNN may not be the simplest or fastest method for object detection, but it is still one of the best performing. Case in point, Tensorflow’s Faster R-CNN with Inception ResNet is their <a href="https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md" target="_self">slowest but most accurate model</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">At the end of the day, Faster R-CNN may look complicated, but its core design is the same as the original R-CNN: <strong>hypothesize object regions and then classify them</strong>. This is now the predominant pipeline for many object detection models, including our next one.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">R-FCN</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Remember how Fast R-CNN improved on the original’s detection speed by sharing a single CNN computation across all region proposals? That kind of thinking was also the motivation behind R-FCN: <em>increase speed by maximizing shared computation.</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">R-FCN, or <strong>R</strong>egion-based <strong>F</strong>ully <strong>C</strong>onvolutional <strong>N</strong>et, shares 100% of the computations across every single output. Being <a href="https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_segmentation.html" target="_self">fully convolutional</a>, it ran into a unique problem in model design.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">On the one hand, when performing classification of an object, we want to learn <em>location invariance</em> in a model: regardless of where the cat appears in the image, we want to classify it as a cat. On the other hand, when performing detection of the object, we want to learn <em>location variance</em>: if the cat is in the top left-hand corner, we want to draw a box in the top left-hand corner. So if we’re trying to share convolutional computations across 100% of the net, how do we compromise between location invariance and location variance?</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">R-FCN’s solution: <strong>position-sensitive score maps</strong>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Each position-sensitive score map represents <em>one relative position</em> of <em>one object class</em>. For example, one score map might activate wherever it detects the <em>top-right</em> of a <em>cat</em>. Another score map might activate where it sees the <em>bottom-left</em> of a <em>car</em>. You get the point. Essentially, these score maps are <strong>convolutional feature maps that have been trained to recognize certain parts of each object</strong>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Now, R-FCN works as follows:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;">Run a CNN (in this case, ResNet) over the input image</li><li class="ff3" style="font-size:22px;">Add a fully convolutional layer to generate a <strong>score bank</strong> of the aforementioned “position-sensitive score maps.” There should be k²(C+1) score maps, with k² representing the number of relative positions to divide an object (e.g. 3² for a 3 by 3 grid) and C+1 representing the number of classes plus the background.</li><li class="ff3" style="font-size:22px;">Run a fully convolutional region proposal network (RPN) to generate regions of interest (RoI’s)</li><li class="ff3" style="font-size:22px;">For each RoI, divide it into the same k² “bins” or subregions as the score maps</li><li class="ff3" style="font-size:22px;">For each bin, check the score bank to see if that bin matches the corresponding position of some object. For example, if I’m on the “upper-left” bin, I will grab the score maps that correspond to the “upper-left” corner of an object and average those values in the RoI region. This process is repeated for each class.</li><li class="ff3" style="font-size:22px;">Once each of the k² bins has an “object match” value for each class, average the bins to get a single score per class.</li><li class="ff3" style="font-size:22px;">Classify the RoI with a softmax over the remaining C+1 dimensional vector</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Altogether, R-FCN looks something like this, with an RPN generating the RoI’s:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*cHEvY3E2HW65AF-mPeMwOg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*cHEvY3E2HW65AF-mPeMwOg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*cHEvY3E2HW65AF-mPeMwOg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*cHEvY3E2HW65AF-mPeMwOg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*cHEvY3E2HW65AF-mPeMwOg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*cHEvY3E2HW65AF-mPeMwOg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cHEvY3E2HW65AF-mPeMwOg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*cHEvY3E2HW65AF-mPeMwOg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*cHEvY3E2HW65AF-mPeMwOg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*cHEvY3E2HW65AF-mPeMwOg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*cHEvY3E2HW65AF-mPeMwOg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*cHEvY3E2HW65AF-mPeMwOg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*cHEvY3E2HW65AF-mPeMwOg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*cHEvY3E2HW65AF-mPeMwOg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*cHEvY3E2HW65AF-mPeMwOg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Even with the explanation and the image, you might still be a little confused on how this model works. Honestly, R-FCN is much easier to understand when you can visualize what it’s doing. Here is one such example of an R-FCN in practice, detecting a baby:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Q20DdanzQbvBjg4DLvJkGg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Q20DdanzQbvBjg4DLvJkGg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Q20DdanzQbvBjg4DLvJkGg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Q20DdanzQbvBjg4DLvJkGg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Q20DdanzQbvBjg4DLvJkGg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Q20DdanzQbvBjg4DLvJkGg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q20DdanzQbvBjg4DLvJkGg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Q20DdanzQbvBjg4DLvJkGg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*Q20DdanzQbvBjg4DLvJkGg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*Q20DdanzQbvBjg4DLvJkGg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*Q20DdanzQbvBjg4DLvJkGg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*Q20DdanzQbvBjg4DLvJkGg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*Q20DdanzQbvBjg4DLvJkGg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*Q20DdanzQbvBjg4DLvJkGg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*Q20DdanzQbvBjg4DLvJkGg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><mark>Simply put, R-FCN considers each region proposal, divides it up into sub-regions, and iterates over the sub-regions asking: “does this look like the top-left of a baby?”, “does this look like the top-center of a baby?” “does this look like the top-right of a baby?”, etc. It repeats this for all possible classes. If enough of the sub-regions say “yes, I match up with that part of a baby!”, the RoI gets classified as a baby after a softmax over all the classes.</mark></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">With this setup, R-FCN is able to simultaneously address <em>location variance </em>by proposing different object regions, and <em>location invariance</em> by having each region proposal refer back to the same bank of score maps. These score maps should learn to classify a cat as a cat, regardless of where the cat appears. Best of all, it is fully convolutional, meaning all of the computation is shared throughout the network.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">As a result, R-FCN is several times faster than Faster R-CNN, and achieves comparable accuracy.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">SSD</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Our final model is SSD, which stands for <strong>S</strong>ingle-<strong>S</strong>hot <strong>D</strong>etector. Like R-FCN, it provides enormous speed gains over Faster R-CNN, but does so in a markedly different manner.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Our first two models performed region proposals and region classifications in two separate steps. First, they used a region proposal network to generate regions of interest; next, they used either fully-connected layers or position-sensitive convolutional layers to classify those regions. SSD does the two in a “single shot,” simultaneously predicting the bounding box and the class as it processes the image.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Concretely, given an input image and a set of ground truth labels, SSD does the following:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;">Pass the image through a series of convolutional layers, yielding several sets of feature maps at different scales (e.g. 10x10, then 6x6, then 3x3, etc.)</li><li class="ff3" style="font-size:22px;">For each location in <em>each</em> of these feature maps, use a 3x3 convolutional filter to evaluate a small set of default bounding boxes. These default bounding boxes are essentially equivalent to Faster R-CNN’s anchor boxes.</li><li class="ff3" style="font-size:22px;">For each box, simultaneously predict a) the bounding box offset and b) the class probabilities</li><li class="ff3" style="font-size:22px;">During training, match the ground truth box with these predicted boxes based on <a href="https://en.wikipedia.org/wiki/Jaccard_index" target="_self">IoU</a>. The best predicted box will be labeled a “positive,” along with all other boxes that have an IoU with the truth &gt;0.5.</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">SSD sounds straightforward, but training it has a unique challenge. With the previous two models, the region proposal network ensured that everything we tried to classify had some minimum probability of being an “object.” With SSD, however, we skip that filtering step. We classify and draw bounding boxes from <em>every single position in the image</em>, using <em>multiple different shapes</em>, at <em>several different scales</em>. As a result, we generate a much greater number of bounding boxes than the other models, and nearly all of the them are negative examples.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To fix this imbalance, SSD does two things. Firstly, it uses <a href="https://docs.microsoft.com/en-us/cognitive-toolkit/Object-Detection-using-Fast-R-CNN#algorithm-details" target="_self">non-maximum suppression</a> to group together highly-overlapping boxes into a single box. In other words, if four boxes of similar shapes, sizes, etc. contain the same dog, NMS would keep the one with the highest confidence and discard the rest. Secondly, the model uses a technique called <a href="https://arxiv.org/pdf/1608.02236.pdf" target="_self">hard negative mining</a> to balance classes during training. In hard negative mining, only a subset of the negative examples with the highest training loss (i.e. false positives) are used at each iteration of training. SSD keeps a 3:1 ratio of negatives to positives.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Its architecture looks like this:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*p-lSawysBsiBzlcWZ9_UMw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*p-lSawysBsiBzlcWZ9_UMw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*p-lSawysBsiBzlcWZ9_UMw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*p-lSawysBsiBzlcWZ9_UMw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*p-lSawysBsiBzlcWZ9_UMw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*p-lSawysBsiBzlcWZ9_UMw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p-lSawysBsiBzlcWZ9_UMw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*p-lSawysBsiBzlcWZ9_UMw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*p-lSawysBsiBzlcWZ9_UMw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*p-lSawysBsiBzlcWZ9_UMw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*p-lSawysBsiBzlcWZ9_UMw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*p-lSawysBsiBzlcWZ9_UMw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*p-lSawysBsiBzlcWZ9_UMw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*p-lSawysBsiBzlcWZ9_UMw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*p-lSawysBsiBzlcWZ9_UMw.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">As I mentioned above, there are “extra feature layers” at the end that scale down in size. These varying-size feature maps help capture objects of different sizes. For example, here is SSD in action:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*JuhjYUWXgfxMMoa4SIKLkA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*JuhjYUWXgfxMMoa4SIKLkA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*JuhjYUWXgfxMMoa4SIKLkA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*JuhjYUWXgfxMMoa4SIKLkA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*JuhjYUWXgfxMMoa4SIKLkA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*JuhjYUWXgfxMMoa4SIKLkA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JuhjYUWXgfxMMoa4SIKLkA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*JuhjYUWXgfxMMoa4SIKLkA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*JuhjYUWXgfxMMoa4SIKLkA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*JuhjYUWXgfxMMoa4SIKLkA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*JuhjYUWXgfxMMoa4SIKLkA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*JuhjYUWXgfxMMoa4SIKLkA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*JuhjYUWXgfxMMoa4SIKLkA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*JuhjYUWXgfxMMoa4SIKLkA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*JuhjYUWXgfxMMoa4SIKLkA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In smaller feature maps (e.g. 4x4), each cell covers a larger region of the image, enabling them to detect larger objects. Region proposal and classification are performed simultaneously: given <em>p</em> object classes, each bounding box is associated with a (4+<em>p</em>)-dimensional vector that outputs 4 box offset coordinates and <em>p</em> class probabilities. In the last step, softmax is again used to classify the object.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Ultimately, SSD is not so different from the first two models. It simply skips the “region proposal” step, instead considering every single bounding box in every location of the image simultaneously with its classification. Because SSD does everything in one shot, it is the <a href="https://github.com/tensorflow/models/blob/master/object_detection/g3doc/detection_model_zoo.md" target="_self">fastest of the three models</a>, and still performs quite comparably.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Conclusion</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Faster R-CNN, R-FCN, and SSD are three of the best and most widely used object detection models out there right now. Other popular models tend to be fairly similar to these three, all relying on deep CNN’s (read: ResNet, Inception, etc.) to do the initial heavy lifting and largely following the same proposal/classification pipeline.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">At this point, putting these models to use just requires knowing Tensorflow’s API. Tensorflow has a starter tutorial on using these models <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb" target="_self">here</a>. Give it a try, and happy hacking!</p></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>