<!DOCTYPE html>
                <html>
                <head>
                    <title>Recurrent Neural Networks by Example in Python</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://williamkoehrsen.medium.com/?source=post_page-----ffd204f99470--------------------------------">Author : Will Koehrsen</a> </h5></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*oT5oL9eO5BF9qKcerVD2qg.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*oT5oL9eO5BF9qKcerVD2qg.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*oT5oL9eO5BF9qKcerVD2qg.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*oT5oL9eO5BF9qKcerVD2qg.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*oT5oL9eO5BF9qKcerVD2qg.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*oT5oL9eO5BF9qKcerVD2qg.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oT5oL9eO5BF9qKcerVD2qg.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*oT5oL9eO5BF9qKcerVD2qg.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*oT5oL9eO5BF9qKcerVD2qg.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*oT5oL9eO5BF9qKcerVD2qg.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*oT5oL9eO5BF9qKcerVD2qg.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*oT5oL9eO5BF9qKcerVD2qg.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*oT5oL9eO5BF9qKcerVD2qg.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*oT5oL9eO5BF9qKcerVD2qg.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*oT5oL9eO5BF9qKcerVD2qg.jpeg"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">(<a href="https://www.pexels.com/photo/agriculture-alternative-energy-clouds-countryside-414837/" target="_self">Source</a>)</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>Recurrent Neural Networks by Example in Python</h3></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5 text-muted">Using a Recurrent Neural Network to Write Patent Abstracts</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The first time I attempted to study recurrent neural networks, I made the mistake of trying to learn the theory behind things like LSTMs and GRUs first. After several frustrating days looking at linear algebra equations, I happened on the following passage in <a href="https://www.manning.com/books/deep-learning-with-python" target="_self">Deep Learning with Python</a><em>:</em></p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4">In summary, you don’t need to understand everything about the specific architecture of an LSTM cell; as a human, it shouldn’t be your job to understand it. Just keep in mind what the LSTM cell is meant to do: allow past information to be reinjected at a later time.</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This was the author of the library Keras (Francois Chollet), an expert in deep learning, telling me I didn’t need to understand everything at the foundational level! I realized that my mistake had been starting at the bottom, with the theory, instead of just trying to build a recurrent neural network.</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4">Shortly thereafter, I switched tactics and decided to try the most effective way of learning a data science technique: find a problem and solve it!</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This <a href="https://course.fast.ai/about.html" target="_self">top-down approach</a> means learning <em>how to </em><strong>implement</strong><em> a method </em><strong>before</strong><em> going back and covering the </em><strong>theory</strong>. This way, I’m able to figure out what I need to know along the way, and when I return to study the concepts, I have a framework into which I can fit each idea. In this mindset, I decided to stop worrying about the details and complete a recurrent neural network project.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This article walks through how to build and use a recurrent neural network in Keras to write patent abstracts. The article is light on the theory, but as you work through the project, you’ll find you pick up what you need to know along the way. The end result is you can build a useful application and figure out how a deep learning method for natural language processing works.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The full code is available as a series of <a href="https://github.com/WillKoehrsen/recurrent-neural-networks" target="_self">Jupyter Notebooks on GitHub</a>. I’ve also provided all the <a href="https://github.com/WillKoehrsen/recurrent-neural-networks/tree/master/models" target="_self">pre-trained models</a> so you don’t have to train them for several hours yourself! To get started as quickly as possible and investigate the models, see the <a href="https://github.com/WillKoehrsen/recurrent-neural-networks/blob/master/notebooks/Quick%20Start%20to%20Recurrent%20Neural%20Networks.ipynb" target="_self">Quick Start to Recurrent Neural Networks</a>, and for in-depth explanations, refer to <a href="https://github.com/WillKoehrsen/recurrent-neural-networks/blob/master/notebooks/Deep%20Dive%20into%20Recurrent%20Neural%20Networks.ipynb" target="_self">Deep Dive into Recurrent Neural Networks</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Recurrent Neural Network</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">It’s helpful to understand at least some of the basics before getting to the implementation. At a high level, a<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_self"> recurrent neural network</a> (RNN) processes sequences — whether daily stock prices, sentences, or sensor measurements — one element at a time while retaining a <em>memory</em> (called a state) of what has come previously in the sequence.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><mark>Recurrent</mark><mark> means the output at the current time step becomes the input to the next time step. At each element of the sequence, the model considers not just the current input, but what it remembers about the preceding elements.</mark></p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:56%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 423px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*KljWrINqItHR6ng05ASR8w.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*KljWrINqItHR6ng05ASR8w.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*KljWrINqItHR6ng05ASR8w.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*KljWrINqItHR6ng05ASR8w.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*KljWrINqItHR6ng05ASR8w.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*KljWrINqItHR6ng05ASR8w.png 1100w, https://miro.medium.com/v2/resize:fit:846/format:webp/1*KljWrINqItHR6ng05ASR8w.png 846w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 423px" srcset="https://miro.medium.com/v2/resize:fit:640/1*KljWrINqItHR6ng05ASR8w.png 640w, https://miro.medium.com/v2/resize:fit:720/1*KljWrINqItHR6ng05ASR8w.png 720w, https://miro.medium.com/v2/resize:fit:750/1*KljWrINqItHR6ng05ASR8w.png 750w, https://miro.medium.com/v2/resize:fit:786/1*KljWrINqItHR6ng05ASR8w.png 786w, https://miro.medium.com/v2/resize:fit:828/1*KljWrINqItHR6ng05ASR8w.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*KljWrINqItHR6ng05ASR8w.png 1100w, https://miro.medium.com/v2/resize:fit:846/1*KljWrINqItHR6ng05ASR8w.png 846w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/423/1*KljWrINqItHR6ng05ASR8w.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Overview of RNN (<a href="https://www.manning.com/books/deep-learning-with-python" target="_self">Source</a>)</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This memory allows the network to learn <em>long-term dependencies</em> in a sequence which means it can take the entire context into account when making a prediction, whether that be the next word in a sentence, a sentiment classification, or the next temperature measurement. A RNN is designed to mimic the human way of processing sequences: we consider the <em>entire sentence when forming a response instead of words by themselves.</em> For example, consider the following sentence:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">“The concert was boring for the first 15 minutes while the band warmed up but then was terribly exciting.”</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">A machine learning model that considers the words in isolation — such as a <a href="https://en.wikipedia.org/wiki/Bag-of-words_model" target="_self">bag of words model </a>— would probably conclude this sentence is negative. An RNN by contrast should be able to see the words “but” and “terribly exciting” and realize that the sentence turns from negative to positive because it has looked at the entire sequence. Reading a whole sequence gives us a context for processing its meaning, a concept encoded in recurrent neural networks.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">At the heart of an RNN is a layer made of memory cells. The most popular cell at the moment is the <a href="https://en.wikipedia.org/wiki/Long_short-term_memory" target="_self">Long Short-Term Memory</a> (LSTM) which maintains a cell state as well as a carry for ensuring that the signal (information in the <a href="https://stats.stackexchange.com/questions/185639/how-does-lstm-prevent-the-vanishing-gradient-problem" target="_self">form of a gradient</a>) is not lost as the sequence is processed. At each time step the LSTM considers the current word, the carry, and the cell state.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*esTGDR3kcDLaTEHKCBedTQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*esTGDR3kcDLaTEHKCBedTQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*esTGDR3kcDLaTEHKCBedTQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*esTGDR3kcDLaTEHKCBedTQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*esTGDR3kcDLaTEHKCBedTQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*esTGDR3kcDLaTEHKCBedTQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*esTGDR3kcDLaTEHKCBedTQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*esTGDR3kcDLaTEHKCBedTQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*esTGDR3kcDLaTEHKCBedTQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*esTGDR3kcDLaTEHKCBedTQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*esTGDR3kcDLaTEHKCBedTQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*esTGDR3kcDLaTEHKCBedTQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*esTGDR3kcDLaTEHKCBedTQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*esTGDR3kcDLaTEHKCBedTQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*esTGDR3kcDLaTEHKCBedTQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">LSTM (Long Short Term Memory) Cell (<a href="https://www.manning.com/books/deep-learning-with-python" target="_self">Source</a>)</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The LSTM has 3 different gates and weight vectors: there is a “forget” gate for discarding irrelevant information; an “input” gate for handling the current input, and an “output” gate for producing predictions at each time step. However, as Chollet points out, it is fruitless trying to assign specific meanings to each of the elements in the cell.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The function of each cell element is ultimately decided by the parameters (weights) which are learned during training. Feel free to label each cell part, but it’s not necessary for effective use! Recall, the benefit of a Recurrent Neural Network for<a href="https://machinelearningmastery.com/sequence-prediction-problems-learning-lstm-recurrent-neural-networks/" target="_self"> sequence learning</a> is it maintains a memory of the entire sequence preventing prior information from being lost.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Problem Formulation</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">There are several ways we can formulate the task of training an RNN to write text, in this case patent abstracts. However, we will choose to train it as a many-to-one sequence mapper. That is, we input a sequence of words and train the model to predict the very next word. The words will be mapped to integers and then to vectors using an embedding matrix (either pre-trained or trainable) before being passed into an LSTM layer.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">When we go to write a new patent, we pass in a starting sequence of words, make a prediction for the next word, update the input sequence, make another prediction, add the word to the sequence and continue for however many words we want to generate.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The steps of the approach are outlined below:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;">Convert abstracts from list of strings into list of lists of integers (sequences)</li><li class="ff3" style="font-size:22px;">Create feature and labels from sequences</li><li class="ff3" style="font-size:22px;">Build LSTM model with Embedding, LSTM, and Dense layers</li><li class="ff3" style="font-size:22px;">Load in pre-trained embeddings</li><li class="ff3" style="font-size:22px;">Train model to predict next work in sequence</li><li class="ff3" style="font-size:22px;">Make predictions by passing in starting sequence</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Keep in mind this is only one formulation of the problem: we could also use a character level model or make predictions for each word in the sequence. As with many concepts in machine learning, there is no one correct answer, but this approach works well in practice.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Data Preparation</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Even with a neural network’s powerful representation ability, getting a quality, clean dataset is paramount. The raw data for this project comes from <a href="http://www.patentsview.org/querydev/" target="_self">USPTO PatentsView</a>, where you can search for information on any patent applied for in the United States. I searched for the term “neural network” and downloaded the resulting patent abstracts — 3500 in all. I found it best to train on a narrow subject, but feel free to try with a different set of patents.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*pAEoYnnufAvTLuvo0TEZ3A.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*pAEoYnnufAvTLuvo0TEZ3A.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*pAEoYnnufAvTLuvo0TEZ3A.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*pAEoYnnufAvTLuvo0TEZ3A.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*pAEoYnnufAvTLuvo0TEZ3A.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*pAEoYnnufAvTLuvo0TEZ3A.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pAEoYnnufAvTLuvo0TEZ3A.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*pAEoYnnufAvTLuvo0TEZ3A.png 640w, https://miro.medium.com/v2/resize:fit:720/1*pAEoYnnufAvTLuvo0TEZ3A.png 720w, https://miro.medium.com/v2/resize:fit:750/1*pAEoYnnufAvTLuvo0TEZ3A.png 750w, https://miro.medium.com/v2/resize:fit:786/1*pAEoYnnufAvTLuvo0TEZ3A.png 786w, https://miro.medium.com/v2/resize:fit:828/1*pAEoYnnufAvTLuvo0TEZ3A.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*pAEoYnnufAvTLuvo0TEZ3A.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*pAEoYnnufAvTLuvo0TEZ3A.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*pAEoYnnufAvTLuvo0TEZ3A.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Patent Abstract Data</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We’ll start out with the patent abstracts as a list of strings. The main data preparation steps for our model are:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;">Remove punctuation and split strings into lists of individual words</li><li class="ff3" style="font-size:22px;">Convert the individual words into integers</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">These two steps can both be done using the <a href="https://keras.io/preprocessing/text/#tokenizer" target="_self">Keras </a><code>Tokenizer</code> class. By default, this removes all punctuation, lowercases words, and then converts words to <code>sequences</code> of integers. A <code>Tokenizer</code> is first <code>fit</code> on a list of strings and then converts this list into a list of lists of <em>integers</em>. This is demonstrated below:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:87%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 643px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*j9tc6cDGzHfBP6WS2ZGeug.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*j9tc6cDGzHfBP6WS2ZGeug.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*j9tc6cDGzHfBP6WS2ZGeug.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*j9tc6cDGzHfBP6WS2ZGeug.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*j9tc6cDGzHfBP6WS2ZGeug.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*j9tc6cDGzHfBP6WS2ZGeug.png 1100w, https://miro.medium.com/v2/resize:fit:1286/format:webp/1*j9tc6cDGzHfBP6WS2ZGeug.png 1286w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 643px" srcset="https://miro.medium.com/v2/resize:fit:640/1*j9tc6cDGzHfBP6WS2ZGeug.png 640w, https://miro.medium.com/v2/resize:fit:720/1*j9tc6cDGzHfBP6WS2ZGeug.png 720w, https://miro.medium.com/v2/resize:fit:750/1*j9tc6cDGzHfBP6WS2ZGeug.png 750w, https://miro.medium.com/v2/resize:fit:786/1*j9tc6cDGzHfBP6WS2ZGeug.png 786w, https://miro.medium.com/v2/resize:fit:828/1*j9tc6cDGzHfBP6WS2ZGeug.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*j9tc6cDGzHfBP6WS2ZGeug.png 1100w, https://miro.medium.com/v2/resize:fit:1286/1*j9tc6cDGzHfBP6WS2ZGeug.png 1286w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/643/1*j9tc6cDGzHfBP6WS2ZGeug.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The output of the first cell shows the original abstract and the output of the second the tokenized sequence. Each abstract is now represented as integers.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We can use the <code>idx_word</code> attribute of the trained tokenizer to figure out what each of these integers means:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:88%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 646px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*5tQCBYgLcH8-kAhLTsh05A.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*5tQCBYgLcH8-kAhLTsh05A.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*5tQCBYgLcH8-kAhLTsh05A.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*5tQCBYgLcH8-kAhLTsh05A.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*5tQCBYgLcH8-kAhLTsh05A.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*5tQCBYgLcH8-kAhLTsh05A.png 1100w, https://miro.medium.com/v2/resize:fit:1292/format:webp/1*5tQCBYgLcH8-kAhLTsh05A.png 1292w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 646px" srcset="https://miro.medium.com/v2/resize:fit:640/1*5tQCBYgLcH8-kAhLTsh05A.png 640w, https://miro.medium.com/v2/resize:fit:720/1*5tQCBYgLcH8-kAhLTsh05A.png 720w, https://miro.medium.com/v2/resize:fit:750/1*5tQCBYgLcH8-kAhLTsh05A.png 750w, https://miro.medium.com/v2/resize:fit:786/1*5tQCBYgLcH8-kAhLTsh05A.png 786w, https://miro.medium.com/v2/resize:fit:828/1*5tQCBYgLcH8-kAhLTsh05A.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*5tQCBYgLcH8-kAhLTsh05A.png 1100w, https://miro.medium.com/v2/resize:fit:1292/1*5tQCBYgLcH8-kAhLTsh05A.png 1292w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/646/1*5tQCBYgLcH8-kAhLTsh05A.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you look closely, you’ll notice that the <code>Tokenizer</code> has removed all punctuation and lowercased all the words. If we use these settings, then the neural network will not learn proper English! We can adjust this by changing the filters to the <code>Tokenizer</code> to not remove punctuation.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span># Don't remove punctuation or uppercase<br/>tokenizer = Tokenizer(num_words=None, <br/>                     filters='#$%&amp;()*+-&lt;=&gt;@[\\]^_`{|}~\t\n',<br/>                     lower = False, split = ' ')</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">See the notebooks for different implementations, but, when we use pre-trained embeddings, we’ll have to remove the uppercase because there are no lowercase letters in the embeddings. When training our own embeddings, we don’t have to worry about this because the model will learn different representations for lower and upper case.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Features and Labels</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The previous step converts all the abstracts to sequences of integers. The next step is to create a supervised machine learning problem with which to train the network. There are numerous ways you can set up a recurrent neural network task for text generation, but we’ll use the following:</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4">Give the network a sequence of words and train it to predict the next word.</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The number of words is left as a parameter; we’ll use 50 for the examples shown here which means we give our network 50 words and train it to predict the 51st. Other ways of training the network would be to have it predict the next word at each point in the sequence — make a prediction for each input word rather than once for the entire sequence — or train the model using individual characters. The implementation used here is not necessarily optimal — there is no accepted best solution —<em> but it works well</em>!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Creating the features and labels is relatively simple and for each abstract (represented as integers) we create multiple sets of features and labels. We use the first 50 words as features with the 51st as the label, then use words 2–51 as features and predict the 52nd and so on. This gives us significantly more training data which is beneficial because the <a href="https://research.google.com/pubs/archive/35179.pdf" target="_self">performance of the network is proportional to the amount of data</a> that it sees during training.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The implementation of creating features and labels is below:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre>features = []
labels = []

training_length = 50

# Iterate through the sequences of tokens
for seq in sequences:

    # Create multiple training examples from each sequence
    for i in range(training_length, len(seq)):
        
        # Extract the features and label
        extract = seq[i - training_length:i + 1]

        # Set the features and label
        features.append(extract[:-1])
        labels.append(extract[-1])
        
features = np.array(features)</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The features end up with shape <code>(296866, 50)</code> which means we have almost 300,000 sequences each with 50 tokens. In the language of recurrent neural networks, each sequence has 50 <em>timesteps </em>each with 1 feature.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We could leave the labels as integers, but a neural network is able to train most effectively when the labels are one-hot encoded. We can one-hot encode the labels with <code>numpy</code> very quickly using the following:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:88%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 644px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*INCF26TfjyH51lAhB8yC0g.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*INCF26TfjyH51lAhB8yC0g.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*INCF26TfjyH51lAhB8yC0g.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*INCF26TfjyH51lAhB8yC0g.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*INCF26TfjyH51lAhB8yC0g.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*INCF26TfjyH51lAhB8yC0g.png 1100w, https://miro.medium.com/v2/resize:fit:1288/format:webp/1*INCF26TfjyH51lAhB8yC0g.png 1288w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 644px" srcset="https://miro.medium.com/v2/resize:fit:640/1*INCF26TfjyH51lAhB8yC0g.png 640w, https://miro.medium.com/v2/resize:fit:720/1*INCF26TfjyH51lAhB8yC0g.png 720w, https://miro.medium.com/v2/resize:fit:750/1*INCF26TfjyH51lAhB8yC0g.png 750w, https://miro.medium.com/v2/resize:fit:786/1*INCF26TfjyH51lAhB8yC0g.png 786w, https://miro.medium.com/v2/resize:fit:828/1*INCF26TfjyH51lAhB8yC0g.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*INCF26TfjyH51lAhB8yC0g.png 1100w, https://miro.medium.com/v2/resize:fit:1288/1*INCF26TfjyH51lAhB8yC0g.png 1288w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/644/1*INCF26TfjyH51lAhB8yC0g.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To find the word corresponding to a row in <code>label_array</code> , we use:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:40%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 308px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*jvr7zW3rql-ekRe-Vjz4Mg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*jvr7zW3rql-ekRe-Vjz4Mg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*jvr7zW3rql-ekRe-Vjz4Mg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*jvr7zW3rql-ekRe-Vjz4Mg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*jvr7zW3rql-ekRe-Vjz4Mg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*jvr7zW3rql-ekRe-Vjz4Mg.png 1100w, https://miro.medium.com/v2/resize:fit:616/format:webp/1*jvr7zW3rql-ekRe-Vjz4Mg.png 616w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 308px" srcset="https://miro.medium.com/v2/resize:fit:640/1*jvr7zW3rql-ekRe-Vjz4Mg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*jvr7zW3rql-ekRe-Vjz4Mg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*jvr7zW3rql-ekRe-Vjz4Mg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*jvr7zW3rql-ekRe-Vjz4Mg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*jvr7zW3rql-ekRe-Vjz4Mg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*jvr7zW3rql-ekRe-Vjz4Mg.png 1100w, https://miro.medium.com/v2/resize:fit:616/1*jvr7zW3rql-ekRe-Vjz4Mg.png 616w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/308/1*jvr7zW3rql-ekRe-Vjz4Mg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">After getting all of our features and labels properly formatted, we want to split them into a training and validation set (see notebook for details). One important point here is to shuffle the features and labels simultaneously so the same abstracts do not all end up in one set.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Building a Recurrent Neural Network</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><a href="http://keras.io" target="_self">Keras </a>is an incredible library: it allows us to build state-of-the-art models in a few lines of understandable Python code. Although <a href="https://deepsense.ai/keras-or-pytorch/" target="_self">other neural network libraries may be faster or allow more flexibility</a>, nothing can beat Keras for development time and ease-of-use.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The code for a simple LSTM is below with an explanation following:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre>from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout, Masking, Embedding

model = Sequential()

# Embedding layer
model.add(
    Embedding(input_dim=num_words,
              input_length = training_length,
              output_dim=100,
              weights=[embedding_matrix],
              trainable=False,
              mask_zero=True))

# Masking layer for pre-trained embeddings
model.add(Masking(mask_value=0.0))

# Recurrent layer
model.add(LSTM(64, return_sequences=False, 
               dropout=0.1, recurrent_dropout=0.1))

# Fully connected layer
model.add(Dense(64, activation='relu'))

# Dropout for regularization
model.add(Dropout(0.5))

# Output layer
model.add(Dense(num_words, activation='softmax'))

# Compile the model
model.compile(
    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We are using the Keras <code>Sequential</code> API which means we build the network up one layer at a time. The layers are as follows:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">An <code>Embedding</code> which maps each input word to a 100-dimensional vector. The embedding can use pre-trained weights (more in a second) which we supply in the <code>weights</code> parameter. <code>trainable</code> can be set <code>False</code> if we don’t want to update the embeddings.</li><li class="ff3" style="font-size:22px;">A <code>Masking</code> layer to mask any words that do not have a pre-trained embedding which will be represented as all zeros. This layer should not be used when training the embeddings.</li><li class="ff3" style="font-size:22px;">The heart of the network: a layer of <code>LSTM </code>cells with <a href="https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/" target="_self">dropout to prevent overfitting</a>. Since we are only using one LSTM layer, it <em>does not</em> return the sequences, for using two or more layers, make sure to return sequences.</li><li class="ff3" style="font-size:22px;">A fully-connected <code>Dense</code> layer with <code>relu</code> activation. This adds additional representational capacity to the network.</li><li class="ff3" style="font-size:22px;">A <code>Dropout</code> layer to prevent overfitting to the training data.</li><li class="ff3" style="font-size:22px;">A <code>Dense</code> fully-connected output layer. This produces a probability for every word in the vocab using <code>softmax</code> activation.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The model is compiled with the <code>Adam</code> optimizer (a variant on Stochastic Gradient Descent) and trained using the <code>categorical_crossentropy</code> loss. During training, the network will try to minimize the log loss by adjusting the trainable parameters (weights). As always, the gradients of the parameters are calculated using <a href="http://neuralnetworksanddeeplearning.com/chap2.html" target="_self">back-propagation</a> and updated with the optimizer. Since we are using Keras, we <a href="https://machinelearningmastery.com/5-step-life-cycle-neural-network-models-keras/" target="_self">don’t have to worry about how this happens</a> behind the scenes, only about setting up the network correctly.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:73%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 541px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*MAy6t2V08M-M5ZKB_9oRsQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*MAy6t2V08M-M5ZKB_9oRsQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*MAy6t2V08M-M5ZKB_9oRsQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*MAy6t2V08M-M5ZKB_9oRsQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*MAy6t2V08M-M5ZKB_9oRsQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*MAy6t2V08M-M5ZKB_9oRsQ.png 1100w, https://miro.medium.com/v2/resize:fit:1082/format:webp/1*MAy6t2V08M-M5ZKB_9oRsQ.png 1082w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 541px" srcset="https://miro.medium.com/v2/resize:fit:640/1*MAy6t2V08M-M5ZKB_9oRsQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*MAy6t2V08M-M5ZKB_9oRsQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*MAy6t2V08M-M5ZKB_9oRsQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*MAy6t2V08M-M5ZKB_9oRsQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*MAy6t2V08M-M5ZKB_9oRsQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*MAy6t2V08M-M5ZKB_9oRsQ.png 1100w, https://miro.medium.com/v2/resize:fit:1082/1*MAy6t2V08M-M5ZKB_9oRsQ.png 1082w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/541/1*MAy6t2V08M-M5ZKB_9oRsQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">LSTM network layout.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Without updating the embeddings, there are many fewer parameters to train in the network. <a href="https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/" target="_self">The input to the </a><code>LSTM</code><a href="https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/" target="_self"> layer is </a><code>(None, 50, 100)</code><a href="https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/" target="_self"> which means </a>that for each batch (the first dimension), each sequence has 50 timesteps (words), each of which has 100 features after embedding. Input to an LSTM layer always has the <code>(batch_size, timesteps, features)</code> shape.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">There are many ways to structure this network and there are several others covered in <a href="https://github.com/WillKoehrsen/recurrent-neural-networks/blob/master/notebooks/Deep%20Dive%20into%20Recurrent%20Neural%20Networks.ipynb" target="_self">the notebook</a>. For example, we can use two <code>LSTM</code> layers stacked on each other, a <code>Bidirectional LSTM</code> layer that processes sequences from both directions, or more <code>Dense</code> layers. I found the set-up above to work well.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Pre-Trained Embeddings</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Once the network is built, we still have to supply it with the pre-trained word embeddings. There are <a href="http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/" target="_self">numerous embeddings you can find online</a> trained on different corpuses (large bodies of text). The ones we’ll use are <a href="https://nlp.stanford.edu/data/" target="_self">available from Stanford</a> and come in 100, 200, or 300 dimensions (we’ll stick to 100). These embeddings are from the <a href="https://nlp.stanford.edu/projects/glove/" target="_self">GloVe (Global Vectors for Word Representation) </a>algorithm and were trained on Wikipedia.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Even though the pre-trained embeddings contain 400,000 words, there are some words in our vocab that are included. When we represent these words with embeddings, they will have 100-d vectors of all zeros. This problem can be overcome by training our own embeddings or by setting the <code>Embedding</code> layer's <code>trainable</code> parameter to <code>True</code> (and removing the <code>Masking</code> layer).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We can quickly load in the pre-trained embeddings from disk and make an embedding matrix with the following code:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre># Load in embeddings
glove_vectors = '/home/ubuntu/.keras/datasets/glove.6B.100d.txt'
glove = np.loadtxt(glove_vectors, dtype='str', comments=None)

# Extract the vectors and words
vectors = glove[:, 1:].astype('float')
words = glove[:, 0]

# Create lookup of words to vectors
word_lookup = {word: vector for word, vector in zip(words, vectors)}

# New matrix to hold word embeddings
embedding_matrix = np.zeros((num_words, vectors.shape[1]))

for i, word in enumerate(word_idx.keys()):
    # Look up the word embedding
    vector = word_lookup.get(word, None)

    # Record in matrix
    if vector is not None:
        embedding_matrix[i + 1, :] = vector</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">What this does is assign a 100-dimensional vector to each word in the vocab. If the word has no pre-trained embedding then this vector will be all zeros.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:79%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 585px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*re5ejOCTVKlwmxu7xA4n2g.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*re5ejOCTVKlwmxu7xA4n2g.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*re5ejOCTVKlwmxu7xA4n2g.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*re5ejOCTVKlwmxu7xA4n2g.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*re5ejOCTVKlwmxu7xA4n2g.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*re5ejOCTVKlwmxu7xA4n2g.png 1100w, https://miro.medium.com/v2/resize:fit:1170/format:webp/1*re5ejOCTVKlwmxu7xA4n2g.png 1170w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 585px" srcset="https://miro.medium.com/v2/resize:fit:640/1*re5ejOCTVKlwmxu7xA4n2g.png 640w, https://miro.medium.com/v2/resize:fit:720/1*re5ejOCTVKlwmxu7xA4n2g.png 720w, https://miro.medium.com/v2/resize:fit:750/1*re5ejOCTVKlwmxu7xA4n2g.png 750w, https://miro.medium.com/v2/resize:fit:786/1*re5ejOCTVKlwmxu7xA4n2g.png 786w, https://miro.medium.com/v2/resize:fit:828/1*re5ejOCTVKlwmxu7xA4n2g.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*re5ejOCTVKlwmxu7xA4n2g.png 1100w, https://miro.medium.com/v2/resize:fit:1170/1*re5ejOCTVKlwmxu7xA4n2g.png 1170w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/585/1*re5ejOCTVKlwmxu7xA4n2g.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To explore the embeddings, we can use the cosine similarity to find the words closest to a given query word in the embedding space:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:51%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 387px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*nml4qX3uOtk5NO-6aVU54g.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*nml4qX3uOtk5NO-6aVU54g.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*nml4qX3uOtk5NO-6aVU54g.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*nml4qX3uOtk5NO-6aVU54g.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*nml4qX3uOtk5NO-6aVU54g.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*nml4qX3uOtk5NO-6aVU54g.png 1100w, https://miro.medium.com/v2/resize:fit:774/format:webp/1*nml4qX3uOtk5NO-6aVU54g.png 774w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 387px" srcset="https://miro.medium.com/v2/resize:fit:640/1*nml4qX3uOtk5NO-6aVU54g.png 640w, https://miro.medium.com/v2/resize:fit:720/1*nml4qX3uOtk5NO-6aVU54g.png 720w, https://miro.medium.com/v2/resize:fit:750/1*nml4qX3uOtk5NO-6aVU54g.png 750w, https://miro.medium.com/v2/resize:fit:786/1*nml4qX3uOtk5NO-6aVU54g.png 786w, https://miro.medium.com/v2/resize:fit:828/1*nml4qX3uOtk5NO-6aVU54g.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*nml4qX3uOtk5NO-6aVU54g.png 1100w, https://miro.medium.com/v2/resize:fit:774/1*nml4qX3uOtk5NO-6aVU54g.png 774w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/387/1*nml4qX3uOtk5NO-6aVU54g.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><a href="https://medium.com/neural-network-embeddings-explained-4d028e6f0526" target="_self">Embeddings are learned</a> which means the representations apply specifically to one task. When using pre-trained embeddings, we hope the task the embeddings were learned on is close enough to our task so the embeddings are meaningful. If these embeddings were trained on tweets, we might not expect them to work well, but since they were trained on Wikipedia data, they should be generally applicable to a range of language processing tasks.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you have a lot of data and the computer time, it’s usually better to learn your own embeddings for a specific task. In the notebook I take both approaches and the learned embeddings perform slightly better.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Training the Model</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">With the training and validation data prepared, the network built, and the embeddings loaded, we are almost ready for our model to learn how to write patent abstracts. However, good steps to take when training neural networks are to use <a href="https://keras.io/callbacks/" target="_self">ModelCheckpoint and EarlyStopping in the form of Keras callbacks</a>:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Model Checkpoint: saves the best model (as measured by validation loss) on disk for using best model</li><li class="ff3" style="font-size:22px;">Early Stopping: halts training when validation loss is no longer decreasing</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Using <a href="https://stats.stackexchange.com/questions/231061/how-to-use-early-stopping-properly-for-training-deep-neural-network" target="_self">Early Stopping</a> means we won’t overfit to the training data and waste time training for extra epochs that don’t improve performance. The Model Checkpoint means we can access the best model and, if our training is disrupted 1000 epochs in, we won’t have lost all the progress!</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre>from keras.callbacks import EarlyStopping, ModelCheckpoint

# Create callbacks
callbacks = [EarlyStopping(monitor='val_loss', patience=5),
             ModelCheckpoint('../models/model.h5'), save_best_only=True, 
                             save_weights_only=False)]</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The model can then be trained with the following code:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre>history = model.fit(X_train,  y_train, 
                    batch_size=2048, epochs=150,
                    callbacks=callbacks,
                    validation_data=(X_valid, y_valid))</pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">On an <a href="https://aws.amazon.com/ec2/instance-types/p2/" target="_self">Amazon p2.xlarge instance</a> ($0.90 / hour reserved), this took just over 1 hour to finish. Once the training is done, we can load back in the best saved model and evaluate a final time on the validation data.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>from keras import load_model</span><span># Load in model and evaluate on validation data<br/>model = load_model('../models/model.h5')<br/>model.evaluate(X_valid, y_valid)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Overall, the model using pre-trained word embeddings achieved a validation accuracy of 23.9%. This is pretty good considering as a human I find it extremely difficult to predict the next word in these abstracts! A naive guess of the most common word (“the”) yields an accuracy around 8%. The metrics for all the models in the notebook are shown below:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*I1Cjf7ZATHDz5A0UxpobQA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*I1Cjf7ZATHDz5A0UxpobQA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*I1Cjf7ZATHDz5A0UxpobQA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*I1Cjf7ZATHDz5A0UxpobQA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*I1Cjf7ZATHDz5A0UxpobQA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*I1Cjf7ZATHDz5A0UxpobQA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I1Cjf7ZATHDz5A0UxpobQA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*I1Cjf7ZATHDz5A0UxpobQA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*I1Cjf7ZATHDz5A0UxpobQA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*I1Cjf7ZATHDz5A0UxpobQA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*I1Cjf7ZATHDz5A0UxpobQA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*I1Cjf7ZATHDz5A0UxpobQA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*I1Cjf7ZATHDz5A0UxpobQA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*I1Cjf7ZATHDz5A0UxpobQA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*I1Cjf7ZATHDz5A0UxpobQA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The best model used pre-trained embeddings and the same architecture as shown above. I’d encourage anyone to try training with a different model!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Patent Abstract Generation</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Of course, while high metrics are nice, what matters is if the network can produce reasonable patent abstracts. Using the best model we can explore the model generation ability. If you want to run this on your own hardware, you can find the <a href="https://github.com/WillKoehrsen/recurrent-neural-networks/blob/master/notebooks/Exploring%20Model%20Results.ipynb" target="_self">notebook here</a> and the <a href="https://github.com/WillKoehrsen/recurrent-neural-networks/tree/master/models" target="_self">pre-trained models</a> are on GitHub.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To produce output, we seed the network with a random sequence chosen from the patent abstracts, have it make a prediction of the next word, add the prediction to the sequence, and continue making predictions for however many words we want. Some results are shown below:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*kQlyQ45s2Turn925Qo4Qrw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*kQlyQ45s2Turn925Qo4Qrw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*kQlyQ45s2Turn925Qo4Qrw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*kQlyQ45s2Turn925Qo4Qrw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*kQlyQ45s2Turn925Qo4Qrw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*kQlyQ45s2Turn925Qo4Qrw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kQlyQ45s2Turn925Qo4Qrw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*kQlyQ45s2Turn925Qo4Qrw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*kQlyQ45s2Turn925Qo4Qrw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*kQlyQ45s2Turn925Qo4Qrw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*kQlyQ45s2Turn925Qo4Qrw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*kQlyQ45s2Turn925Qo4Qrw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*kQlyQ45s2Turn925Qo4Qrw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*kQlyQ45s2Turn925Qo4Qrw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*kQlyQ45s2Turn925Qo4Qrw.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">One important parameter for the output is the <em>diversity</em> of <a href="https://medium.com/machine-learning-at-petiteprogrammer/sampling-strategies-for-recurrent-neural-networks-9aea02a6616f" target="_self">the predictions</a>. Instead of using the predicted word with the highest probability, we inject diversity into the predictions and then choose the next word with a probability proportional to the more diverse predictions. Too high a diversity and the generated output starts to seem random, but too low and the network can get into recursive loops of output.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*oX-d7QmH5sAAhyUuEZTn5Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*oX-d7QmH5sAAhyUuEZTn5Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*oX-d7QmH5sAAhyUuEZTn5Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*oX-d7QmH5sAAhyUuEZTn5Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*oX-d7QmH5sAAhyUuEZTn5Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*oX-d7QmH5sAAhyUuEZTn5Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oX-d7QmH5sAAhyUuEZTn5Q.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*oX-d7QmH5sAAhyUuEZTn5Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*oX-d7QmH5sAAhyUuEZTn5Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*oX-d7QmH5sAAhyUuEZTn5Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*oX-d7QmH5sAAhyUuEZTn5Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*oX-d7QmH5sAAhyUuEZTn5Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*oX-d7QmH5sAAhyUuEZTn5Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*oX-d7QmH5sAAhyUuEZTn5Q.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*oX-d7QmH5sAAhyUuEZTn5Q.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The output isn’t too bad! Some of the time it’s tough to determine which is computer generated and which is from a machine. Part of this is due to the <a href="http://www.wipo.int/standards/en/pdf/03-12-a.pdf" target="_self">nature of patent abstracts</a> which, most of the time, don’t sound like they were written by a human.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Another use of the network is to seed it with our own starting sequence. We can use any text we want and see where the network takes it:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*RzwRgUxCh9z2NuMltAoPFg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*RzwRgUxCh9z2NuMltAoPFg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*RzwRgUxCh9z2NuMltAoPFg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*RzwRgUxCh9z2NuMltAoPFg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*RzwRgUxCh9z2NuMltAoPFg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*RzwRgUxCh9z2NuMltAoPFg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RzwRgUxCh9z2NuMltAoPFg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*RzwRgUxCh9z2NuMltAoPFg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*RzwRgUxCh9z2NuMltAoPFg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*RzwRgUxCh9z2NuMltAoPFg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*RzwRgUxCh9z2NuMltAoPFg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*RzwRgUxCh9z2NuMltAoPFg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*RzwRgUxCh9z2NuMltAoPFg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*RzwRgUxCh9z2NuMltAoPFg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*RzwRgUxCh9z2NuMltAoPFg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Again, the results are not entirely believable but they do resemble English.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Human or Machine?</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">As a final test of the recurrent neural network, I created a game to guess whether the model or a human generated the output. Here’s the first example where two of the options are from a computer and one is from a human:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*ujviMus1Gb1LGVLEEjXmFw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*ujviMus1Gb1LGVLEEjXmFw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*ujviMus1Gb1LGVLEEjXmFw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*ujviMus1Gb1LGVLEEjXmFw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*ujviMus1Gb1LGVLEEjXmFw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ujviMus1Gb1LGVLEEjXmFw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ujviMus1Gb1LGVLEEjXmFw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*ujviMus1Gb1LGVLEEjXmFw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*ujviMus1Gb1LGVLEEjXmFw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*ujviMus1Gb1LGVLEEjXmFw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*ujviMus1Gb1LGVLEEjXmFw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*ujviMus1Gb1LGVLEEjXmFw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*ujviMus1Gb1LGVLEEjXmFw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*ujviMus1Gb1LGVLEEjXmFw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*ujviMus1Gb1LGVLEEjXmFw.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">What’s your guess? The answer is that the <em>second</em> is the actual abstract written by a person (well, it’s what was actually in the abstract. I’m not sure these abstracts are written by people). Here’s another one:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*-v864Qni74z-su4zmJQcNA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*-v864Qni74z-su4zmJQcNA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*-v864Qni74z-su4zmJQcNA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*-v864Qni74z-su4zmJQcNA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*-v864Qni74z-su4zmJQcNA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*-v864Qni74z-su4zmJQcNA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-v864Qni74z-su4zmJQcNA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*-v864Qni74z-su4zmJQcNA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*-v864Qni74z-su4zmJQcNA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*-v864Qni74z-su4zmJQcNA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*-v864Qni74z-su4zmJQcNA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*-v864Qni74z-su4zmJQcNA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*-v864Qni74z-su4zmJQcNA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*-v864Qni74z-su4zmJQcNA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*-v864Qni74z-su4zmJQcNA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This time the <em>third</em> had a flesh and blood writer.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">There are additional steps we can use to interpret the model such as finding which neurons light up with different input sequences. We can also look at the learned embeddings (or visualize them with the <a href="https://projector.tensorflow.org" target="_self">Projector tool</a>). We’ll leave those topics for another time, and conclude that we know now <em>how to implement a recurrent neural network to effectively mimic human text.</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Conclusions</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">It’s important to recognize that the recurrent neural network has no concept of language understanding. It is effectively a very sophisticated pattern recognition machine. Nonetheless, unlike methods such as Markov chains or frequency analysis, the rnn makes predictions based on the <em>ordering of elements</em> in the sequence. Getting a little philosophical here, <a href="https://bigthink.com/endless-innovation/humans-are-the-worlds-best-pattern-recognition-machines-but-for-how-long" target="_self">you could argue that humans are simply extreme pattern recognition machines</a> and therefore the recurrent neural network is only acting like a human machine.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The uses of recurrent neural networks go far beyond text generation to <a href="https://machinelearningmastery.com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/" target="_self">machine translation</a>, <a href="https://cs.stanford.edu/people/karpathy/sfmltalk.pdf" target="_self">image captioning</a>, and <a href="https://arxiv.org/ftp/arxiv/papers/1506/1506.04891.pdf" target="_self">authorship identification</a>. Although this application we covered here will not displace any humans, it’s conceivable that with more training data and a larger model, a neural network would be able to synthesize new, reasonable patent abstracts.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*r0SdCL5x2-ufHTWpfrx2EQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*r0SdCL5x2-ufHTWpfrx2EQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*r0SdCL5x2-ufHTWpfrx2EQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*r0SdCL5x2-ufHTWpfrx2EQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*r0SdCL5x2-ufHTWpfrx2EQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*r0SdCL5x2-ufHTWpfrx2EQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r0SdCL5x2-ufHTWpfrx2EQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*r0SdCL5x2-ufHTWpfrx2EQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*r0SdCL5x2-ufHTWpfrx2EQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*r0SdCL5x2-ufHTWpfrx2EQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*r0SdCL5x2-ufHTWpfrx2EQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*r0SdCL5x2-ufHTWpfrx2EQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*r0SdCL5x2-ufHTWpfrx2EQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*r0SdCL5x2-ufHTWpfrx2EQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*r0SdCL5x2-ufHTWpfrx2EQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">A Bi-Directional LSTM Cell (<a href="https://developer.nvidia.com/discover/lstm" target="_self">Source</a>)</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">It can be easy to get stuck in the details or the theory behind a complex technique, but a more effective method for learning data science tools is to <a href="https://github.com/DOsinga/deep_learning_cookbook" target="_self">dive in and build applications</a>. You can always go back later and catch up on the theory once you know what a technique is capable of and how it works in practice. Most of us won’t be designing neural networks, but it’s worth learning how to use them effectively. This means putting away the books, breaking out the keyboard, and coding up your very own network.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">As always, I welcome feedback and constructive criticism. I can be reached on Twitter <a href="http://twitter.com/@koehrsen_will" target="_self">@koehrsen_will</a> or through my website at <a href="https://willk.online" target="_self">willk.online</a>.</p></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>