<!DOCTYPE html>
                <html>
                <head>
                    <title>Neural Network Architectures</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://towardsdatascience.com/neural-network-architectures-156e5bad51ba"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://culurciello.medium.com/?source=post_page-----156e5bad51ba--------------------------------">Author : Eugenio Culurciello</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>Neural Network Architectures</h3></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Deep neural networks and Deep Learning are powerful and popular algorithms. And a lot of their success lays in the careful design of the neural network architecture.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I wanted to revisit the history of neural network design in the last few years and in the context of Deep Learning.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For a more in-depth analysis and comparison of all the networks reported here, please see our <a href="https://arxiv.org/abs/1605.07678" target="_self">recent article</a> (and <a href="https://medium.com/@culurciello/analysis-of-deep-neural-networks-dcf398e71aae" target="_self">updated post</a>). One representative figure from this article is here:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*n16lj3lSkz2miMc_5cvkrA.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*n16lj3lSkz2miMc_5cvkrA.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*n16lj3lSkz2miMc_5cvkrA.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*n16lj3lSkz2miMc_5cvkrA.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*n16lj3lSkz2miMc_5cvkrA.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*n16lj3lSkz2miMc_5cvkrA.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n16lj3lSkz2miMc_5cvkrA.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*n16lj3lSkz2miMc_5cvkrA.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*n16lj3lSkz2miMc_5cvkrA.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*n16lj3lSkz2miMc_5cvkrA.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*n16lj3lSkz2miMc_5cvkrA.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*n16lj3lSkz2miMc_5cvkrA.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*n16lj3lSkz2miMc_5cvkrA.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*n16lj3lSkz2miMc_5cvkrA.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*n16lj3lSkz2miMc_5cvkrA.jpeg"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Top1 vs. operations, size ∝ parameters. Top-1 one-crop accuracy versus amount of operations required for a single forward pass. See also <a href="https://medium.com/@culurciello/analysis-of-deep-neural-networks-dcf398e71aae" target="_self">here</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Reporting top-1 one-crop accuracy versus amount of operations required for a single forward pass in multiple popular neural network architectures.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">LeNet5</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">It is the year 1994, and this is one of the very first convolutional neural networks, and what propelled the field of Deep Learning. This pioneering work by Yann LeCun was named <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" target="_self">LeNet5</a> after many previous successful iterations since the year 1988!</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*V1vb9SDnsU1eZQUy.jpg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*V1vb9SDnsU1eZQUy.jpg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*V1vb9SDnsU1eZQUy.jpg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*V1vb9SDnsU1eZQUy.jpg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*V1vb9SDnsU1eZQUy.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*V1vb9SDnsU1eZQUy.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*V1vb9SDnsU1eZQUy.jpg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*V1vb9SDnsU1eZQUy.jpg 640w, https://miro.medium.com/v2/resize:fit:720/0*V1vb9SDnsU1eZQUy.jpg 720w, https://miro.medium.com/v2/resize:fit:750/0*V1vb9SDnsU1eZQUy.jpg 750w, https://miro.medium.com/v2/resize:fit:786/0*V1vb9SDnsU1eZQUy.jpg 786w, https://miro.medium.com/v2/resize:fit:828/0*V1vb9SDnsU1eZQUy.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/0*V1vb9SDnsU1eZQUy.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/0*V1vb9SDnsU1eZQUy.jpg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*V1vb9SDnsU1eZQUy.jpg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The LeNet5 architecture was fundamental, in particular the insight that image features are distributed across the entire image, and convolutions with learnable parameters are an effective way to extract similar features at multiple location with few parameters. At the time there was no GPU to help training, and even CPUs were slow. Therefore being able to save parameters and computation was a key advantage. This is in contrast to using each pixel as a separate input of a large multi-layer neural network. LeNet5 explained that those should not be used in the first layer, because images are highly spatially correlated, and using individual pixel of the image as separate input features would not take advantage of these correlations.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">LeNet5 features can be summarized as:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">convolutional neural network use sequence of 3 layers: convolution, pooling, non-linearity –&gt; This may be the key feature of Deep Learning for images since this paper!</li><li class="ff3" style="font-size:22px;">use convolution to extract spatial features</li><li class="ff3" style="font-size:22px;">subsample using spatial average of maps</li><li class="ff3" style="font-size:22px;">non-linearity in the form of tanh or sigmoids</li><li class="ff3" style="font-size:22px;">multi-layer neural network (MLP) as final classifier</li><li class="ff3" style="font-size:22px;">sparse connection matrix between layers to avoid large computational cost</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In overall this network was the origin of much of the recent architectures, and a true inspiration for many people in the field.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">The gap</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In the years from 1998 to 2010 neural network were in incubation. Most people did not notice their increasing power, while many other researchers slowly progressed. More and more data was available because of the rise of cell-phone cameras and cheap digital cameras. And computing power was on the rise, CPUs were becoming faster, and GPUs became a general-purpose computing tool. Both of these trends made neural network progress, albeit at a slow rate. Both data and computing power made the tasks that neural networks tackled more and more interesting. And then it became clear…</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Dan Ciresan Net</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In 2010 Dan Claudiu Ciresan and Jurgen Schmidhuber published one of the very fist implementations of <a href="http://arxiv.org/abs/1003.0358" target="_self">GPU Neural nets</a>. This implementation had both forward and backward implemented on a a <a href="http://www.geforce.com/hardware/desktop-gpus/geforce-gtx-280" target="_self">NVIDIA GTX 280</a> graphic processor of an up to 9 layers neural network.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">AlexNet</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In 2012, Alex Krizhevsky released <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_self">AlexNet</a> which was a deeper and much wider version of the LeNet and won by a large margin the difficult ImageNet competition.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:74%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 550px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*vsi8JJFV_O6Z34ks.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*vsi8JJFV_O6Z34ks.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*vsi8JJFV_O6Z34ks.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*vsi8JJFV_O6Z34ks.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*vsi8JJFV_O6Z34ks.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*vsi8JJFV_O6Z34ks.png 1100w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*vsi8JJFV_O6Z34ks.png 1100w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 550px" srcset="https://miro.medium.com/v2/resize:fit:640/0*vsi8JJFV_O6Z34ks.png 640w, https://miro.medium.com/v2/resize:fit:720/0*vsi8JJFV_O6Z34ks.png 720w, https://miro.medium.com/v2/resize:fit:750/0*vsi8JJFV_O6Z34ks.png 750w, https://miro.medium.com/v2/resize:fit:786/0*vsi8JJFV_O6Z34ks.png 786w, https://miro.medium.com/v2/resize:fit:828/0*vsi8JJFV_O6Z34ks.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*vsi8JJFV_O6Z34ks.png 1100w, https://miro.medium.com/v2/resize:fit:1100/0*vsi8JJFV_O6Z34ks.png 1100w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/550/0*vsi8JJFV_O6Z34ks.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">AlexNet scaled the insights of LeNet into a much larger neural network that could be used to learn much more complex objects and object hierarchies. The contribution of this work were:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">use of rectified linear units (ReLU) as non-linearities</li><li class="ff3" style="font-size:22px;"><mark>use of dropout technique to selectively ignore single neurons during training, a way to avoid overfitting of the model</mark></li><li class="ff3" style="font-size:22px;">overlapping max pooling, avoiding the averaging effects of average pooling</li><li class="ff3" style="font-size:22px;">use of GPUs <a href="http://www.geforce.com/hardware/desktop-gpus/geforce-gtx-580/specifications" target="_self">NVIDIA GTX 580</a> to reduce training time</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">At the time GPU offered a much larger number of cores than CPUs, and allowed 10x faster training time, which in turn allowed to use larger datasets and also bigger images.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The success of AlexNet started a small revolution. Convolutional neural network were now the workhorse of Deep Learning, which became the new name for “large neural networks that can now solve useful tasks”.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Overfeat</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In December 2013 the NYU lab from Yann LeCun came up with <a href="http://arxiv.org/abs/1312.6229" target="_self">Overfeat</a>, which is a derivative of AlexNet. The article also proposed learning bounding boxes, which later gave rise to many other papers on the same topic. I believe it is better to learn to segment objects rather than learn artificial bounding boxes.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">VGG</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The <a href="http://arxiv.org/abs/1409.1556" target="_self">VGG networks</a> from Oxford were the first to use much smaller 3×3 filters in each convolutional layers and also combined them as a sequence of convolutions.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This seems to be contrary to the principles of LeNet, where large convolutions were used to capture similar features in an image. Instead of the 9×9 or 11×11 filters of AlexNet, filters started to become smaller, too dangerously close to the infamous 1×1 convolutions that LeNet wanted to avoid, at least on the first layers of the network. But the great advantage of VGG was the insight that multiple 3×3 convolution in sequence can emulate the effect of larger receptive fields, for examples 5×5 and 7×7. These ideas will be also used in more recent network architectures as Inception and ResNet.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*HREIJ1hjF7z4y9Dd.jpg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*HREIJ1hjF7z4y9Dd.jpg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*HREIJ1hjF7z4y9Dd.jpg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*HREIJ1hjF7z4y9Dd.jpg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*HREIJ1hjF7z4y9Dd.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*HREIJ1hjF7z4y9Dd.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HREIJ1hjF7z4y9Dd.jpg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*HREIJ1hjF7z4y9Dd.jpg 640w, https://miro.medium.com/v2/resize:fit:720/0*HREIJ1hjF7z4y9Dd.jpg 720w, https://miro.medium.com/v2/resize:fit:750/0*HREIJ1hjF7z4y9Dd.jpg 750w, https://miro.medium.com/v2/resize:fit:786/0*HREIJ1hjF7z4y9Dd.jpg 786w, https://miro.medium.com/v2/resize:fit:828/0*HREIJ1hjF7z4y9Dd.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/0*HREIJ1hjF7z4y9Dd.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/0*HREIJ1hjF7z4y9Dd.jpg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*HREIJ1hjF7z4y9Dd.jpg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The VGG networks uses multiple 3x3 convolutional layers to represent complex features. Notice blocks 3, 4, 5 of VGG-E: 256×256 and 512×512 3×3 filters are used multiple times in sequence to extract more complex features and the combination of such features. This is effectively like having large 512×512 classifiers with 3 layers, which are convolutional! This obviously amounts to a massive number of parameters, and also learning power. But training of these network was difficult, and had to be split into smaller networks with layers added one by one. All this because of the lack of strong ways to regularize the model, or to somehow restrict the massive search space promoted by the large amount of parameters.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">VGG used large feature sizes in many layers and thus inference was quite <a href="http://arxiv.org/abs/1605.07678" target="_self">costly at run-time</a>. Reducing the number of features, as done in Inception bottlenecks, will save some of the computational cost.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Network-in-network</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><a href="https://arxiv.org/abs/1312.4400" target="_self">Network-in-network</a> (NiN) had the great and simple insight of using 1x1 convolutions to provide more combinational power to the features of a convolutional layers.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The NiN architecture used spatial MLP layers after each convolution, in order to better combine features before another layer. Again one can think the 1x1 convolutions are against the original principles of LeNet, but really they instead help to combine convolutional features in a better way, which is not possible by simply stacking more convolutional layers. This is different from using raw pixels as input to the next layer. Here 1×1 convolution are used to spatially combine features across features maps after convolution, so they effectively use very few parameters, shared across all pixels of these features!</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*JIa4PbbfSFqNedPW.jpg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*JIa4PbbfSFqNedPW.jpg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*JIa4PbbfSFqNedPW.jpg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*JIa4PbbfSFqNedPW.jpg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*JIa4PbbfSFqNedPW.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*JIa4PbbfSFqNedPW.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JIa4PbbfSFqNedPW.jpg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*JIa4PbbfSFqNedPW.jpg 640w, https://miro.medium.com/v2/resize:fit:720/0*JIa4PbbfSFqNedPW.jpg 720w, https://miro.medium.com/v2/resize:fit:750/0*JIa4PbbfSFqNedPW.jpg 750w, https://miro.medium.com/v2/resize:fit:786/0*JIa4PbbfSFqNedPW.jpg 786w, https://miro.medium.com/v2/resize:fit:828/0*JIa4PbbfSFqNedPW.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/0*JIa4PbbfSFqNedPW.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/0*JIa4PbbfSFqNedPW.jpg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*JIa4PbbfSFqNedPW.jpg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The power of MLP can greatly increase the effectiveness of individual convolutional features by combining them into more complex groups. This idea will be later used in most recent architectures as ResNet and Inception and derivatives.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">NiN also used an average pooling layer as part of the last classifier, another practice that will become common. This was done to average the response of the network to multiple are of the input image before classification.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">GoogLeNet and Inception</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Christian Szegedy from Google begun a quest aimed at reducing the computational burden of deep neural networks, and devised the <a href="https://arxiv.org/abs/1409.4842" target="_self">GoogLeNet the first Inception architecture</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">By now, Fall 2014, deep learning models were becoming extermely useful in categorizing the content of images and video frames. Most skeptics had given in that Deep Learning and neural nets came back to stay this time. Given the usefulness of these techniques, the internet giants like Google were very interested in efficient and large deployments of architectures on their server farms.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Christian thought a lot about ways to reduce the computational burden of deep neural nets while obtaining state-of-art performance (on ImageNet, for example). Or be able to keep the computational cost the same, while offering improved performance.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">He and his team came up with the Inception module:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:94%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 692px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*CJZdXZULMr_on1Ao.jpg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*CJZdXZULMr_on1Ao.jpg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*CJZdXZULMr_on1Ao.jpg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*CJZdXZULMr_on1Ao.jpg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*CJZdXZULMr_on1Ao.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*CJZdXZULMr_on1Ao.jpg 1100w, https://miro.medium.com/v2/resize:fit:1384/format:webp/0*CJZdXZULMr_on1Ao.jpg 1384w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 692px" srcset="https://miro.medium.com/v2/resize:fit:640/0*CJZdXZULMr_on1Ao.jpg 640w, https://miro.medium.com/v2/resize:fit:720/0*CJZdXZULMr_on1Ao.jpg 720w, https://miro.medium.com/v2/resize:fit:750/0*CJZdXZULMr_on1Ao.jpg 750w, https://miro.medium.com/v2/resize:fit:786/0*CJZdXZULMr_on1Ao.jpg 786w, https://miro.medium.com/v2/resize:fit:828/0*CJZdXZULMr_on1Ao.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/0*CJZdXZULMr_on1Ao.jpg 1100w, https://miro.medium.com/v2/resize:fit:1384/0*CJZdXZULMr_on1Ao.jpg 1384w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/692/0*CJZdXZULMr_on1Ao.jpg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">which at a first glance is basically the parallel combination of 1×1, 3×3, and 5×5 convolutional filters. But the great insight of the inception module was the use of 1×1 convolutional blocks (NiN) to reduce the number of features before the expensive parallel blocks. This is commonly referred as “bottleneck”. This deserves its own section to explain: see “bottleneck layer” section below.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">GoogLeNet used a stem without inception modules as initial layers, and an average pooling plus softmax classifier similar to NiN. This classifier is also extremely low number of operations, compared to the ones of AlexNet and VGG. This also contributed to a <a href="http://arxiv.org/abs/1605.07678" target="_self">very efficient network design</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Bottleneck layer</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Inspired by NiN, the bottleneck layer of Inception was reducing the number of features, and thus operations, at each layer, so the inference time could be kept low. Before passing data to the expensive convolution modules, the number of features was reduce by, say, 4 times. This led to large savings in computational cost, and the success of this architecture.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Let’s examine this in detail. Let’s say you have 256 features coming in, and 256 coming out, and let’s say the Inception layer only performs 3x3 convolutions. That is 256x256 x 3x3 convolutions that have to be performed (589,000s multiply-accumulate, or MAC operations). That may be more than the computational budget we have, say, to run this layer in 0.5 milli-seconds on a Google Server. Instead of doing this, we decide to reduce the number of features that will have to be convolved, say to 64 or 256/4. In this case, we first perform 256 -&gt; 64 1×1 convolutions, then 64 convolution on all Inception branches, and then we use again a 1x1 convolution from 64 -&gt; 256 features back again. The operations are now:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">256×64 × 1×1 = 16,000s</li><li class="ff3" style="font-size:22px;">64×64 × 3×3 = 36,000s</li><li class="ff3" style="font-size:22px;">64×256 × 1×1 = 16,000s</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For a total of about 70,000 versus the almost 600,000 we had before. Almost 10x less operations!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">And although we are doing less operations, we are not losing generality in this layer. In fact the bottleneck layers have been proven to perform at state-of-art on the ImageNet dataset, for example, and will be also used in later architectures such as ResNet.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The reason for the success is that the input features are correlated, and thus redundancy can be removed by combining them appropriately with the 1x1 convolutions. Then, after convolution with a smaller number of features, they can be expanded again into meaningful combination for the next layer.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Inception V3 (and V2)</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Christian and his team are very efficient researchers. In February 2015 <a href="http://arxiv.org/abs/1502.03167" target="_self">Batch-normalized Inception</a> was introduced as Inception V2. Batch-normalization computes the mean and standard-deviation of all feature maps at the output of a layer, and normalizes their responses with these values. This corresponds to “whitening” the data, and thus making all the neural maps have responses in the same range, and with zero mean. This helps training as the next layer does not have to learn offsets in the input data, and can focus on how to best combine features.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In December 2015 they released a <a href="http://arxiv.org/abs/1512.00567" target="_self">new version of the Inception modules and the corresponding architecture</a> This article better explains the original GoogLeNet architecture, giving a lot more detail on the design choices. A list of the original ideas are:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">maximize information flow into the network, by carefully constructing networks that balance depth and width. Before each pooling, increase the feature maps.</li><li class="ff3" style="font-size:22px;">when depth is increased, the number of features, or width of the layer is also increased systematically</li><li class="ff3" style="font-size:22px;">use width increase at each layer to increase the combination of features before next layer</li><li class="ff3" style="font-size:22px;">use only 3x3 convolution, when possible, given that filter of 5x5 and 7x7 can be decomposed with multiple 3x3. See figure:</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:72%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 538px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*FPt8z6-yKjzdob4E.jpg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*FPt8z6-yKjzdob4E.jpg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*FPt8z6-yKjzdob4E.jpg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*FPt8z6-yKjzdob4E.jpg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*FPt8z6-yKjzdob4E.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*FPt8z6-yKjzdob4E.jpg 1100w, https://miro.medium.com/v2/resize:fit:1076/format:webp/0*FPt8z6-yKjzdob4E.jpg 1076w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 538px" srcset="https://miro.medium.com/v2/resize:fit:640/0*FPt8z6-yKjzdob4E.jpg 640w, https://miro.medium.com/v2/resize:fit:720/0*FPt8z6-yKjzdob4E.jpg 720w, https://miro.medium.com/v2/resize:fit:750/0*FPt8z6-yKjzdob4E.jpg 750w, https://miro.medium.com/v2/resize:fit:786/0*FPt8z6-yKjzdob4E.jpg 786w, https://miro.medium.com/v2/resize:fit:828/0*FPt8z6-yKjzdob4E.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/0*FPt8z6-yKjzdob4E.jpg 1100w, https://miro.medium.com/v2/resize:fit:1076/0*FPt8z6-yKjzdob4E.jpg 1076w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/538/0*FPt8z6-yKjzdob4E.jpg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">the new inception module thus becomes:</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*Y9mKbwp1R8vAmT2L.jpg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*Y9mKbwp1R8vAmT2L.jpg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*Y9mKbwp1R8vAmT2L.jpg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*Y9mKbwp1R8vAmT2L.jpg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*Y9mKbwp1R8vAmT2L.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*Y9mKbwp1R8vAmT2L.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Y9mKbwp1R8vAmT2L.jpg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*Y9mKbwp1R8vAmT2L.jpg 640w, https://miro.medium.com/v2/resize:fit:720/0*Y9mKbwp1R8vAmT2L.jpg 720w, https://miro.medium.com/v2/resize:fit:750/0*Y9mKbwp1R8vAmT2L.jpg 750w, https://miro.medium.com/v2/resize:fit:786/0*Y9mKbwp1R8vAmT2L.jpg 786w, https://miro.medium.com/v2/resize:fit:828/0*Y9mKbwp1R8vAmT2L.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/0*Y9mKbwp1R8vAmT2L.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/0*Y9mKbwp1R8vAmT2L.jpg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*Y9mKbwp1R8vAmT2L.jpg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">filters can also be decomposed by <a href="http://arxiv.org/abs/1412.5474" target="_self">flattened convolutions</a> into more complex modules:</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*rRv_N9rLYJnmq6jz.jpg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*rRv_N9rLYJnmq6jz.jpg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*rRv_N9rLYJnmq6jz.jpg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*rRv_N9rLYJnmq6jz.jpg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*rRv_N9rLYJnmq6jz.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*rRv_N9rLYJnmq6jz.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rRv_N9rLYJnmq6jz.jpg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*rRv_N9rLYJnmq6jz.jpg 640w, https://miro.medium.com/v2/resize:fit:720/0*rRv_N9rLYJnmq6jz.jpg 720w, https://miro.medium.com/v2/resize:fit:750/0*rRv_N9rLYJnmq6jz.jpg 750w, https://miro.medium.com/v2/resize:fit:786/0*rRv_N9rLYJnmq6jz.jpg 786w, https://miro.medium.com/v2/resize:fit:828/0*rRv_N9rLYJnmq6jz.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/0*rRv_N9rLYJnmq6jz.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/0*rRv_N9rLYJnmq6jz.jpg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*rRv_N9rLYJnmq6jz.jpg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">inception modules can also decrease the size of the data by providing pooling while performing the inception computation. This is basically identical to performing a convolution with strides in parallel with a simple pooling layer:</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:58%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 438px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*rxf30_SJRsbFIFCW.jpg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*rxf30_SJRsbFIFCW.jpg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*rxf30_SJRsbFIFCW.jpg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*rxf30_SJRsbFIFCW.jpg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*rxf30_SJRsbFIFCW.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*rxf30_SJRsbFIFCW.jpg 1100w, https://miro.medium.com/v2/resize:fit:876/format:webp/0*rxf30_SJRsbFIFCW.jpg 876w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 438px" srcset="https://miro.medium.com/v2/resize:fit:640/0*rxf30_SJRsbFIFCW.jpg 640w, https://miro.medium.com/v2/resize:fit:720/0*rxf30_SJRsbFIFCW.jpg 720w, https://miro.medium.com/v2/resize:fit:750/0*rxf30_SJRsbFIFCW.jpg 750w, https://miro.medium.com/v2/resize:fit:786/0*rxf30_SJRsbFIFCW.jpg 786w, https://miro.medium.com/v2/resize:fit:828/0*rxf30_SJRsbFIFCW.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/0*rxf30_SJRsbFIFCW.jpg 1100w, https://miro.medium.com/v2/resize:fit:876/0*rxf30_SJRsbFIFCW.jpg 876w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/438/0*rxf30_SJRsbFIFCW.jpg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Inception still uses a pooling layer plus softmax as final classifier.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">ResNet</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The revolution then came in December 2015, at about the same time as Inception v3. <a href="https://arxiv.org/abs/1512.03385" target="_self">ResNet</a> have a simple ideas: feed the output of two successive convolutional layer AND also bypass the input to the next layers!</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:74%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 546px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*0r0vS8myiqyOb79L.jpg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*0r0vS8myiqyOb79L.jpg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*0r0vS8myiqyOb79L.jpg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*0r0vS8myiqyOb79L.jpg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*0r0vS8myiqyOb79L.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*0r0vS8myiqyOb79L.jpg 1100w, https://miro.medium.com/v2/resize:fit:1092/format:webp/0*0r0vS8myiqyOb79L.jpg 1092w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 546px" srcset="https://miro.medium.com/v2/resize:fit:640/0*0r0vS8myiqyOb79L.jpg 640w, https://miro.medium.com/v2/resize:fit:720/0*0r0vS8myiqyOb79L.jpg 720w, https://miro.medium.com/v2/resize:fit:750/0*0r0vS8myiqyOb79L.jpg 750w, https://miro.medium.com/v2/resize:fit:786/0*0r0vS8myiqyOb79L.jpg 786w, https://miro.medium.com/v2/resize:fit:828/0*0r0vS8myiqyOb79L.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/0*0r0vS8myiqyOb79L.jpg 1100w, https://miro.medium.com/v2/resize:fit:1092/0*0r0vS8myiqyOb79L.jpg 1092w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/546/0*0r0vS8myiqyOb79L.jpg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This is similar to older ideas like <a href="http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf" target="_self">this one</a>. But here they bypass TWO layers and are applied to large scales. Bypassing after 2 layers is a key intuition, as bypassing a single layer did not give much improvements. By 2 layers can be thought as a small classifier, or a Network-In-Network!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This is also the very first time that a network of &gt; hundred, even 1000 layers was trained.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">ResNet with a large number of layers started to use a bottleneck layer similar to the Inception bottleneck:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:38%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 294px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*9tCUFp28oQGOK6bE.jpg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*9tCUFp28oQGOK6bE.jpg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*9tCUFp28oQGOK6bE.jpg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*9tCUFp28oQGOK6bE.jpg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*9tCUFp28oQGOK6bE.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*9tCUFp28oQGOK6bE.jpg 1100w, https://miro.medium.com/v2/resize:fit:588/format:webp/0*9tCUFp28oQGOK6bE.jpg 588w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 294px" srcset="https://miro.medium.com/v2/resize:fit:640/0*9tCUFp28oQGOK6bE.jpg 640w, https://miro.medium.com/v2/resize:fit:720/0*9tCUFp28oQGOK6bE.jpg 720w, https://miro.medium.com/v2/resize:fit:750/0*9tCUFp28oQGOK6bE.jpg 750w, https://miro.medium.com/v2/resize:fit:786/0*9tCUFp28oQGOK6bE.jpg 786w, https://miro.medium.com/v2/resize:fit:828/0*9tCUFp28oQGOK6bE.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/0*9tCUFp28oQGOK6bE.jpg 1100w, https://miro.medium.com/v2/resize:fit:588/0*9tCUFp28oQGOK6bE.jpg 588w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/294/0*9tCUFp28oQGOK6bE.jpg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This layer reduces the number of features at each layer by first using a 1x1 convolution with a smaller output (usually 1/4 of the input), and then a 3x3 layer, and then again a 1x1 convolution to a larger number of features. Like in the case of Inception modules, this allows to keep the computation low, while providing rich combination of features. See “bottleneck layer” section after “GoogLeNet and Inception”.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">ResNet uses a fairly simple initial layers at the input (stem): a 7x7 conv layer followed with a pool of 2. Contrast this to more complex and less intuitive stems as in Inception V3, V4.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">ResNet also uses a pooling layer plus softmax as final classifier.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Additional insights about the ResNet architecture are appearing every day:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">ResNet can be seen as both parallel and serial modules, by just thinking of the inout as going to many modules in parallel, while the output of each modules connect in series</li><li class="ff3" style="font-size:22px;">ResNet can also be thought as <a href="http://arxiv.org/abs/1605.06431" target="_self">multiple ensembles of parallel or serial modules</a></li><li class="ff3" style="font-size:22px;">it has been found that ResNet usually operates on blocks of relatively low depth ~20–30 layers, which act in parallel, rather than serially flow the entire length of the network.</li><li class="ff3" style="font-size:22px;">ResNet, when the output is fed back to the input, as in RNN, the network can be seen as a better <a href="https://arxiv.org/abs/1604.03640" target="_self">bio-plausible model of the cortex</a></li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Inception V4</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">And Christian and team are at it again with a <a href="http://arxiv.org/abs/1602.07261" target="_self">new version of Inception</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The Inception module after the stem is rather similar to Inception V3:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*SJ7DP_-0R1vdpVzv.jpg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*SJ7DP_-0R1vdpVzv.jpg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*SJ7DP_-0R1vdpVzv.jpg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*SJ7DP_-0R1vdpVzv.jpg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*SJ7DP_-0R1vdpVzv.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*SJ7DP_-0R1vdpVzv.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SJ7DP_-0R1vdpVzv.jpg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*SJ7DP_-0R1vdpVzv.jpg 640w, https://miro.medium.com/v2/resize:fit:720/0*SJ7DP_-0R1vdpVzv.jpg 720w, https://miro.medium.com/v2/resize:fit:750/0*SJ7DP_-0R1vdpVzv.jpg 750w, https://miro.medium.com/v2/resize:fit:786/0*SJ7DP_-0R1vdpVzv.jpg 786w, https://miro.medium.com/v2/resize:fit:828/0*SJ7DP_-0R1vdpVzv.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/0*SJ7DP_-0R1vdpVzv.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/0*SJ7DP_-0R1vdpVzv.jpg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*SJ7DP_-0R1vdpVzv.jpg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">They also combined the Inception module with the ResNet module:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*exGWbD4A0QKM2lU_.jpg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*exGWbD4A0QKM2lU_.jpg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*exGWbD4A0QKM2lU_.jpg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*exGWbD4A0QKM2lU_.jpg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*exGWbD4A0QKM2lU_.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*exGWbD4A0QKM2lU_.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*exGWbD4A0QKM2lU_.jpg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*exGWbD4A0QKM2lU_.jpg 640w, https://miro.medium.com/v2/resize:fit:720/0*exGWbD4A0QKM2lU_.jpg 720w, https://miro.medium.com/v2/resize:fit:750/0*exGWbD4A0QKM2lU_.jpg 750w, https://miro.medium.com/v2/resize:fit:786/0*exGWbD4A0QKM2lU_.jpg 786w, https://miro.medium.com/v2/resize:fit:828/0*exGWbD4A0QKM2lU_.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/0*exGWbD4A0QKM2lU_.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/0*exGWbD4A0QKM2lU_.jpg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*exGWbD4A0QKM2lU_.jpg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This time though the solution is, in my opinion, less elegant and more complex, but also full of less transparent heuristics. It is hard to understand the choices and it is also hard for the authors to justify them.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In this regard the prize for a clean and simple network that can be easily understood and modified now goes to ResNet.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">SqueezeNet</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><a href="http://arxiv.org/abs/1602.07360" target="_self">SqueezeNet</a> has been recently released. It is a re-hash of many concepts from ResNet and Inception, and show that after all, a better design of architecture will deliver small network sizes and parameters without needing complex compression algorithms.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">ENet</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Our team set up to combine all the features of the recent architectures into a very efficient and light-weight network that uses very few parameters and computation to achieve state-of-the-art results. This network architecture is dubbed <a href="https://arxiv.org/abs/1606.02147" target="_self">ENet</a>, and was designed by <a href="https://apaszke.github.io/posts.html" target="_self">Adam Paszke</a>. We have used it to perform pixel-wise labeling and scene-parsing. Here are <a href="https://www.youtube.com/watch?v=3jq4FnO5Nco&list=PLNgy4gid0G9c4qiVBrERE_5v_b1pu-5pQ" target="_self">some videos of ENet in action</a>. These videos are not part of the <a href="https://www.cityscapes-dataset.com/" target="_self">training dataset</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><a href="https://arxiv.org/abs/1606.02147" target="_self">The technical report on ENet is available here</a>. ENet is a encoder plus decoder network. The encoder is a regular CNN design for categorization, while the decoder is a upsampling network designed to propagate the categories back into the original image size for segmentation. This worked used only neural networks, and no other algorithm to perform image segmentation.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*rokvtiIyLgMqkJiORkaLvA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*rokvtiIyLgMqkJiORkaLvA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*rokvtiIyLgMqkJiORkaLvA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*rokvtiIyLgMqkJiORkaLvA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*rokvtiIyLgMqkJiORkaLvA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*rokvtiIyLgMqkJiORkaLvA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rokvtiIyLgMqkJiORkaLvA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*rokvtiIyLgMqkJiORkaLvA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*rokvtiIyLgMqkJiORkaLvA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*rokvtiIyLgMqkJiORkaLvA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*rokvtiIyLgMqkJiORkaLvA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*rokvtiIyLgMqkJiORkaLvA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*rokvtiIyLgMqkJiORkaLvA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*rokvtiIyLgMqkJiORkaLvA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*rokvtiIyLgMqkJiORkaLvA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">As you can see in this figure ENet has the highest accuracy per parameter used of any neural network out there!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">ENet was designed to use the minimum number of resources possible from the start. As such it achieves such a small footprint that both encoder and decoder network together only occupies 0.7 MB with fp16 precision. Even at this small size, ENet is similar or above other pure neural network solutions in accuracy of segmentation.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">An analysis of modules</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">A systematic evaluation of CNN modules <a href="https://arxiv.org/abs/1606.02228" target="_self">has been presented</a>. The found out that is advantageous to use:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">• use ELU non-linearity without batchnorm or ReLU with it.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">• apply a learned colorspace transformation of RGB.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">• use the linear learning rate decay policy.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">• use a sum of the average and max pooling layers.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">• use mini-batch size around 128 or 256. If this is too big for your GPU, decrease the learning rate proportionally to the batch size.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">• use fully-connected layers as convolutional and average the predictions for the final decision.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">• when investing in increasing training set size, check if a plateau has not been reach. • cleanliness of the data is more important then the size.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">• if you cannot increase the input image size, reduce the stride in the con- sequent layers, it has roughly the same effect.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">• if your network has a complex and highly optimized architecture, like e.g. GoogLeNet, be careful with modifications.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Xception</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><a href="https://arxiv.org/abs/1610.02357" target="_self">Xception</a> improves on the inception module and architecture with a simple and more elegant architecture that is as effective as ResNet and Inception V4.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The Xception module is presented here:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*V7nfBsZsE6tlI92Y.jpg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*V7nfBsZsE6tlI92Y.jpg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*V7nfBsZsE6tlI92Y.jpg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*V7nfBsZsE6tlI92Y.jpg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*V7nfBsZsE6tlI92Y.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*V7nfBsZsE6tlI92Y.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*V7nfBsZsE6tlI92Y.jpg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*V7nfBsZsE6tlI92Y.jpg 640w, https://miro.medium.com/v2/resize:fit:720/0*V7nfBsZsE6tlI92Y.jpg 720w, https://miro.medium.com/v2/resize:fit:750/0*V7nfBsZsE6tlI92Y.jpg 750w, https://miro.medium.com/v2/resize:fit:786/0*V7nfBsZsE6tlI92Y.jpg 786w, https://miro.medium.com/v2/resize:fit:828/0*V7nfBsZsE6tlI92Y.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/0*V7nfBsZsE6tlI92Y.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/0*V7nfBsZsE6tlI92Y.jpg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*V7nfBsZsE6tlI92Y.jpg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This network can be anyone’s favorite given the simplicity and elegance of the architecture, presented here:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*l4NLzvWleahcwnck.jpg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*l4NLzvWleahcwnck.jpg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*l4NLzvWleahcwnck.jpg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*l4NLzvWleahcwnck.jpg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*l4NLzvWleahcwnck.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*l4NLzvWleahcwnck.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*l4NLzvWleahcwnck.jpg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*l4NLzvWleahcwnck.jpg 640w, https://miro.medium.com/v2/resize:fit:720/0*l4NLzvWleahcwnck.jpg 720w, https://miro.medium.com/v2/resize:fit:750/0*l4NLzvWleahcwnck.jpg 750w, https://miro.medium.com/v2/resize:fit:786/0*l4NLzvWleahcwnck.jpg 786w, https://miro.medium.com/v2/resize:fit:828/0*l4NLzvWleahcwnck.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/0*l4NLzvWleahcwnck.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/0*l4NLzvWleahcwnck.jpg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/0*l4NLzvWleahcwnck.jpg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The architecture has 36 convolutional stages, making it close in similarity to a ResNet-34. But the model and code is as simple as ResNet and much more comprehensible than Inception V4.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">A Torch7 implementation of this network is available <a href="https://gist.github.com/culurciello/554c8e56d3bbaf7c66bf66c6089dc221" target="_self">here</a> An implementation in Keras/TF is availble <a href="https://keras.io/applications/#xception" target="_self">here</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">It is interesting to note that the recent Xception architecture was also inspired by <a href="https://arxiv.org/abs/1412.5474" target="_self">our work on separable convolutional filters</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">MobileNets</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">A new M<a href="https://arxiv.org/abs/1704.04861" target="_self">obileNets</a> architecture is also available since April 2017. This architecture uses separable convolutions to reduce the number of parameters. The separate convolution is the same as Xception above. Now the claim of the paper is that there is a great reduction in parameters — about 1/2 in case of FaceNet, as reported in the paper. Here is the complete model architecture:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*SIjuEEhcaLs-K5tk-5M9Kw.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*SIjuEEhcaLs-K5tk-5M9Kw.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*SIjuEEhcaLs-K5tk-5M9Kw.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*SIjuEEhcaLs-K5tk-5M9Kw.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*SIjuEEhcaLs-K5tk-5M9Kw.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*SIjuEEhcaLs-K5tk-5M9Kw.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SIjuEEhcaLs-K5tk-5M9Kw.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*SIjuEEhcaLs-K5tk-5M9Kw.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*SIjuEEhcaLs-K5tk-5M9Kw.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*SIjuEEhcaLs-K5tk-5M9Kw.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*SIjuEEhcaLs-K5tk-5M9Kw.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*SIjuEEhcaLs-K5tk-5M9Kw.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*SIjuEEhcaLs-K5tk-5M9Kw.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*SIjuEEhcaLs-K5tk-5M9Kw.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*SIjuEEhcaLs-K5tk-5M9Kw.jpeg"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">MobileNets</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Unfortunately, we have tested this network in actual application and found it to be abysmally slow on a batch of 1 on a Titan Xp GPU. Look at a comparison here of inference time per image:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">resnet18 : 0.002871</li><li class="ff3" style="font-size:22px;">alexnet : 0.001003</li><li class="ff3" style="font-size:22px;">vgg16 : 0.001698</li><li class="ff3" style="font-size:22px;">squeezenet : 0.002725</li><li class="ff3" style="font-size:22px;">mobilenet : 0.033251</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Clearly this is not a contender in fast inference! It may reduce the parameters and size of network on disk, but is not usable.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Other notable architectures</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><a href="https://arxiv.org/abs/1605.07648" target="_self">FractalNet</a> uses a recursive architecture, that was not tested on ImageNet, and is a derivative or the more general ResNet.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Update</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For an update on comparison, please <a href="https://medium.com/@culurciello/analysis-of-deep-neural-networks-dcf398e71aae" target="_self">see this post</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">The future</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We believe that crafting neural network architectures is of paramount importance for the progress of the Deep Learning field. Our group highly recommends reading carefully and understanding all the papers in this post.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">But one could now wonder why we have to spend so much time in crafting architectures, and why instead we do not use data to tell us what to use, and how to combine modules. This would be nice, but now it is work in progress. Some initial interesting results are <a href="https://arxiv.org/abs/1606.06216" target="_self">here</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Note also that here we mostly talked about architectures for computer vision. Similarly neural network architectures developed in other areas, and it is interesting to study the evolution of architectures for all other tasks also.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you are interested in a comparison of neural network architecture and computational performance, see <a href="http://arxiv.org/abs/1605.07678" target="_self">our recent paper</a>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Acknowledgments</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This post was inspired by discussions with Abhishek Chaurasia, Adam Paszke, Sangpil Kim, Alfredo Canziani and others in our e-Lab at Purdue University.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">About the author</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: <a href="https://medium.com/@culurciello/" target="_self">Medium</a>, <a href="https://e-lab.github.io/html/contact-eugenio-culurciello.html" target="_self">webpage</a>, <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ" target="_self">Scholar</a>, <a href="https://www.linkedin.com/in/eugenioculurciello/" target="_self">LinkedIn</a>, and more…</p></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>