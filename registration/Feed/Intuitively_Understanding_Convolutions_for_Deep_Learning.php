<!DOCTYPE html>
                <html>
                <head>
                    <title>Intuitively Understanding Convolutions for Deep Learning</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://medium.com/@irhumshafkat?source=post_page-----1f6f42faee1--------------------------------">Author : Irhum Shafkat</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>Intuitively Understanding Convolutions for Deep Learning</h3></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5 text-muted">Exploring the strong visual hierarchies that makes them work</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Fw-ehcNBR9byHtho-Rxbtw.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*Fw-ehcNBR9byHtho-Rxbtw.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*Fw-ehcNBR9byHtho-Rxbtw.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*Fw-ehcNBR9byHtho-Rxbtw.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*Fw-ehcNBR9byHtho-Rxbtw.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*Fw-ehcNBR9byHtho-Rxbtw.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*Fw-ehcNBR9byHtho-Rxbtw.gif 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Fw-ehcNBR9byHtho-Rxbtw.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*Fw-ehcNBR9byHtho-Rxbtw.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*Fw-ehcNBR9byHtho-Rxbtw.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*Fw-ehcNBR9byHtho-Rxbtw.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*Fw-ehcNBR9byHtho-Rxbtw.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*Fw-ehcNBR9byHtho-Rxbtw.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*Fw-ehcNBR9byHtho-Rxbtw.gif 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*Fw-ehcNBR9byHtho-Rxbtw.gif"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The advent of powerful and versatile deep learning frameworks in recent years has made it possible to implement convolution layers into a deep learning model an extremely simple task, often achievable in a single line of code.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">However, understanding convolutions, especially for the first time can often feel a bit unnerving, with terms like kernels, filters, channels and so on all stacked onto each other. Yet, convolutions as a concept are fascinatingly powerful and highly extensible, and in this post, we’ll break down the mechanics of the convolution operation, step-by-step, relate it to the standard fully connected network, and explore just how they build up a strong visual hierarchy, making them powerful feature extractors for images.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">2D Convolutions: The Operation</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The 2D convolution is a fairly simple operation at heart: you start with a kernel, which is simply a small matrix of weights. This kernel “slides” over the 2D input data, performing an elementwise multiplication with the part of the input it is currently on, and then summing up the results into a single output pixel.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:72%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 535px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif 1100w, https://miro.medium.com/v2/resize:fit:1070/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif 1070w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 535px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif 1100w, https://miro.medium.com/v2/resize:fit:1070/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif 1070w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/535/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;"><a href="https://arxiv.org/abs/1603.07285" target="_self">A standard convolution[1]</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The kernel repeats this process for every location it slides over, converting a 2D matrix of features into yet another 2D matrix of features. The output features are essentially, the weighted sums (with the weights being the values of the kernel itself) of the input features located roughly in the same location of the output pixel on the input layer.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Whether or not an input feature falls within this “roughly same location”, gets determined directly by whether it’s in the area of the kernel that produced the output or not. This means the size of the kernel directly determines how many (or few) input features get combined in the production of a new output feature.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This is all in pretty stark contrast to a fully connected layer. In the above example, we have 5×5=25 input features, and 3×3=9 output features. If this were a standard fully connected layer, you’d have a weight matrix of 25×9 = 225 parameters, with every output feature being the weighted sum of <em>every single</em> input feature. Convolutions allow us to do this transformation with only 9 parameters, with each output feature, instead of “looking at” every input feature, only getting to “look” at input features coming from roughly the same location. Do take note of this, as it’ll be critical to our later discussion.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Some commonly used techniques</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Before we move on, it’s definitely worth looking into two techniques that are commonplace in convolution layers: Padding and Strides.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Padding: If you see the animation above, notice that during the sliding process, the edges essentially get “trimmed off”, converting a 5×5 feature matrix to a 3×3 one. The pixels on the edge are never at the center of the kernel, because there is nothing for the kernel to extend to beyond the edge. This isn’t ideal, as often we’d like the size of the output to equal the input.</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:52%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 395px" srcset="https://miro.medium.com/v2/resize:fit:640/1*1okwhewf5KCtIPaFib4XaA.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*1okwhewf5KCtIPaFib4XaA.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*1okwhewf5KCtIPaFib4XaA.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*1okwhewf5KCtIPaFib4XaA.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*1okwhewf5KCtIPaFib4XaA.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*1okwhewf5KCtIPaFib4XaA.gif 1100w, https://miro.medium.com/v2/resize:fit:790/1*1okwhewf5KCtIPaFib4XaA.gif 790w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 395px" srcset="https://miro.medium.com/v2/resize:fit:640/1*1okwhewf5KCtIPaFib4XaA.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*1okwhewf5KCtIPaFib4XaA.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*1okwhewf5KCtIPaFib4XaA.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*1okwhewf5KCtIPaFib4XaA.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*1okwhewf5KCtIPaFib4XaA.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*1okwhewf5KCtIPaFib4XaA.gif 1100w, https://miro.medium.com/v2/resize:fit:790/1*1okwhewf5KCtIPaFib4XaA.gif 790w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/395/1*1okwhewf5KCtIPaFib4XaA.gif"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;"><a href="https://arxiv.org/abs/1603.07285" target="_self">Same padding[1]</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Padding does something pretty clever to solve this: pad the edges with extra, “fake” pixels (usually of value 0, hence the oft-used term “zero padding”). This way, the kernel when sliding can allow the original edge pixels to be at its center, while extending into the fake pixels beyond the edge, producing an output the same size as the input.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Striding: Often when running a convolution layer, you want an output with a lower size than the input. This is commonplace in convolutional neural networks, where the size of the spatial dimensions are reduced when increasing the number of channels. One way of accomplishing this is by using a pooling layer (eg. taking the average/max of every 2×2 grid to reduce each spatial dimensions in half). Yet another way to do is is to use a stride:</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:38%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 294px" srcset="https://miro.medium.com/v2/resize:fit:640/1*BMngs93_rm2_BpJFH2mS0Q.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*BMngs93_rm2_BpJFH2mS0Q.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*BMngs93_rm2_BpJFH2mS0Q.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*BMngs93_rm2_BpJFH2mS0Q.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*BMngs93_rm2_BpJFH2mS0Q.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*BMngs93_rm2_BpJFH2mS0Q.gif 1100w, https://miro.medium.com/v2/resize:fit:588/1*BMngs93_rm2_BpJFH2mS0Q.gif 588w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 294px" srcset="https://miro.medium.com/v2/resize:fit:640/1*BMngs93_rm2_BpJFH2mS0Q.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*BMngs93_rm2_BpJFH2mS0Q.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*BMngs93_rm2_BpJFH2mS0Q.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*BMngs93_rm2_BpJFH2mS0Q.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*BMngs93_rm2_BpJFH2mS0Q.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*BMngs93_rm2_BpJFH2mS0Q.gif 1100w, https://miro.medium.com/v2/resize:fit:588/1*BMngs93_rm2_BpJFH2mS0Q.gif 588w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/294/1*BMngs93_rm2_BpJFH2mS0Q.gif"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;"><a href="https://arxiv.org/abs/1603.07285" target="_self">A stride 2 convolution[1]</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The idea of the stride is to <em>skip </em>some of the slide locations of the kernel. A stride of 1 means to pick slides a pixel apart, so basically every single slide, acting as a standard convolution. A stride of 2 means picking slides 2 pixels apart, skipping every other slide in the process, downsizing by roughly a factor of 2, a stride of 3 means skipping every 2 slides, downsizing roughly by factor 3, and so on.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">More modern networks, such as the ResNet architectures entirely forgo pooling layers in their internal layers, in favor of strided convolutions when needing to reduce their output sizes.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">The multi-channel version</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Of course, the diagrams above only deals with the case where the image has a single input channel. In practicality, most input images have 3 channels, and that number only increases the deeper you go into a network. It’s pretty easy to think of channels, in general, as being a “view” of the image as a whole, emphasising some aspects, de-emphasising others.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*k8P28Ayl-5hOqIMSv-qosw.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*k8P28Ayl-5hOqIMSv-qosw.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*k8P28Ayl-5hOqIMSv-qosw.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*k8P28Ayl-5hOqIMSv-qosw.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*k8P28Ayl-5hOqIMSv-qosw.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*k8P28Ayl-5hOqIMSv-qosw.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k8P28Ayl-5hOqIMSv-qosw.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*k8P28Ayl-5hOqIMSv-qosw.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*k8P28Ayl-5hOqIMSv-qosw.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*k8P28Ayl-5hOqIMSv-qosw.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*k8P28Ayl-5hOqIMSv-qosw.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*k8P28Ayl-5hOqIMSv-qosw.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*k8P28Ayl-5hOqIMSv-qosw.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*k8P28Ayl-5hOqIMSv-qosw.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*k8P28Ayl-5hOqIMSv-qosw.jpeg"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Most of the time, we deal with RGB images with three channels. (Credit: <a href="https://unsplash.com/photos/_d3sppFprWI" target="_self">Andre Mouton</a>)</p><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:81%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 600px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*lRpx5pTrVewFTD8YXjhIKA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*lRpx5pTrVewFTD8YXjhIKA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*lRpx5pTrVewFTD8YXjhIKA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*lRpx5pTrVewFTD8YXjhIKA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*lRpx5pTrVewFTD8YXjhIKA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*lRpx5pTrVewFTD8YXjhIKA.png 1100w, https://miro.medium.com/v2/resize:fit:1200/format:webp/1*lRpx5pTrVewFTD8YXjhIKA.png 1200w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 600px" srcset="https://miro.medium.com/v2/resize:fit:640/1*lRpx5pTrVewFTD8YXjhIKA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*lRpx5pTrVewFTD8YXjhIKA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*lRpx5pTrVewFTD8YXjhIKA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*lRpx5pTrVewFTD8YXjhIKA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*lRpx5pTrVewFTD8YXjhIKA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*lRpx5pTrVewFTD8YXjhIKA.png 1100w, https://miro.medium.com/v2/resize:fit:1200/1*lRpx5pTrVewFTD8YXjhIKA.png 1200w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/600/1*lRpx5pTrVewFTD8YXjhIKA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">A filter: A collection of kernels</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">So this is where a key distinction between terms comes in handy: whereas in the 1 channel case, where the term filter and kernel are interchangeable, in the general case, they’re actually pretty different. <mark>Each filter actually happens to be a </mark><mark>collection of kernels</mark><mark>,</mark><mark> </mark><mark>with there being one kernel for every single input channel to the layer, and each kernel being unique.</mark></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Each filter in a convolution layer produces one and only one output channel, and they do it like so:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Each of the kernels of the filter “slides” over their respective input channels, producing a processed version of each. Some kernels may have stronger weights than others, to give more emphasis to certain input channels than others (eg. a filter may have a red kernel channel with stronger weights than others, and hence, respond more to differences in the red channel features than the others).</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*8dx6nxpUh2JqvYWPadTwMQ.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*8dx6nxpUh2JqvYWPadTwMQ.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*8dx6nxpUh2JqvYWPadTwMQ.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*8dx6nxpUh2JqvYWPadTwMQ.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*8dx6nxpUh2JqvYWPadTwMQ.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*8dx6nxpUh2JqvYWPadTwMQ.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*8dx6nxpUh2JqvYWPadTwMQ.gif 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*8dx6nxpUh2JqvYWPadTwMQ.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*8dx6nxpUh2JqvYWPadTwMQ.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*8dx6nxpUh2JqvYWPadTwMQ.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*8dx6nxpUh2JqvYWPadTwMQ.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*8dx6nxpUh2JqvYWPadTwMQ.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*8dx6nxpUh2JqvYWPadTwMQ.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*8dx6nxpUh2JqvYWPadTwMQ.gif 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*8dx6nxpUh2JqvYWPadTwMQ.gif"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Each of the per-channel processed versions are then summed together to form <em>one</em> channel. The kernels of a filter each produce one version of each channel, and the filter as a whole produces one overall output channel.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*CYB2dyR3EhFs1xNLK8ewiA.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*CYB2dyR3EhFs1xNLK8ewiA.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*CYB2dyR3EhFs1xNLK8ewiA.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*CYB2dyR3EhFs1xNLK8ewiA.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*CYB2dyR3EhFs1xNLK8ewiA.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*CYB2dyR3EhFs1xNLK8ewiA.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*CYB2dyR3EhFs1xNLK8ewiA.gif 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*CYB2dyR3EhFs1xNLK8ewiA.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*CYB2dyR3EhFs1xNLK8ewiA.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*CYB2dyR3EhFs1xNLK8ewiA.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*CYB2dyR3EhFs1xNLK8ewiA.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*CYB2dyR3EhFs1xNLK8ewiA.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*CYB2dyR3EhFs1xNLK8ewiA.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*CYB2dyR3EhFs1xNLK8ewiA.gif 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*CYB2dyR3EhFs1xNLK8ewiA.gif"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Finally, then there’s the bias term. The way the bias term works here is that each output filter has one bias term. The bias gets added to the output channel so far to produce the final output channel.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:38%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 294px" srcset="https://miro.medium.com/v2/resize:fit:640/1*RYYucIh3U-YFxrIkyQKzRw.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*RYYucIh3U-YFxrIkyQKzRw.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*RYYucIh3U-YFxrIkyQKzRw.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*RYYucIh3U-YFxrIkyQKzRw.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*RYYucIh3U-YFxrIkyQKzRw.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*RYYucIh3U-YFxrIkyQKzRw.gif 1100w, https://miro.medium.com/v2/resize:fit:588/1*RYYucIh3U-YFxrIkyQKzRw.gif 588w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 294px" srcset="https://miro.medium.com/v2/resize:fit:640/1*RYYucIh3U-YFxrIkyQKzRw.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*RYYucIh3U-YFxrIkyQKzRw.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*RYYucIh3U-YFxrIkyQKzRw.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*RYYucIh3U-YFxrIkyQKzRw.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*RYYucIh3U-YFxrIkyQKzRw.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*RYYucIh3U-YFxrIkyQKzRw.gif 1100w, https://miro.medium.com/v2/resize:fit:588/1*RYYucIh3U-YFxrIkyQKzRw.gif 588w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/294/1*RYYucIh3U-YFxrIkyQKzRw.gif"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">And with the single filter case down, the case for any number of filters is identical: Each filter processes the input with its own, different set of kernels and a scalar bias with the process described above, producing a single output channel. They are then concatenated together to produce the overall output, with the number of output channels being the number of filters. A nonlinearity is then usually applied before passing this as input to another convolution layer, which then repeats this process.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">2D Convolutions: The Intuition</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Convolutions are still linear transforms</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Even with the mechanics of the convolution layer down, it can still be hard to relate it back to a standard feed-forward network, and it still doesn’t explain why convolutions scale to, and work so much better for image data.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Suppose we have a 4×4 input, and we want to transform it into a 2×2 grid. If we were using a feedforward network, we’d reshape the 4×4 input into a vector of length 16, and pass it through a densely connected layer with 16 inputs and 4 outputs. One could visualize the weight matrix <strong>W </strong>for a layer:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Nq-Za2-OzW8J5n7Tu7QIWw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Nq-Za2-OzW8J5n7Tu7QIWw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Nq-Za2-OzW8J5n7Tu7QIWw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Nq-Za2-OzW8J5n7Tu7QIWw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Nq-Za2-OzW8J5n7Tu7QIWw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Nq-Za2-OzW8J5n7Tu7QIWw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nq-Za2-OzW8J5n7Tu7QIWw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Nq-Za2-OzW8J5n7Tu7QIWw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*Nq-Za2-OzW8J5n7Tu7QIWw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*Nq-Za2-OzW8J5n7Tu7QIWw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*Nq-Za2-OzW8J5n7Tu7QIWw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*Nq-Za2-OzW8J5n7Tu7QIWw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*Nq-Za2-OzW8J5n7Tu7QIWw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*Nq-Za2-OzW8J5n7Tu7QIWw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*Nq-Za2-OzW8J5n7Tu7QIWw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">All in all, some 64 parameters</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">And although the convolution kernel operation may seem a bit strange at first, it is still a linear transformation with an equivalent transformation matrix. If we were to use a kernel <strong>K </strong>of size 3 on the reshaped 4×4 input to get a 2×2 output, the equivalent transformation matrix would be:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*cr0IabpKu4zIyvDgCTQ64A.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*cr0IabpKu4zIyvDgCTQ64A.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*cr0IabpKu4zIyvDgCTQ64A.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*cr0IabpKu4zIyvDgCTQ64A.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*cr0IabpKu4zIyvDgCTQ64A.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*cr0IabpKu4zIyvDgCTQ64A.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cr0IabpKu4zIyvDgCTQ64A.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*cr0IabpKu4zIyvDgCTQ64A.png 640w, https://miro.medium.com/v2/resize:fit:720/1*cr0IabpKu4zIyvDgCTQ64A.png 720w, https://miro.medium.com/v2/resize:fit:750/1*cr0IabpKu4zIyvDgCTQ64A.png 750w, https://miro.medium.com/v2/resize:fit:786/1*cr0IabpKu4zIyvDgCTQ64A.png 786w, https://miro.medium.com/v2/resize:fit:828/1*cr0IabpKu4zIyvDgCTQ64A.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*cr0IabpKu4zIyvDgCTQ64A.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*cr0IabpKu4zIyvDgCTQ64A.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*cr0IabpKu4zIyvDgCTQ64A.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">There’s really just 9 parameters here.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">(Note: while the above matrix is an <em>equivalent </em>transformation matrix, the actual operation is usually implemented as a very different <a href="http://cs231n.github.io/convolutional-networks/" target="_self">matrix multiplication[2]</a>)</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The convolution then, as a whole, is still a linear transformation, but at the same time it’s also a dramatically different kind of transformation. For a matrix with 64 elements, there’s just 9 parameters which themselves are reused several times. Each output node only gets to see a select number of inputs (the ones inside the kernel). There is no interaction with any of the other inputs, as the weights to them are set to 0.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">It’s useful to see the convolution operation as a <em>hard prior</em> on the weight matrix. In this context, by prior, I mean predefined network parameters. For example, when you use a pretrained model for image classification, you use the<em> pretrained network parameters</em> as your prior, as a feature extractor to your final densely connected layer.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In that sense, there’s a direct intuition between why both are so efficient (compared to their alternatives). Transfer learning is efficient by orders of magnitude compared to random initialization, because you only really need to optimize the parameters of the final fully connected layer, which means you can have fantastic performance with only a few dozen images per class.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Here, you don’t need to optimize all 64 parameters, because we set most of them to zero (and they’ll stay that way), and the rest we convert to shared parameters, resulting in only 9 actual parameters to optimize. This efficiency matters, because when you move from the 784 inputs of MNIST to real world 224×224×3 images, thats over 150,000 inputs. A dense layer attempting to halve the input to 75,000 inputs would still require over 10 <em>billion </em>parameters. For comparison, the <em>entirety </em>of ResNet-50 has some 25 million parameters.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">So fixing some parameters to 0, and tying parameters increases efficiency, but unlike the transfer learning case, where we know the prior is good because it works on a large general set of images, how do we know <em>this </em>is any good?</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The answer lies in the feature combinations the prior leads the parameters to learn.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Locality</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Early on in this article, we discussed that:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Kernels combine pixels only from a small, local area to form an output. That is, the output feature only “sees” input features from a small local area.</li><li class="ff3" style="font-size:22px;">The kernel is applied globally across the whole image to produce a matrix of outputs.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">So with backpropagation coming in all the way from the classification nodes of the network, the kernels have the interesting task of learning weights to produce features only from a set of local inputs. Additionally, because the kernel itself is applied across the entire image, the features the kernel learns must be general enough to come from any part of the image.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If this were any other kind of data, eg. categorical data of app installs, this would’ve been a disaster, for just because your number of <em>app installs</em> and <em>app type</em> columns are next to each other doesn’t mean they have any “local, shared features” common with <em>app install dates</em> and <em>time used</em>. Sure, the four may have an underlying higher level feature (eg. which apps people want most) that can be found, but that gives us no reason to believe the parameters for the first two are exactly the same as the parameters for the latter two. The four could’ve been in any (consistent) order and still be valid!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Pixels however, always appear in a consistent order, and nearby pixels influence a pixel e.g. if all nearby pixels are red, it’s pretty likely the pixel is also red. If there are deviations, that’s an interesting anomaly that could be converted into a feature, and all this can be detected from comparing a pixel with its neighbors, with other pixels in its locality.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">And this idea is really what a lot of earlier computer vision feature extraction methods were based around. For instance, for edge detection, one can use a Sobel edge detection filter, a kernel with fixed parameters, operating just like the standard one-channel convolution:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*wju0Urp6KpAT11wktTp5Sg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*wju0Urp6KpAT11wktTp5Sg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*wju0Urp6KpAT11wktTp5Sg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*wju0Urp6KpAT11wktTp5Sg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*wju0Urp6KpAT11wktTp5Sg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*wju0Urp6KpAT11wktTp5Sg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wju0Urp6KpAT11wktTp5Sg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*wju0Urp6KpAT11wktTp5Sg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*wju0Urp6KpAT11wktTp5Sg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*wju0Urp6KpAT11wktTp5Sg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*wju0Urp6KpAT11wktTp5Sg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*wju0Urp6KpAT11wktTp5Sg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*wju0Urp6KpAT11wktTp5Sg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*wju0Urp6KpAT11wktTp5Sg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*wju0Urp6KpAT11wktTp5Sg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Applying a vertical edge detector kernel</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For a non-edge containing grid (eg. the background sky), most of the pixels are the same value, so the overall output of the kernel at that point is 0. For a grid with an vertical edge, there is a difference between the pixels to the left and right of the edge, and the kernel computes that difference to be non-zero, activating and revealing the edges. The kernel only works only a 3×3 grids at a time, detecting anomalies on a local scale, yet when applied across the entire image, is enough to detect a certain feature on a global scale, anywhere in the image!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">So the key difference we make with deep learning is ask this question: Can useful kernels be learnt? For early layers operating on raw pixels, we could reasonably expect feature detectors of fairly low level features, like edges, lines, etc.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">There’s an entire branch of deep learning research focused on making neural network models interpretable. One of the most powerful tools to come out of that is<a href="https://distill.pub/2017/feature-visualization/" target="_self"> Feature Visualization using optimization[3]</a>. The idea at core is simple: optimize a image (usually initialized with random noise) to activate a filter as strongly as possible. This does make intuitive sense: if the optimized image is completely filled with edges, that’s strong evidence that’s what the filter itself is looking for and is activated by. Using this, we can peek into the learnt filters, and the results are stunning:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*yPqJIu2mRtv10r3yvTNmEQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*yPqJIu2mRtv10r3yvTNmEQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*yPqJIu2mRtv10r3yvTNmEQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*yPqJIu2mRtv10r3yvTNmEQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*yPqJIu2mRtv10r3yvTNmEQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*yPqJIu2mRtv10r3yvTNmEQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yPqJIu2mRtv10r3yvTNmEQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*yPqJIu2mRtv10r3yvTNmEQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*yPqJIu2mRtv10r3yvTNmEQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*yPqJIu2mRtv10r3yvTNmEQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*yPqJIu2mRtv10r3yvTNmEQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*yPqJIu2mRtv10r3yvTNmEQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*yPqJIu2mRtv10r3yvTNmEQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*yPqJIu2mRtv10r3yvTNmEQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*yPqJIu2mRtv10r3yvTNmEQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Feature visualization for 3 different channels from the 1st convolution layer of GoogLeNet[3]. Notice that while they detect different types of edges, they’re still low-level edge detectors.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:65%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 488px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*xfgprei46gBTHS73Sx3VJQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*xfgprei46gBTHS73Sx3VJQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*xfgprei46gBTHS73Sx3VJQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*xfgprei46gBTHS73Sx3VJQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*xfgprei46gBTHS73Sx3VJQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*xfgprei46gBTHS73Sx3VJQ.png 1100w, https://miro.medium.com/v2/resize:fit:976/format:webp/1*xfgprei46gBTHS73Sx3VJQ.png 976w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 488px" srcset="https://miro.medium.com/v2/resize:fit:640/1*xfgprei46gBTHS73Sx3VJQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*xfgprei46gBTHS73Sx3VJQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*xfgprei46gBTHS73Sx3VJQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*xfgprei46gBTHS73Sx3VJQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*xfgprei46gBTHS73Sx3VJQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*xfgprei46gBTHS73Sx3VJQ.png 1100w, https://miro.medium.com/v2/resize:fit:976/1*xfgprei46gBTHS73Sx3VJQ.png 976w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/488/1*xfgprei46gBTHS73Sx3VJQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Feature Visualization of channel 12 from the 2nd and 3rd convolutions[3]</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">One important thing to notice here is that <em>convolved images are still images. </em>The output of a small grid of pixels from the top left of an image will still be on the top left. So you can run another convolution layer on top of another (such as the two on the left) to extract deeper features, which we visualize.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Yet, however deep our feature detectors get, without any further changes they’ll still be operating on very small patches of the image. No matter how deep your detectors are, you can’t detect faces from a 3×3 grid. And this is where the idea of the receptive field comes in.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Receptive field</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">A essential design choice of any CNN architecture is that the input sizes grow smaller and smaller from the start to the end of the network, while the number of channels grow deeper. This, as mentioned earlier, is often done through strides or pooling layers. Locality determines what inputs from the previous layer the outputs get to see. The receptive field determines what area of the <em>original input</em> to the entire network the output gets to see.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The idea of a strided convolution is that we only process slides a fixed distance apart, and skip the ones in the middle. From a different point of view, we only keep outputs a fixed distance apart, and remove the rest[1].</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*LO3xzgJmcKNAfFC6v7tqtg.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*LO3xzgJmcKNAfFC6v7tqtg.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*LO3xzgJmcKNAfFC6v7tqtg.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*LO3xzgJmcKNAfFC6v7tqtg.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*LO3xzgJmcKNAfFC6v7tqtg.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*LO3xzgJmcKNAfFC6v7tqtg.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*LO3xzgJmcKNAfFC6v7tqtg.gif 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*LO3xzgJmcKNAfFC6v7tqtg.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*LO3xzgJmcKNAfFC6v7tqtg.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*LO3xzgJmcKNAfFC6v7tqtg.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*LO3xzgJmcKNAfFC6v7tqtg.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*LO3xzgJmcKNAfFC6v7tqtg.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*LO3xzgJmcKNAfFC6v7tqtg.gif 1100w, https://miro.medium.com/v2/resize:fit:1400/1*LO3xzgJmcKNAfFC6v7tqtg.gif 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*LO3xzgJmcKNAfFC6v7tqtg.gif"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">3×3 convolution, stride 2</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We then apply a nonlinearity to the output, and per usual, then stack another new convolution layer on top. And this is where things get interesting. Even if were we to apply a kernel of the same size (3×3), having the same local area, to the output of the strided convolution, the kernel would have a larger effective receptive field:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*uEgDEUwB8z6QUT-jQQJlIA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*uEgDEUwB8z6QUT-jQQJlIA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*uEgDEUwB8z6QUT-jQQJlIA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*uEgDEUwB8z6QUT-jQQJlIA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*uEgDEUwB8z6QUT-jQQJlIA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*uEgDEUwB8z6QUT-jQQJlIA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uEgDEUwB8z6QUT-jQQJlIA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*uEgDEUwB8z6QUT-jQQJlIA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*uEgDEUwB8z6QUT-jQQJlIA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*uEgDEUwB8z6QUT-jQQJlIA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*uEgDEUwB8z6QUT-jQQJlIA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*uEgDEUwB8z6QUT-jQQJlIA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*uEgDEUwB8z6QUT-jQQJlIA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*uEgDEUwB8z6QUT-jQQJlIA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*uEgDEUwB8z6QUT-jQQJlIA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This is because the output of the strided layer still does represent the same image. It is not so much cropping as it is resizing, only thing is that each single pixel in the output is a “representative” of a larger area (of whose other pixels were discarded) from the same rough location from the original input. So when the next layer’s kernel operates on the output, it’s operating on pixels collected from a larger area.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">(Note: if you’re familiar with dilated convolutions, note that the above is <em>not</em> a dilated convolution. Both are methods of increasing the receptive field, but dilated convolutions are a single layer, while this takes place on a regular convolution following a strided convolution, with a nonlinearity inbetween)</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:32%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 257px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*QebJ6hejQh074dYkDkyo6g.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*QebJ6hejQh074dYkDkyo6g.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*QebJ6hejQh074dYkDkyo6g.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*QebJ6hejQh074dYkDkyo6g.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*QebJ6hejQh074dYkDkyo6g.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*QebJ6hejQh074dYkDkyo6g.png 1100w, https://miro.medium.com/v2/resize:fit:514/format:webp/1*QebJ6hejQh074dYkDkyo6g.png 514w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 257px" srcset="https://miro.medium.com/v2/resize:fit:640/1*QebJ6hejQh074dYkDkyo6g.png 640w, https://miro.medium.com/v2/resize:fit:720/1*QebJ6hejQh074dYkDkyo6g.png 720w, https://miro.medium.com/v2/resize:fit:750/1*QebJ6hejQh074dYkDkyo6g.png 750w, https://miro.medium.com/v2/resize:fit:786/1*QebJ6hejQh074dYkDkyo6g.png 786w, https://miro.medium.com/v2/resize:fit:828/1*QebJ6hejQh074dYkDkyo6g.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*QebJ6hejQh074dYkDkyo6g.png 1100w, https://miro.medium.com/v2/resize:fit:514/1*QebJ6hejQh074dYkDkyo6g.png 514w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/257/1*QebJ6hejQh074dYkDkyo6g.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Feature visualization of channels from each of the major collections of convolution blocks, showing a progressive increase in complexity[3]</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This expansion of the receptive field allows the convolution layers to combine the low level features (lines, edges), into higher level features (curves, textures), as we see in the mixed3a layer.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Followed by a pooling/strided layer, the network continues to create detectors for even higher level features (parts, patterns), as we see for mixed4a.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The repeated reduction in image size across the network results in, by the 5th block on convolutions, input sizes of just 7×7, compared to inputs of 224×224. At this point, each <em>single </em>pixel represents a grid of 32×32 pixels, which is huge.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Compared to earlier layers, where an activation meant detecting an edge, here, an activation on the tiny 7×7 grid is one for a very high level feature, such as for birds.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The network as a whole progresses from a small number of filters (64 in case of GoogLeNet), detecting low level features, to a very large number of filters(1024 in the final convolution), each looking for an extremely specific high level feature. Followed by a final pooling layer, which collapses each 7×7 grid into a single pixel, each channel is a feature detector with a receptive field equivalent to the <em>entire </em>image.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Compared to what a standard feedforward network would have done, the output here is really nothing short of awe-inspiring. A standard feedforward network would have produced abstract feature vectors, from combinations of every single pixel in the image, requiring intractable amounts of data to train.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The CNN, with the priors imposed on it, starts by learning very low level feature detectors, and as across the layers as its receptive field is expanded, learns to combine those low-level features into progressively higher level features; not an abstract combination of every single pixel, but rather, a strong <em>visual hierarchy </em>of concepts.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">By detecting low level features, and using them to detect higher level features as it progresses up its visual hierarchy, it is eventually able to detect entire visual concepts such as faces, birds, trees, etc, and that’s what makes them such powerful, yet efficient with image data.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">A final note on adversarial attacks</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">With the visual hierarchy CNNs build, it is pretty reasonable to assume that their vision systems are similar to humans. And they’re really great with real world images, but they also fail in ways that strongly suggest their vision systems aren’t entirely human-like. The most major problem: <a href="https://blog.openai.com/adversarial-example-research/" target="_self">Adversarial Examples[4]</a>, examples which have been specifically modified to fool the model.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:63%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 470px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*m2ngYZx3Bc5OWdeDyM8X0A.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*m2ngYZx3Bc5OWdeDyM8X0A.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*m2ngYZx3Bc5OWdeDyM8X0A.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*m2ngYZx3Bc5OWdeDyM8X0A.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*m2ngYZx3Bc5OWdeDyM8X0A.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*m2ngYZx3Bc5OWdeDyM8X0A.png 1100w, https://miro.medium.com/v2/resize:fit:940/format:webp/1*m2ngYZx3Bc5OWdeDyM8X0A.png 940w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 470px" srcset="https://miro.medium.com/v2/resize:fit:640/1*m2ngYZx3Bc5OWdeDyM8X0A.png 640w, https://miro.medium.com/v2/resize:fit:720/1*m2ngYZx3Bc5OWdeDyM8X0A.png 720w, https://miro.medium.com/v2/resize:fit:750/1*m2ngYZx3Bc5OWdeDyM8X0A.png 750w, https://miro.medium.com/v2/resize:fit:786/1*m2ngYZx3Bc5OWdeDyM8X0A.png 786w, https://miro.medium.com/v2/resize:fit:828/1*m2ngYZx3Bc5OWdeDyM8X0A.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*m2ngYZx3Bc5OWdeDyM8X0A.png 1100w, https://miro.medium.com/v2/resize:fit:940/1*m2ngYZx3Bc5OWdeDyM8X0A.png 940w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/470/1*m2ngYZx3Bc5OWdeDyM8X0A.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">To a human, both images are obviously pandas. To the model, not so much.[4]</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Adversarial examples would be a non-issue if the only tampered ones that caused the models to fail were ones that even humans would notice. The problem is, the models are susceptible to attacks by samples which have only been tampered with ever so slightly, and would clearly not fool any human. This opens the door for models to silently fail, which can be pretty dangerous for a wide range of applications from self-driving cars to healthcare.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Robustness against adversarial attacks is currently a highly active area of research, the subject of many papers and even competitions, and solutions will certainly improve CNN architectures to become safer and more reliable.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Conclusion</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">CNNs were the models that allowed computer vision to scale from simple applications to powering sophisticated products and services, ranging from face detection in your photo gallery to making better medical diagnoses. They might be the key method in computer vision going forward, or some other new breakthrough might just be around the corner. Regardless, one thing is for sure: they’re nothing short of amazing, at the heart of many present-day innovative applications, and are most certainly worth deeply understanding.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">References</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/abs/1603.07285" target="_self">A guide to convolution arithmetic for deep learning</a></li><li class="ff3" style="font-size:22px;"><a href="http://cs231n.github.io/convolutional-networks/" target="_self">CS231n Convolutional Neural Networks for Visual Recognition — Convolutional Neural Networks</a></li><li class="ff3" style="font-size:22px;"><a href="https://distill.pub/2017/feature-visualization/" target="_self">Feature Visualization — How neural networks build up their understanding of images</a> (of note: the feature visualizations here were produced with the <a href="https://github.com/tensorflow/lucid" target="_self">Lucid</a> library, an open source implementation of the techniques from this journal article)</li><li class="ff3" style="font-size:22px;"><a href="https://blog.openai.com/adversarial-example-research/" target="_self">Attacking Machine Learning with Adversarial Examples</a></li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Further Resources</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;"><a href="http://course.fast.ai/lessons/lesson3.html" target="_self">fast.ai — Lesson 3: Improving your Image Classifier</a></li><li class="ff3" style="font-size:22px;"><a href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/" target="_self">Conv Nets: A Modular Perspective</a></li><li class="ff3" style="font-size:22px;"><a href="https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html" target="_self">Building powerful image classification models using very little data</a></li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>Hope you enjoyed this article! If you’d like to stay connected, you’ll find me on Twitter </em><a href="https://twitter.com/irhumshafkat" target="_self">here</a><em>. If you have a question, comments are welcome! — I find them to be useful to my own learning process as well.</em></p></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>