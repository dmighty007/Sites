<!DOCTYPE html>
                <html>
                <head>
                    <title>A review of Dropout as applied to RNNs</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://adriangcoder.medium.com/a-review-of-dropout-as-applied-to-rnns-72e79ecd5b7b"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://medium.com/?source=post_page-----72e79ecd5b7b--------------------------------">Author : Adrian G</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>A review of Dropout as applied to RNNs</h3></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*4YA0b_kWMY0gefFI3Wd7Lw.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*4YA0b_kWMY0gefFI3Wd7Lw.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*4YA0b_kWMY0gefFI3Wd7Lw.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*4YA0b_kWMY0gefFI3Wd7Lw.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*4YA0b_kWMY0gefFI3Wd7Lw.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*4YA0b_kWMY0gefFI3Wd7Lw.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4YA0b_kWMY0gefFI3Wd7Lw.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*4YA0b_kWMY0gefFI3Wd7Lw.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*4YA0b_kWMY0gefFI3Wd7Lw.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*4YA0b_kWMY0gefFI3Wd7Lw.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*4YA0b_kWMY0gefFI3Wd7Lw.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*4YA0b_kWMY0gefFI3Wd7Lw.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*4YA0b_kWMY0gefFI3Wd7Lw.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*4YA0b_kWMY0gefFI3Wd7Lw.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*4YA0b_kWMY0gefFI3Wd7Lw.jpeg"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Not this sort of dropout.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In this post I will provide a background and overview of dropout and an analysis of dropout parameters as applied to language modelling using LSTM/GRU recurrent neural networks. After taking part 2 of the <a href="http://course.fast.ai/part2.html" target="_self">Deep Learning for Coders</a> online course earlier this year I became intrigued by the application of RNNs to natural language processing. Core components of the fastai <a href="https://github.com/fastai" target="_self">codebase</a> were borrowed from the<a href="https://github.com/salesforce/awd-lstm-lm" target="_self"> awd-lstm-lm</a> project, and on delving into it’s code I wanted to better understand the regularisation strategies used.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In <a href="https://medium.com/@bingobee01/a-review-of-dropout-as-applied-to-rnns-part-2-4e35ba3a4360" target="_self">part 2</a> of this blog post I will show results of analysis on the effect and importance of dropout parameter variation on the resultant loss for RNNs used for language modelling and translation problems.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Dropout</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Originally motivated by the role of sex in evolution, dropout was proposed by Hinton et al. (2012), whereby a unit in a neural network is temporarily removed from a network. Srivastava et al. (2014) applied dropout to feed forward neural network’s and RBM’s and noted a probability of dropout around 0.5 for hidden units and 0.2 for inputs worked well for a variety of tasks.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*D8jriroKkjno8RztHKmMnA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*D8jriroKkjno8RztHKmMnA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*D8jriroKkjno8RztHKmMnA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*D8jriroKkjno8RztHKmMnA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*D8jriroKkjno8RztHKmMnA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*D8jriroKkjno8RztHKmMnA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D8jriroKkjno8RztHKmMnA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*D8jriroKkjno8RztHKmMnA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*D8jriroKkjno8RztHKmMnA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*D8jriroKkjno8RztHKmMnA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*D8jriroKkjno8RztHKmMnA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*D8jriroKkjno8RztHKmMnA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*D8jriroKkjno8RztHKmMnA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*D8jriroKkjno8RztHKmMnA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*D8jriroKkjno8RztHKmMnA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Fig 1. After Srivastava et al. 2014. Dropout Neural Net Model. a) A standard neural net, with no dropout. b) Neural net with dropout applied.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The core concept of Srivastava el al. (2014) is that “e<em>ach hidden unit in a neural network trained with dropout must learn to work with a randomly chosen sample of other units. This should make each hidden unit more robust and drive it towards creating useful features on its own without relying on other hidden units to correct its mistakes.</em>”. “<em>In a standard neural network, the derivative received by each parameter tells it how it should change so the final loss function is reduced, given what all other units are doing. Therefore, units may change in a way that they fix up the mistakes of the other units. This may lead to complex co-adaptations. This in turn leads to overfitting because these co-adaptations do not generalize to unseen data.</em>” Srivastava et al. (2014) hypothesize that by making the presence of other hidden units unreliable, dropout prevents co-adaptation of each hidden unit.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">For each training sample the network is re-adjusted and a new set of neurons are dropped out. At test time the weights are multiplied by their probability of their associated units’ dropout.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*SgxDyfnugZrsWUcsxRLO4g.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*SgxDyfnugZrsWUcsxRLO4g.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*SgxDyfnugZrsWUcsxRLO4g.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*SgxDyfnugZrsWUcsxRLO4g.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*SgxDyfnugZrsWUcsxRLO4g.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*SgxDyfnugZrsWUcsxRLO4g.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SgxDyfnugZrsWUcsxRLO4g.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*SgxDyfnugZrsWUcsxRLO4g.png 640w, https://miro.medium.com/v2/resize:fit:720/1*SgxDyfnugZrsWUcsxRLO4g.png 720w, https://miro.medium.com/v2/resize:fit:750/1*SgxDyfnugZrsWUcsxRLO4g.png 750w, https://miro.medium.com/v2/resize:fit:786/1*SgxDyfnugZrsWUcsxRLO4g.png 786w, https://miro.medium.com/v2/resize:fit:828/1*SgxDyfnugZrsWUcsxRLO4g.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*SgxDyfnugZrsWUcsxRLO4g.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*SgxDyfnugZrsWUcsxRLO4g.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*SgxDyfnugZrsWUcsxRLO4g.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Fig 2. After Srivastava el al., (2014). Effect of dropout rate on a) Constant number (n) of hidden units. b) Variable number of hidden units (n) multiplied by variable dropout probability (p) so that the number of hidden units after dropout (pn) is held constant.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We can see in Figure 2 a) that the test error is stable between around 0.4 and 0.8 probability of retaining a neuron (1-dropout). Test time errors increase as dropout is decreased below c. 0.2 (P&gt;0.8), and with too much dropout (p&lt;0.3) the network underfits.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Srivatava et al. (2104) further find that “a<em>s the size of the data set is increased, the gain from doing dropout increases up to a point and then declines. This suggests that for any given architecture and dropout rate, there is a “sweet spot” corresponding to some amount of data that is large enough to not be memorized in spite of the noise but not so large that overfitting is not a problem anyways.</em>”</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Srivastava et al. (2014) multiplied hidden activations by Bernoulli distributed random variables which take the value 1 with probability p and 0 otherwise.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:19%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 166px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Pz_La_FQkedMQ4zwROas0g.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Pz_La_FQkedMQ4zwROas0g.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Pz_La_FQkedMQ4zwROas0g.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Pz_La_FQkedMQ4zwROas0g.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Pz_La_FQkedMQ4zwROas0g.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Pz_La_FQkedMQ4zwROas0g.png 1100w, https://miro.medium.com/v2/resize:fit:332/format:webp/1*Pz_La_FQkedMQ4zwROas0g.png 332w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 166px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Pz_La_FQkedMQ4zwROas0g.png 640w, https://miro.medium.com/v2/resize:fit:720/1*Pz_La_FQkedMQ4zwROas0g.png 720w, https://miro.medium.com/v2/resize:fit:750/1*Pz_La_FQkedMQ4zwROas0g.png 750w, https://miro.medium.com/v2/resize:fit:786/1*Pz_La_FQkedMQ4zwROas0g.png 786w, https://miro.medium.com/v2/resize:fit:828/1*Pz_La_FQkedMQ4zwROas0g.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*Pz_La_FQkedMQ4zwROas0g.png 1100w, https://miro.medium.com/v2/resize:fit:332/1*Pz_La_FQkedMQ4zwROas0g.png 332w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/166/1*Pz_La_FQkedMQ4zwROas0g.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Eq 1. Probability density function of a Bernoulli distribution of two outcomes — (in this case drop neuron or not) where probability of drop is given by p. The simpest example of a Bernoulli distribution is a coin toss, in which cas the probability (p) of heads is 0.5.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Source code for an example dropout layer is shown below.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>class Dropout():</span><span>    def __init__(self, prob=0.5):</span><span>        self.prob = prob</span><span>        self.params = []</span><span>    def forward(self,X):</span><span>        self.mask = np.random.binomial(1,self.prob,size=X.shape) / self.prob</span><span>        out = X * self.mask</span><span>    return out.reshape(X.shape)</span><span>    def backward(self,dout):</span><span>        dX = dout * self.mask</span><span>        return dX,[]</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Code 1: after deepnotes.io</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>DropConnect</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Building further on Dropout, Wan et al. (2013) proposed DropConnect which “<em>generalizes Dropout by randomly dropping the weights rather than the activations</em>”. “<em>With Drop connect each connection, rather than each output unit can be dropped with probability 1 − p</em>” Wan et al. (2013). Like Dropout, the technique was only applied to fully connected layers.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*spyp4wufkbvzCzEQdKg5Zg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*spyp4wufkbvzCzEQdKg5Zg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*spyp4wufkbvzCzEQdKg5Zg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*spyp4wufkbvzCzEQdKg5Zg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*spyp4wufkbvzCzEQdKg5Zg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*spyp4wufkbvzCzEQdKg5Zg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*spyp4wufkbvzCzEQdKg5Zg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*spyp4wufkbvzCzEQdKg5Zg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*spyp4wufkbvzCzEQdKg5Zg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*spyp4wufkbvzCzEQdKg5Zg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*spyp4wufkbvzCzEQdKg5Zg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*spyp4wufkbvzCzEQdKg5Zg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*spyp4wufkbvzCzEQdKg5Zg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*spyp4wufkbvzCzEQdKg5Zg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*spyp4wufkbvzCzEQdKg5Zg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Fig 3. After Wan et al. (2013) (a): An example model layout for a single DropConnect layer. After running feature extractor g() on input x, a random instantiation of the mask M (e.g. (b)), masks out the weight matrix W. The masked weights are multiplied with this feature vector to produce u which is the input to an activation function a and a softmax layer s. For comparison, c) shows an effective weight mask for elements that Dropout uses when applied to the previous layer’s output (columns) and this layer’s output (rows).</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">How DropConnect differers from Droput can be visualised when we see the basic structure of a neuron in neural net, as per the figure below. By appling dropout to input weights rather than the activations, DropConnect generalizes to the entire connectivity structure of a fully connected neural network layer.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*ZsnVe7r2856hseYhGiYpbg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*ZsnVe7r2856hseYhGiYpbg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*ZsnVe7r2856hseYhGiYpbg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*ZsnVe7r2856hseYhGiYpbg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*ZsnVe7r2856hseYhGiYpbg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ZsnVe7r2856hseYhGiYpbg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZsnVe7r2856hseYhGiYpbg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*ZsnVe7r2856hseYhGiYpbg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*ZsnVe7r2856hseYhGiYpbg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*ZsnVe7r2856hseYhGiYpbg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*ZsnVe7r2856hseYhGiYpbg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*ZsnVe7r2856hseYhGiYpbg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*ZsnVe7r2856hseYhGiYpbg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*ZsnVe7r2856hseYhGiYpbg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*ZsnVe7r2856hseYhGiYpbg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Fig 4. after ml-cheatsheet.readthedocs.io. A neuron takes as an input a series of weights and applies a non-linear activation function to generate an output.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The two dropout methodologies mentioned above were applied to feed-foward convolutional neural networks. RNN’s differ from feed-forward only neural nets in that previous state is fed-back into the network, allowing the network to retain memory of previous states. <mark>As such, applying standard dropout to RNN’s tends limits the ability of the networks to retain their memory, hindering their performance.</mark> The issue with applying dropout to a recurrent neural network (RNN) was noted by Bayer et al. (2013) in that if the complete outgoing weight vecors were set to zero, the “<em>resulting changes to the dynamics of an RNN during every forward pass are quite dramatic.</em>”.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*IUgrgHZ6_fgsKRifvoDQJA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*IUgrgHZ6_fgsKRifvoDQJA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*IUgrgHZ6_fgsKRifvoDQJA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*IUgrgHZ6_fgsKRifvoDQJA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*IUgrgHZ6_fgsKRifvoDQJA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*IUgrgHZ6_fgsKRifvoDQJA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IUgrgHZ6_fgsKRifvoDQJA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*IUgrgHZ6_fgsKRifvoDQJA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*IUgrgHZ6_fgsKRifvoDQJA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*IUgrgHZ6_fgsKRifvoDQJA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*IUgrgHZ6_fgsKRifvoDQJA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*IUgrgHZ6_fgsKRifvoDQJA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*IUgrgHZ6_fgsKRifvoDQJA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*IUgrgHZ6_fgsKRifvoDQJA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*IUgrgHZ6_fgsKRifvoDQJA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Fig 5. Example of a regual feed forward and (also feed forward) Convolutional Neural Network (ConvNet) after cs231n.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*eCf2x2XdGk89YP_t5-C33g.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*eCf2x2XdGk89YP_t5-C33g.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*eCf2x2XdGk89YP_t5-C33g.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*eCf2x2XdGk89YP_t5-C33g.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*eCf2x2XdGk89YP_t5-C33g.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*eCf2x2XdGk89YP_t5-C33g.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eCf2x2XdGk89YP_t5-C33g.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*eCf2x2XdGk89YP_t5-C33g.png 640w, https://miro.medium.com/v2/resize:fit:720/1*eCf2x2XdGk89YP_t5-C33g.png 720w, https://miro.medium.com/v2/resize:fit:750/1*eCf2x2XdGk89YP_t5-C33g.png 750w, https://miro.medium.com/v2/resize:fit:786/1*eCf2x2XdGk89YP_t5-C33g.png 786w, https://miro.medium.com/v2/resize:fit:828/1*eCf2x2XdGk89YP_t5-C33g.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*eCf2x2XdGk89YP_t5-C33g.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*eCf2x2XdGk89YP_t5-C33g.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*eCf2x2XdGk89YP_t5-C33g.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Fig 6. Recurrent neuron after Narwekar and Pampari (2016)</p><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*LHlvad6oEdQeZZXQj2wM7w.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*LHlvad6oEdQeZZXQj2wM7w.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*LHlvad6oEdQeZZXQj2wM7w.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*LHlvad6oEdQeZZXQj2wM7w.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*LHlvad6oEdQeZZXQj2wM7w.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*LHlvad6oEdQeZZXQj2wM7w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LHlvad6oEdQeZZXQj2wM7w.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*LHlvad6oEdQeZZXQj2wM7w.png 640w, https://miro.medium.com/v2/resize:fit:720/1*LHlvad6oEdQeZZXQj2wM7w.png 720w, https://miro.medium.com/v2/resize:fit:750/1*LHlvad6oEdQeZZXQj2wM7w.png 750w, https://miro.medium.com/v2/resize:fit:786/1*LHlvad6oEdQeZZXQj2wM7w.png 786w, https://miro.medium.com/v2/resize:fit:828/1*LHlvad6oEdQeZZXQj2wM7w.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*LHlvad6oEdQeZZXQj2wM7w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*LHlvad6oEdQeZZXQj2wM7w.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*LHlvad6oEdQeZZXQj2wM7w.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Fig 7. Unfolding an RNN after Narwekar and Pampari (2016)</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Example code to show how a RNN keeps this hidden state can bee seen in the code below from karpathy.github.io:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>class RNN:</span><span># ...</span><span>def step(self, x):</span><span>    # update the hidden state</span><span>    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))</span><span>    # compute the output vector</span><span>    y = np.dot(self.W_hy, self.h)</span><span>    return y</span><span>rnn = RNN()</span><span>y = rnn.step(x) # x is an input vector, y is the RNN's output vector</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Dropout applied to RNN’s</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">As a way of overcoming performance issues with dropout applied to RNN’s, Zaremba et al. (2014) and Pham et al. (2013) applied dropout only to the non-recurrent connections (Dropout was not applied to the hidden states). “<em>By not using dropout on the recurrent connections, the LSTM can benefit from dropout regularization without sacrificing its valuable memorization ability</em>.” (Zaremba et al.,2014)</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:66%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 496px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*-7pHmlcrdZkcX5BfcarTPA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*-7pHmlcrdZkcX5BfcarTPA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*-7pHmlcrdZkcX5BfcarTPA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*-7pHmlcrdZkcX5BfcarTPA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*-7pHmlcrdZkcX5BfcarTPA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*-7pHmlcrdZkcX5BfcarTPA.png 1100w, https://miro.medium.com/v2/resize:fit:992/format:webp/1*-7pHmlcrdZkcX5BfcarTPA.png 992w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 496px" srcset="https://miro.medium.com/v2/resize:fit:640/1*-7pHmlcrdZkcX5BfcarTPA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*-7pHmlcrdZkcX5BfcarTPA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*-7pHmlcrdZkcX5BfcarTPA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*-7pHmlcrdZkcX5BfcarTPA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*-7pHmlcrdZkcX5BfcarTPA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*-7pHmlcrdZkcX5BfcarTPA.png 1100w, https://miro.medium.com/v2/resize:fit:992/1*-7pHmlcrdZkcX5BfcarTPA.png 992w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/496/1*-7pHmlcrdZkcX5BfcarTPA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Fig 8. after Zaremba et al. (2014) Regularized multilayer RNN. Dropout is only applied to the non-recurrent connections (ie only applied to the feedforward dashed lines). The thick line shows a typical path of information flow in the LSTM. The information is affected by dropout L + 1 times, where L is depth of network.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Variational Dropout</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Gal and Ghahramani (2015) anaysed the application of dropout to the feedforward only parts of a RNN and found this approach still leads to overfitting. They proposed ‘variational dropout’ where by repeating “<em>the same dropout mask at each time step for both inputs, outputs, and recurrent layers (drop the same network units at each time step)”</em> using a bayesian interpretaton , they saw an improvement in Language Modelling and Sentiment Analysis tasks over ‘naive dropout’.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*x0Qr982CQMv6lBRgsA9BUw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*x0Qr982CQMv6lBRgsA9BUw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*x0Qr982CQMv6lBRgsA9BUw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*x0Qr982CQMv6lBRgsA9BUw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*x0Qr982CQMv6lBRgsA9BUw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*x0Qr982CQMv6lBRgsA9BUw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x0Qr982CQMv6lBRgsA9BUw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*x0Qr982CQMv6lBRgsA9BUw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*x0Qr982CQMv6lBRgsA9BUw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*x0Qr982CQMv6lBRgsA9BUw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*x0Qr982CQMv6lBRgsA9BUw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*x0Qr982CQMv6lBRgsA9BUw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*x0Qr982CQMv6lBRgsA9BUw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*x0Qr982CQMv6lBRgsA9BUw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*x0Qr982CQMv6lBRgsA9BUw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Fig 9. after Gal and Ghahramani (2015). Naive dropout (a) (eg Zaremba et al., 2014) uses different masks at different time steps, with no dropout on the recurrent layers. Variational Dropout (b) uses the same dropout mask at each time step, including the recurrent layers (colours representing dropout masks, solid lines representing dropout, dashed lines representing standard connections with no dropout).</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">‘<strong>Recurrent Dropout’</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Like Moon et al., (2015) and Gal and Ghahramani (2015), Semeniuta et al., (2016) proposed applying dropout to the recurrent connections of RNN’s so that recurrent weights could be regularized to improve performance. Gal and Ghahramani (2015) where use a network’s hidden state as input to sub-networks that compute gate values and cell updates and use dropout is to regularize the sub-networks (Fig. 9b below). Semeniuta et al., (2016) differers in that they consider “<em>the architecture as a whole with the hidden state as its key part and regularize the whole network</em>” (Fig. 9c below). This is similar to the concept of Moon et al., (2015) (as seen in Fig 9a below) but Semeniuta et al., (2016) found that dropping previous states directly as per Moon et al. (2015) produced mixed results and that applying dropout to the hidden state updates vector is a more principled way to drop recurrent connections.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:68%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 506px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*c74xMvJCy50vxACrMH_u3w.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*c74xMvJCy50vxACrMH_u3w.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*c74xMvJCy50vxACrMH_u3w.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*c74xMvJCy50vxACrMH_u3w.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*c74xMvJCy50vxACrMH_u3w.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*c74xMvJCy50vxACrMH_u3w.png 1100w, https://miro.medium.com/v2/resize:fit:1012/format:webp/1*c74xMvJCy50vxACrMH_u3w.png 1012w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 506px" srcset="https://miro.medium.com/v2/resize:fit:640/1*c74xMvJCy50vxACrMH_u3w.png 640w, https://miro.medium.com/v2/resize:fit:720/1*c74xMvJCy50vxACrMH_u3w.png 720w, https://miro.medium.com/v2/resize:fit:750/1*c74xMvJCy50vxACrMH_u3w.png 750w, https://miro.medium.com/v2/resize:fit:786/1*c74xMvJCy50vxACrMH_u3w.png 786w, https://miro.medium.com/v2/resize:fit:828/1*c74xMvJCy50vxACrMH_u3w.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*c74xMvJCy50vxACrMH_u3w.png 1100w, https://miro.medium.com/v2/resize:fit:1012/1*c74xMvJCy50vxACrMH_u3w.png 1012w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/506/1*c74xMvJCy50vxACrMH_u3w.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Eg 1. after Semeniuta et al., (2016), where it, ft, are input and forget gates at step t; gt is the vector of cell updates and ct is the updated cell vector used to update the hidden state ht; and ∗ represent the element-wise multiplication. Dropout d is applied to the update vector gt.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">“O<em>ur technique allows for adding a strong regularizer on the model weights responsible for learning short and long-term dependencies without affecting the ability to capture long- term relationships, which are especially important to model when dealing with natural language.</em>” Semeniuta et al., (2016).</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*mt-i3bduFkOBj481fT13yw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*mt-i3bduFkOBj481fT13yw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*mt-i3bduFkOBj481fT13yw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*mt-i3bduFkOBj481fT13yw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*mt-i3bduFkOBj481fT13yw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*mt-i3bduFkOBj481fT13yw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mt-i3bduFkOBj481fT13yw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*mt-i3bduFkOBj481fT13yw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*mt-i3bduFkOBj481fT13yw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*mt-i3bduFkOBj481fT13yw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*mt-i3bduFkOBj481fT13yw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*mt-i3bduFkOBj481fT13yw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*mt-i3bduFkOBj481fT13yw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*mt-i3bduFkOBj481fT13yw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*mt-i3bduFkOBj481fT13yw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Fig 10. after Semeniuta et al. “<em>Illustration of the three types of dropout in recurrent connections of LSTM networks. Dashed arrows refer to dropped connections. Input connections are omitted for clarity.</em>”. Note how Semeniuta et al. (2016) apply recurrent dropout to the updates to LSTM memory cells.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">“<em>We demonstrate that recurrent dropout is most ef- fective when applied to hidden state update vec- tors in LSTMs rather than to hidden states; (ii) we observe an improvement in the network’s per- formance when our recurrent dropout is coupled with the standard forward dropout, though the extent of this improvement depends on the val- ues of dropout rates; (iii) contrary to our expec- tations, networks trained with per-step and per- sequence mask sampling produce similar results when using our recurrent dropout method, both being better than the dropout scheme proposed by Moon et al. (2015).</em>” Semeniuta et al., (2016).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Zoneout</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In a variation on the dropout philosophy, Krueger et al. (2017) proposed Zoneout where “<em>instead of setting some units’ activations to 0 as in dropout, zoneout randomly replaces some units’ activations with their activations from the previous timestep.</em>” this “<em>makes it easier for the network to preserve information from previous timesteps going forward, and facilitates, rather than hinders, the flow of gradient information going backward</em>”</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*106Zt9ZgKj63N07BKW6Eog.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*106Zt9ZgKj63N07BKW6Eog.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*106Zt9ZgKj63N07BKW6Eog.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*106Zt9ZgKj63N07BKW6Eog.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*106Zt9ZgKj63N07BKW6Eog.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*106Zt9ZgKj63N07BKW6Eog.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*106Zt9ZgKj63N07BKW6Eog.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*106Zt9ZgKj63N07BKW6Eog.png 640w, https://miro.medium.com/v2/resize:fit:720/1*106Zt9ZgKj63N07BKW6Eog.png 720w, https://miro.medium.com/v2/resize:fit:750/1*106Zt9ZgKj63N07BKW6Eog.png 750w, https://miro.medium.com/v2/resize:fit:786/1*106Zt9ZgKj63N07BKW6Eog.png 786w, https://miro.medium.com/v2/resize:fit:828/1*106Zt9ZgKj63N07BKW6Eog.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*106Zt9ZgKj63N07BKW6Eog.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*106Zt9ZgKj63N07BKW6Eog.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*106Zt9ZgKj63N07BKW6Eog.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Fig. 11 after Kruegar et al. (2017) Zoneout as a special case of dropout; ˜ht is the unit h’s hidden activation for the next time step (if not zoned out). Zoneout can be seen as applying dropout on the hidden state delta, ˜ht − ht−1. When this update is dropped out (represented by the dashed line), ht becomes ht−1.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">While both recurrent dropout (Semeniuta et al., 2016) and Zoneout both prevent the loss of long-term memories built up in the states/cells of GRUs/LSTMS<em> “zoneout does this by preserving units’ activations exactly. This difference is most salient when zoning out the hidden states (not the memory cells) of an LSTM, for which there is no analogue in recurrent dropout. Whereas saturated output gates or output nonlinearities would cause recurrent dropout to suffer from vanishing gradients (Bengio et al., 1994), zoned-out units still propagate gradients effectively in this situation. Furthermore, while the recurrent dropout method is specific to LSTMs and GRUs, zoneout generalizes to any model that sequentially builds distributed representations of its input, including vanilla RNNs.</em>” Kruegar et al. (2017).</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*w0la5283_24ZKTKjrW1eAQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*w0la5283_24ZKTKjrW1eAQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*w0la5283_24ZKTKjrW1eAQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*w0la5283_24ZKTKjrW1eAQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*w0la5283_24ZKTKjrW1eAQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*w0la5283_24ZKTKjrW1eAQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w0la5283_24ZKTKjrW1eAQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*w0la5283_24ZKTKjrW1eAQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*w0la5283_24ZKTKjrW1eAQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*w0la5283_24ZKTKjrW1eAQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*w0la5283_24ZKTKjrW1eAQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*w0la5283_24ZKTKjrW1eAQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*w0la5283_24ZKTKjrW1eAQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*w0la5283_24ZKTKjrW1eAQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*w0la5283_24ZKTKjrW1eAQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Fig. 12. after Kruegar et al. (2017) (a) Zoneout, vs (b) the recurrent dropout strategy of (Semeniuta et al., 2016) in an LSTM. Dashed lines are zero-masked; in zoneout, the corresponding dotted lines are masked with the corresponding opposite zero-mask. Rectangular nodes are embedding layers.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The core concept of zoneout for tensorflow:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span><strong>if </strong>self.is_training:<br/>    new_state = (1 - state_part_zoneout_prob) * tf.python.nn_ops.dropout(<br/>                  new_state_part - state_part, (1 - state_part_zoneout_prob), seed=self._seed) + state_part<br/><strong>else</strong>:<br/>    new_state = state_part_zoneout_prob * state_part + (1 - state_part_zoneout_prob) * new_state_part</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>AWD-LSTM</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In a seminal work on regularization of RNNs for language modelling, Merity et al. (2017) proposed an approach they termed ASGD Weight-Dropped LSTM (AWD-LSTM). In this approach Merity et al., (2017) use DropConnect (Wan et al., 2013) on the recurrent hidden to hidden weight matrices, and variational dropout for all other dropout operations, as well as several other regularization strategies including randomized-length backpropagation through time (BPTT), activation regularization (AR), and temporal activation regularization (TAR).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Regarding the application of DropConnect Metity et al. 2017 mention <em>“as the same weights are reused over multiple timesteps, the same individual dropped weights remain dropped for the entirety of the forward and backward pass. The result is similar to variational dropout, which applies the same dropout mask to recurrent connections within the LSTM by performing dropout on ht−1, except that the dropout is applied to the recurrent weights.”.</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">On the use of variational dropout Metity et al. 2017 note that <em>“each example within the mini-batch uses a unique dropout mask, rather than a single dropout mask being used over all examples, ensuring diversity in the elements dropped out.”</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>By utilizing Embedding dropout like Gal & Ghahramani (2016), </em>Metity et al. 2017 futher note that this “<em>is equivalent to performing dropout on the embedding matrix at a word level, where the dropout is broadcast across all the word vector’s embedding.</em>”. “<em>As the dropout occurs on the embedding matrix that is used for a full forward and backward pass, this means that all occurrences of a specific word will disappear within that pass, equivalent to performing variational dropout on the connection between the one-hot embedding and the embedding</em>”.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The code used by Merity et al. 2017 to apply variational dropout:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span><strong>class </strong>LockedDropout(nn.Module):<br/>    <strong>def </strong>__init__(self):<br/>        super().__init__()<br/>    <strong>def </strong>forward(self, x, dropout=0.5):<br/>        <strong>if not </strong>self.training <strong>or not </strong>dropout:<br/>            <strong>return </strong>x<br/>        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)<br/>        mask = Variable(m, requires_grad=<strong>False</strong>) / (1 - dropout)<br/>        mask = mask.expand_as(x)<br/>        <strong>return </strong>mask * x</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">where in the RNNModel(nn.Module) forward method dropout is applied thus (note self.lockdrop = LockedDropout(mask=mask)):</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span><strong>def </strong>forward(self, input, hidden, return_h=<strong>False</strong>):<br/>    emb = embedded_dropout(self.encoder, input, dropout=self.dropoute <strong>if </strong>self.training <strong>else </strong>0)<br/>    emb = self.lockdrop(emb, self.dropouti)<br/>    raw_output = emb<br/>    new_hidden = []<br/>    raw_outputs = []<br/>    outputs = []<br/>    <strong>for </strong>l, rnn <strong>in </strong>enumerate(self.rnns):<br/>        current_input = raw_output<br/>        raw_output, new_h = rnn(raw_output, hidden[l])<br/>        new_hidden.append(new_h)<br/>        raw_outputs.append(raw_output)<br/>        <strong>if </strong>l != self.nlayers - 1:<br/>            raw_output = self.lockdrop(raw_output, self.dropouth)<br/>            outputs.append(raw_output)<br/>    hidden = new_hidden<br/>    output = self.lockdrop(raw_output, self.dropout)<br/>    outputs.append(output)<br/>    result = output.view(output.size(0)*output.size(1), output.size(2))<br/>    <strong>if </strong>return_h:<br/>        <strong>return </strong>result, hidden, raw_outputs, outputs<br/>    <strong>return </strong>result, hidden</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">DropConnect is applied in the __init__ method of the same RNNModel above thus:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span><strong>if </strong>rnn_type == <strong>'LSTM'</strong>:<br/>    self.rnns = [torch.nn.LSTM(ninp <strong>if </strong>l == 0 <strong>else </strong>nhid, nhid <strong>if </strong>l != nlayers - 1 <strong>else </strong>(ninp <strong>if </strong>tie_weights <strong>else </strong>nhid), 1, dropout=0) <strong>for </strong>l <strong>in </strong>range(nlayers)]<br/>    <strong>if </strong>wdrop:<br/>        self.rnns = [WeightDrop(rnn, [<strong>'weight_hh_l0'</strong>], dropout=wdrop) <strong>for </strong>rnn <strong>in </strong>self.rnns]</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">With the key part of the WeightDrop class being the following method:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span><strong>def </strong>_setweights(self):<br/>    <strong>for </strong>name_w <strong>in </strong>self.weights:<br/>        raw_w = getattr(self.module, name_w + <strong>'_raw'</strong>)<br/>        w = <strong>None</strong><br/>        <strong>if </strong>self.variational:<br/>            mask = torch.autograd.Variable(torch.ones(raw_w.size(0), 1))<br/>            <strong>if </strong>raw_w.is_cuda: mask = mask.cuda()<br/>            mask = torch.nn.functional.dropout(mask, p=self.dropout, training=<strong>True</strong>)<br/>            w = mask.expand_as(raw_w) * raw_w<br/>        <strong>else</strong>:<br/>            w = torch.nn.functional.dropout(raw_w, p=self.dropout, training=self.training)<br/>        setattr(self.module, name_w, w)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Fraternal Dropout</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Resarch into dropout regularization has continued, with Zolna et al. 2017 proposing Fraternal Dropout. The methodology of Fraternal Dropout is to “<em>minimize an equally weighted sum of prediction losses from two identical copies of the same LSTM with different dropout masks, and add as a regularization the L2 difference between the predictions (pre-softmax) of the two networks.</em>”. Zolna et al. 2017 note that “the prediction of models with dropout generally vary with different dropout masks” and that ideally final predictions should be invariant to dropout masks. As such Fraternal Dropout attempts to minimize the variance in predictions under different dropout masks.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In fraternal dropout, we simultaneously feed-forward the input sample X through two identical copies of the RNN that share the same parameters θ but with different dropout masks sti and st j at each time step t. This yields two loss values at each time step t given by lt(pt(zt, sti; θ),Y), and lt(pt(zt, stj; θ),Y) as per the equation below:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*h8_rkonsBm04ChiLbZ_LtQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*h8_rkonsBm04ChiLbZ_LtQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*h8_rkonsBm04ChiLbZ_LtQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*h8_rkonsBm04ChiLbZ_LtQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*h8_rkonsBm04ChiLbZ_LtQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*h8_rkonsBm04ChiLbZ_LtQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h8_rkonsBm04ChiLbZ_LtQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*h8_rkonsBm04ChiLbZ_LtQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*h8_rkonsBm04ChiLbZ_LtQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*h8_rkonsBm04ChiLbZ_LtQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*h8_rkonsBm04ChiLbZ_LtQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*h8_rkonsBm04ChiLbZ_LtQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*h8_rkonsBm04ChiLbZ_LtQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*h8_rkonsBm04ChiLbZ_LtQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*h8_rkonsBm04ChiLbZ_LtQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Eq 3 after Zolna et al. 2017. Overall loss function of Fraternal Dropout, where κ is the regularization coefficient, m is the dimensions of pt(zt, st i; θ) and RFD(zt; θ) is the fraternal dropout regularization.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*hDJs3KSb3mtnYd3NbYBGFQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*hDJs3KSb3mtnYd3NbYBGFQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*hDJs3KSb3mtnYd3NbYBGFQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*hDJs3KSb3mtnYd3NbYBGFQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*hDJs3KSb3mtnYd3NbYBGFQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*hDJs3KSb3mtnYd3NbYBGFQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hDJs3KSb3mtnYd3NbYBGFQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*hDJs3KSb3mtnYd3NbYBGFQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*hDJs3KSb3mtnYd3NbYBGFQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*hDJs3KSb3mtnYd3NbYBGFQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*hDJs3KSb3mtnYd3NbYBGFQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*hDJs3KSb3mtnYd3NbYBGFQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*hDJs3KSb3mtnYd3NbYBGFQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*hDJs3KSb3mtnYd3NbYBGFQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*hDJs3KSb3mtnYd3NbYBGFQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Fig 13. after Zolna et al. 2017. Ablation study: Train (left) and validation (right) perplexity on PTB word level model- ing with single layer LSTM (10M parameters). These curves study the learning dynamics of the baseline model, temporal activity regularization (TAR), prediction regularization (PR), activity reg- ularization (AR) and fraternal dropout. Fraternal Dropout converges faster and generalizes better than the other regularizers in comparison.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Curriculum Dropout</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Although currently only applied to feed forward Convolutional Neural Networks, Curriculum Droput proposed by Morerio et al., 2017 is an interesting line of research. Morerio et al. 2017 propose a “scheduling for dropout training applied to deep neural networks. By softly increasing the amount of units to be suppressed layerwise, we achieve an adaptive regularization and provide a better smooth initialization for weight optimization.”</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:88%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 649px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*iWWEoxCIZW_4tcJOGxWCNA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*iWWEoxCIZW_4tcJOGxWCNA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*iWWEoxCIZW_4tcJOGxWCNA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*iWWEoxCIZW_4tcJOGxWCNA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*iWWEoxCIZW_4tcJOGxWCNA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*iWWEoxCIZW_4tcJOGxWCNA.png 1100w, https://miro.medium.com/v2/resize:fit:1298/format:webp/1*iWWEoxCIZW_4tcJOGxWCNA.png 1298w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 649px" srcset="https://miro.medium.com/v2/resize:fit:640/1*iWWEoxCIZW_4tcJOGxWCNA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*iWWEoxCIZW_4tcJOGxWCNA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*iWWEoxCIZW_4tcJOGxWCNA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*iWWEoxCIZW_4tcJOGxWCNA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*iWWEoxCIZW_4tcJOGxWCNA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*iWWEoxCIZW_4tcJOGxWCNA.png 1100w, https://miro.medium.com/v2/resize:fit:1298/1*iWWEoxCIZW_4tcJOGxWCNA.png 1298w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/649/1*iWWEoxCIZW_4tcJOGxWCNA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Fig 14. after Morerio et al., 2017. Curriculum functions for dropout, where dropout is increased with time based on various functions.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>References:</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">J. Bayer, C. Osendorfer, D. Korhammer, N. Chen, S. Urban, P. van der Smagt. 2013. On Fast Dropout and its Applicability to Recurrent Networks.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Y. Bengio, P. Simard, P. Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">cs231n. <a href="https://cs231n.github.io/convolutional-networks/" target="_self">https://cs231n.github.io/convolutional-networks/</a></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">deepnotes.io. <a href="https://deepnotes.io/dropout" target="_self">https://deepnotes.io/dropout</a></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Y. Gal, abd Z. Ghahramani. 2015. A Theoretically Grounded Application of Dropout in Recurrent Neural Networks.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever and R. R. Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">karpathy.github.io. <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_self">https://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">D. Krueger, T. Maharaj, J. Kramár, M. Pezeshki, N. Ballas, N. Rosemary Ke, A. Goyal, Y. Bengio, A. Courville, C. Pal. 2016. Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">S. Merity, N. Shirish Keskar, R. Socher. 2017. Regularizing and Optimizing LSTM Language Models.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">ml-cheatsheet.readthedocs.io. <a href="https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html" target="_self">https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html</a></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">T. Moon, H. Choi, H. Lee, I. Song. 2015. Rnndrop: A novel dropout for rnns.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">P. Morerio, J. Cavazza, R. Volpi, R.Vidal, V. Murino. 2017. Curriculum Dropout</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">A. Narwekar, A. Pampari. 2016. Recurrent Neural Network Architectures. <a href="http://slazebni.cs.illinois.edu/spring17/lec20_rnn.pdf" target="_self">http://slazebni.cs.illinois.edu/spring17/lec20_rnn.pdf</a></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">V. Pham, T. Bluche, C. Kermorvant, J. Louradour. 2013. Dropout improves Recurrent Neural Networks for Handwriting Recognition</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">S. Semeniuta, A. Severyn, E. Barth. 2016. Recurrent Dropout without Memory Loss.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from Overfitting.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">L. Wan, M. Zeiler, Matthew, S. Zhang, Y. LeCun, R. Fergus. 2013. Regularization of neural networks using dropconnect.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">W. Zaremba, I. Sutskever, O. Vinyals. 2014. Recurrent Neural Network Regularization</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">K. Zolna, D. Arpit, D. Suhubdy, Y. Bengio. 2017. Fraternal Dropout</p></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>