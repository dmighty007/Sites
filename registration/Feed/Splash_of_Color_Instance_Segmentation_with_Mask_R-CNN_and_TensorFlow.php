<!DOCTYPE html>
                <html>
                <head>
                    <title>Splash of Color: Instance Segmentation with Mask R-CNN and TensorFlow</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://medium.com/@waleedka?source=post_page-----7c761e238b46--------------------------------">Author : Waleed Abdulla</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>Splash of Color: Instance Segmentation with Mask R-CNN and TensorFlow</h3></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5 text-muted">Explained by building a color splash filter</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Back in November, we open-sourced our <a href="https://github.com/matterport/Mask_RCNN" target="_self">implementation of Mask R-CNN</a>, and since then it’s been forked 1400 times, used in a lot of projects, and improved upon by many generous contributors. We received a lot of questions as well, so in this post I’ll explain how the model works and show how to use it in a real application.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I’ll cover two things: First, an overview of Mask RCNN. And, second, how to train a model from scratch and use it to build a smart color splash filter.</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4"><strong>Code Tip:</strong><br></br>We’re sharing the code <a href="https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon" target="_self">here</a>. Including the dataset I built and the trained model. Follow along!</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">What is Instance Segmentation?</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Instance segmentation is the task of identifying object outlines at the pixel level. Compared to similar computer vision tasks, it’s one of the hardest possible vision tasks. Consider the following asks:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*-zw_Mh1e-8YncnokbAFWxg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*-zw_Mh1e-8YncnokbAFWxg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*-zw_Mh1e-8YncnokbAFWxg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*-zw_Mh1e-8YncnokbAFWxg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*-zw_Mh1e-8YncnokbAFWxg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*-zw_Mh1e-8YncnokbAFWxg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-zw_Mh1e-8YncnokbAFWxg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*-zw_Mh1e-8YncnokbAFWxg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*-zw_Mh1e-8YncnokbAFWxg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*-zw_Mh1e-8YncnokbAFWxg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*-zw_Mh1e-8YncnokbAFWxg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*-zw_Mh1e-8YncnokbAFWxg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*-zw_Mh1e-8YncnokbAFWxg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*-zw_Mh1e-8YncnokbAFWxg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*-zw_Mh1e-8YncnokbAFWxg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><strong>Classification: </strong>There is a balloon in this image.</li><li class="ff3" style="font-size:22px;"><strong>Semantic Segmentation:</strong> These are all the balloon pixels.</li><li class="ff3" style="font-size:22px;"><strong>Object Detection: </strong>There are 7 balloons in this image at these locations. We’re starting to account for objects that overlap.</li><li class="ff3" style="font-size:22px;"><strong>Instance Segmentation</strong>: There are 7 balloons at these locations, and these are the pixels that belong to each one.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Mask R-CNN</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Mask R-CNN (regional convolutional neural network) is a two stage framework: the first stage scans the image and generates <em>proposals</em>(areas likely to contain an object). And the second stage classifies the proposals and generates bounding boxes and masks.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">It was introduced last year via the <a href="https://arxiv.org/abs/1703.06870" target="_self">Mask R-CNN paper</a> to extend its predecessor, <a href="https://arxiv.org/abs/1506.01497" target="_self">Faster R-CNN</a>, by the same authors. Faster R-CNN is a popular framework for object detection, and Mask R-CNN extends it with instance segmentation, among other things.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*IWWOPIYLqqF9i_gXPmBk3g.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*IWWOPIYLqqF9i_gXPmBk3g.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*IWWOPIYLqqF9i_gXPmBk3g.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*IWWOPIYLqqF9i_gXPmBk3g.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*IWWOPIYLqqF9i_gXPmBk3g.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*IWWOPIYLqqF9i_gXPmBk3g.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IWWOPIYLqqF9i_gXPmBk3g.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*IWWOPIYLqqF9i_gXPmBk3g.png 640w, https://miro.medium.com/v2/resize:fit:720/1*IWWOPIYLqqF9i_gXPmBk3g.png 720w, https://miro.medium.com/v2/resize:fit:750/1*IWWOPIYLqqF9i_gXPmBk3g.png 750w, https://miro.medium.com/v2/resize:fit:786/1*IWWOPIYLqqF9i_gXPmBk3g.png 786w, https://miro.medium.com/v2/resize:fit:828/1*IWWOPIYLqqF9i_gXPmBk3g.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*IWWOPIYLqqF9i_gXPmBk3g.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*IWWOPIYLqqF9i_gXPmBk3g.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*IWWOPIYLqqF9i_gXPmBk3g.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Mask R-CNN framework. Source: <a href="https://arxiv.org/abs/1703.06870" target="_self">https://arxiv.org/abs/1703.06870</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">At a high level, Mask R-CNN consists of these modules:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">1. Backbone</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:40%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 309px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*IDjLXsSw5QMFWDudayIBfw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*IDjLXsSw5QMFWDudayIBfw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*IDjLXsSw5QMFWDudayIBfw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*IDjLXsSw5QMFWDudayIBfw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*IDjLXsSw5QMFWDudayIBfw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*IDjLXsSw5QMFWDudayIBfw.png 1100w, https://miro.medium.com/v2/resize:fit:618/format:webp/1*IDjLXsSw5QMFWDudayIBfw.png 618w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 309px" srcset="https://miro.medium.com/v2/resize:fit:640/1*IDjLXsSw5QMFWDudayIBfw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*IDjLXsSw5QMFWDudayIBfw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*IDjLXsSw5QMFWDudayIBfw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*IDjLXsSw5QMFWDudayIBfw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*IDjLXsSw5QMFWDudayIBfw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*IDjLXsSw5QMFWDudayIBfw.png 1100w, https://miro.medium.com/v2/resize:fit:618/1*IDjLXsSw5QMFWDudayIBfw.png 618w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/309/1*IDjLXsSw5QMFWDudayIBfw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Simplified illustration of the backbone nework</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This is a standard convolutional neural network (typically, ResNet50 or ResNet101) that serves as a feature extractor. The early layers detect low level features (edges and corners), and later layers successively detect higher level features (car, person, sky).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Passing through the backbone network, the image is converted from 1024x1024px x 3 (RGB) to a feature map of shape 32x32x2048. This feature map becomes the input for the following stages.</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4"><strong>Code Tip:</strong><br></br>The backbone is built in the function <a href="https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py#L171" target="_self">resnet_graph()</a>. The code supports ResNet50 and ResNet101.</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Feature Pyramid Network</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:60%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 452px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*1sCveJrqfthOQsGGZRs2tQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*1sCveJrqfthOQsGGZRs2tQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*1sCveJrqfthOQsGGZRs2tQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*1sCveJrqfthOQsGGZRs2tQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*1sCveJrqfthOQsGGZRs2tQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*1sCveJrqfthOQsGGZRs2tQ.png 1100w, https://miro.medium.com/v2/resize:fit:904/format:webp/1*1sCveJrqfthOQsGGZRs2tQ.png 904w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 452px" srcset="https://miro.medium.com/v2/resize:fit:640/1*1sCveJrqfthOQsGGZRs2tQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*1sCveJrqfthOQsGGZRs2tQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*1sCveJrqfthOQsGGZRs2tQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*1sCveJrqfthOQsGGZRs2tQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*1sCveJrqfthOQsGGZRs2tQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*1sCveJrqfthOQsGGZRs2tQ.png 1100w, https://miro.medium.com/v2/resize:fit:904/1*1sCveJrqfthOQsGGZRs2tQ.png 904w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/452/1*1sCveJrqfthOQsGGZRs2tQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Source: Feature Pyramid Networks paper</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">While the backbone described above works great, it can be improved upon. The <a href="https://arxiv.org/abs/1612.03144" target="_self">Feature Pyramid Network (FPN)</a> was introduced by the same authors of Mask R-CNN as an extension that can better represent objects at multiple scales.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">FPN improves the standard feature extraction pyramid by adding a second pyramid that takes the high level features from the first pyramid and passes them down to lower layers. By doing so, it allows features at every level to have access to both, lower and higher level features.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Our implementation of Mask RCNN uses a ResNet101 + FPN backbone.</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4"><strong>Code Tip:</strong><br></br> The FPN is created in <a href="https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py#L1840" target="_self">MaskRCNN.build()</a>. The section after building the ResNet. <br></br>RPN introduces additional complexity: rather than a single backbone feature map in the standard backbone (i.e. the top layer of the first pyramid), in FPN there is a feature map at each level of the second pyramid. We pick which to use dynamically depending on the size of the object. I’ll continue to refer to the <strong>backbone feature map</strong> as if it’s one feature map, but keep in mind that when using FPN, we’re actually picking one out of several at runtime.</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">2. Region Proposal Network (RPN)</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:80%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 593px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*ESpJx0XLvyBa86TNo2BfLQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*ESpJx0XLvyBa86TNo2BfLQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*ESpJx0XLvyBa86TNo2BfLQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*ESpJx0XLvyBa86TNo2BfLQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*ESpJx0XLvyBa86TNo2BfLQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ESpJx0XLvyBa86TNo2BfLQ.png 1100w, https://miro.medium.com/v2/resize:fit:1186/format:webp/1*ESpJx0XLvyBa86TNo2BfLQ.png 1186w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 593px" srcset="https://miro.medium.com/v2/resize:fit:640/1*ESpJx0XLvyBa86TNo2BfLQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*ESpJx0XLvyBa86TNo2BfLQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*ESpJx0XLvyBa86TNo2BfLQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*ESpJx0XLvyBa86TNo2BfLQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*ESpJx0XLvyBa86TNo2BfLQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*ESpJx0XLvyBa86TNo2BfLQ.png 1100w, https://miro.medium.com/v2/resize:fit:1186/1*ESpJx0XLvyBa86TNo2BfLQ.png 1186w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/593/1*ESpJx0XLvyBa86TNo2BfLQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Simplified illustration showing 49 anchor boxes</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The RPN is a lightweight neural network that scans the image in a sliding-window fashion and finds areas that contain objects.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The regions that the RPN scans over are called <em>anchors</em>. Which are boxes distributed over the image area, as show on the left. This is a simplified view, though. In practice, there are about 200K anchors of different sizes and aspect ratios, and they overlap to cover as much of the image as possible.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">How fast can the RPN scan that many anchors? Pretty fast, actually. The sliding window is handled by the convolutional nature of the RPN, which allows it to scan all regions in parallel (on a GPU). Further, the RPN doesn’t scan over the image directly (even though we draw the anchors on the image for illustration). <mark>Instead, the RPN scans over the backbone feature map.</mark> This allows the RPN to reuse the extracted features efficiently and avoid duplicate calculations. With these optimizations, the RPN runs in about 10 ms according to the <a href="https://arxiv.org/abs/1506.01497" target="_self">Faster RCNN paper</a> that introduced it. In Mask RCNN we typically use larger images and more anchors, so it might take a bit longer.</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4"><strong>Code Tip:</strong>The RPN is created in <a href="https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py#L831" target="_self">rpn_graph()</a>. Anchor scales and aspect ratios are controlled by RPN_ANCHOR_SCALES and RPN_ANCHOR_RATIOS in <a href="https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/config.py" target="_self">config.py</a>.</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The RPN generates two outputs for each anchor:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:54%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 407px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*EMNE8bxOT4RI3HMjIqjCwQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*EMNE8bxOT4RI3HMjIqjCwQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*EMNE8bxOT4RI3HMjIqjCwQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*EMNE8bxOT4RI3HMjIqjCwQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*EMNE8bxOT4RI3HMjIqjCwQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*EMNE8bxOT4RI3HMjIqjCwQ.png 1100w, https://miro.medium.com/v2/resize:fit:814/format:webp/1*EMNE8bxOT4RI3HMjIqjCwQ.png 814w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 407px" srcset="https://miro.medium.com/v2/resize:fit:640/1*EMNE8bxOT4RI3HMjIqjCwQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*EMNE8bxOT4RI3HMjIqjCwQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*EMNE8bxOT4RI3HMjIqjCwQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*EMNE8bxOT4RI3HMjIqjCwQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*EMNE8bxOT4RI3HMjIqjCwQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*EMNE8bxOT4RI3HMjIqjCwQ.png 1100w, https://miro.medium.com/v2/resize:fit:814/1*EMNE8bxOT4RI3HMjIqjCwQ.png 814w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/407/1*EMNE8bxOT4RI3HMjIqjCwQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">3 anchor boxes (dotted) and the shift/scale applied to them to fit the object precisely (solid). Several anchors can map to the same object.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;"><strong>Anchor Class:</strong> One of two classes: foreground or background. The FG class implies that there is likely an object in that box.</li><li class="ff3" style="font-size:22px;"><strong>Bounding Box Refinement:</strong> A foreground anchor (also called positive anchor) might not be centered perfectly over the object. So the RPN estimates a delta (% change in x, y, width, height) to refine the anchor box to fit the object better.</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Using the RPN predictions, we pick the top anchors that are likely to contain objects and refine their location and size. If several anchors overlap too much, we keep the one with the highest foreground score and discard the rest (referred to as Non-max Suppression). After that we have the final <em>proposals </em>(regions of interest)<em> </em>that we pass to the next stage.</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4"><strong>Code Tip:</strong><br></br>The <a href="https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py#L255" target="_self">ProposalLayer</a> is a custom Keras layer that reads the output of the RPN, picks top anchors, and applies bounding box refinement.</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">3. ROI Classifier & Bounding Box Regressor</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This stage runs on the regions of interest (ROIs) proposed by the RPN. And just like the RPN, it generates two outputs for each ROI:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*xQYuM_9mu5kt8nNN8Ms2TQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*xQYuM_9mu5kt8nNN8Ms2TQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*xQYuM_9mu5kt8nNN8Ms2TQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*xQYuM_9mu5kt8nNN8Ms2TQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*xQYuM_9mu5kt8nNN8Ms2TQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*xQYuM_9mu5kt8nNN8Ms2TQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xQYuM_9mu5kt8nNN8Ms2TQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*xQYuM_9mu5kt8nNN8Ms2TQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*xQYuM_9mu5kt8nNN8Ms2TQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*xQYuM_9mu5kt8nNN8Ms2TQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*xQYuM_9mu5kt8nNN8Ms2TQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*xQYuM_9mu5kt8nNN8Ms2TQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*xQYuM_9mu5kt8nNN8Ms2TQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*xQYuM_9mu5kt8nNN8Ms2TQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*xQYuM_9mu5kt8nNN8Ms2TQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Illustration of stage 2. Source: Fast R-CNN (<a href="https://arxiv.org/abs/1504.08083" target="_self">https://arxiv.org/abs/1504.08083</a>)</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;"><strong>Class:</strong> The class of the object in the ROI. Unlike the RPN, which has two classes (FG/BG), this network is deeper and has the capacity to classify regions to specific classes (person, car, chair, …etc.). It can also generate a <em>background</em> class, which causes the ROI to be discarded.</li><li class="ff3" style="font-size:22px;"><strong>Bounding Box Refinement:</strong> Very similar to how it’s done in the RPN, and its purpose is to further refine the location and size of the bounding box to encapsulate the object.</li></ol></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4"><strong>Code Tip:</strong><br></br>The classifier and bounding box regressor are created in <a href="https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py#L901" target="_self">fpn_classifier_graph()</a>.</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">ROI Pooling</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">There is a bit of a problem to solve before we continue. Classifiers don’t handle variable input size very well. They typically require a fixed input size. But, due to the bounding box refinement step in the RPN, the ROI boxes can have different sizes. That’s where ROI Pooling comes into play.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:88%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 645px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*bsT00ickNk7vaRJNrTvKPQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*bsT00ickNk7vaRJNrTvKPQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*bsT00ickNk7vaRJNrTvKPQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*bsT00ickNk7vaRJNrTvKPQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*bsT00ickNk7vaRJNrTvKPQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*bsT00ickNk7vaRJNrTvKPQ.png 1100w, https://miro.medium.com/v2/resize:fit:1290/format:webp/1*bsT00ickNk7vaRJNrTvKPQ.png 1290w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 645px" srcset="https://miro.medium.com/v2/resize:fit:640/1*bsT00ickNk7vaRJNrTvKPQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*bsT00ickNk7vaRJNrTvKPQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*bsT00ickNk7vaRJNrTvKPQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*bsT00ickNk7vaRJNrTvKPQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*bsT00ickNk7vaRJNrTvKPQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*bsT00ickNk7vaRJNrTvKPQ.png 1100w, https://miro.medium.com/v2/resize:fit:1290/1*bsT00ickNk7vaRJNrTvKPQ.png 1290w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/645/1*bsT00ickNk7vaRJNrTvKPQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">The feature map here is from a low-level layer, for illustration, to make it easier to understand.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">ROI pooling refers to cropping a part of a feature map and resizing it to a fixed size. It’s similar in principle to cropping part of an image and then resizing it (but there are differences in implementation details).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The authors of Mask R-CNN suggest a method they named ROIAlign, in which they sample the feature map at different points and apply a bilinear interpolation. In our implementation, we used TensorFlow’s <a href="https://www.tensorflow.org/api_docs/python/tf/image/crop_and_resize" target="_self">crop_and_resize</a> function for simplicity and because it’s close enough for most purposes.</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4"><strong>Code Tip:</strong><br></br>ROI pooling is implemented in the class <a href="https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py#L344" target="_self">PyramidROIAlign</a>.</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">4. Segmentation Masks</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you stop at the end of the last section then you have a <a href="https://arxiv.org/abs/1506.01497" target="_self">Faster R-CNN</a> framework for object detection. The mask network is the addition that the Mask R-CNN paper introduced.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:61%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 455px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*l55WzUq1ZD2b5EGwW05LDA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*l55WzUq1ZD2b5EGwW05LDA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*l55WzUq1ZD2b5EGwW05LDA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*l55WzUq1ZD2b5EGwW05LDA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*l55WzUq1ZD2b5EGwW05LDA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*l55WzUq1ZD2b5EGwW05LDA.png 1100w, https://miro.medium.com/v2/resize:fit:910/format:webp/1*l55WzUq1ZD2b5EGwW05LDA.png 910w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 455px" srcset="https://miro.medium.com/v2/resize:fit:640/1*l55WzUq1ZD2b5EGwW05LDA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*l55WzUq1ZD2b5EGwW05LDA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*l55WzUq1ZD2b5EGwW05LDA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*l55WzUq1ZD2b5EGwW05LDA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*l55WzUq1ZD2b5EGwW05LDA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*l55WzUq1ZD2b5EGwW05LDA.png 1100w, https://miro.medium.com/v2/resize:fit:910/1*l55WzUq1ZD2b5EGwW05LDA.png 910w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/455/1*l55WzUq1ZD2b5EGwW05LDA.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The mask branch is a convolutional network that takes the positive regions selected by the ROI classifier and generates masks for them. The generated masks are low resolution: 28x28 pixels. But they are <em>soft</em> masks, represented by float numbers, so they hold more details than binary masks. The small mask size helps keep the mask branch light. During training, we scale down the ground-truth masks to 28x28 to compute the loss, and during inferencing we scale up the predicted masks to the size of the ROI bounding box and that gives us the final masks, one per object.</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4"><strong>Code Tip:</strong><br></br>The mask branch is in <a href="https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py#L957" target="_self">build_fpn_mask_graph()</a>.</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Let’s Build a Color Splash Filter</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:61%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 460px" srcset="https://miro.medium.com/v2/resize:fit:640/1*lAP6vX1tLQaxFn6XGEQ32g.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*lAP6vX1tLQaxFn6XGEQ32g.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*lAP6vX1tLQaxFn6XGEQ32g.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*lAP6vX1tLQaxFn6XGEQ32g.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*lAP6vX1tLQaxFn6XGEQ32g.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*lAP6vX1tLQaxFn6XGEQ32g.gif 1100w, https://miro.medium.com/v2/resize:fit:920/1*lAP6vX1tLQaxFn6XGEQ32g.gif 920w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 460px" srcset="https://miro.medium.com/v2/resize:fit:640/1*lAP6vX1tLQaxFn6XGEQ32g.gif 640w, https://miro.medium.com/v2/resize:fit:720/1*lAP6vX1tLQaxFn6XGEQ32g.gif 720w, https://miro.medium.com/v2/resize:fit:750/1*lAP6vX1tLQaxFn6XGEQ32g.gif 750w, https://miro.medium.com/v2/resize:fit:786/1*lAP6vX1tLQaxFn6XGEQ32g.gif 786w, https://miro.medium.com/v2/resize:fit:828/1*lAP6vX1tLQaxFn6XGEQ32g.gif 828w, https://miro.medium.com/v2/resize:fit:1100/1*lAP6vX1tLQaxFn6XGEQ32g.gif 1100w, https://miro.medium.com/v2/resize:fit:920/1*lAP6vX1tLQaxFn6XGEQ32g.gif 920w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/460/1*lAP6vX1tLQaxFn6XGEQ32g.gif"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Sample generated by this project</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Unlike most image editing apps that include this filter, our filter will be a bit smarter: It finds the objects automatically. Which becomes even more useful if you want to apply it to videos rather than a single image.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Training Dataset</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Typically, I’d start by searching for public datasets that contain the objects I need. But in this case, I wanted to document the full cycle and show how to build a dataset from scratch.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I searched for balloon images on flickr, limiting the license type to “Commercial use & mods allowed”. This returned more than enough images for my needs. I picked a total of 75 images and divided them into a training set and a validation set. Finding images is easy. Annotating them is the hard part.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Q4tCdhwrklvJLM9zn5aDhg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Q4tCdhwrklvJLM9zn5aDhg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Q4tCdhwrklvJLM9zn5aDhg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Q4tCdhwrklvJLM9zn5aDhg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Q4tCdhwrklvJLM9zn5aDhg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Q4tCdhwrklvJLM9zn5aDhg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q4tCdhwrklvJLM9zn5aDhg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Q4tCdhwrklvJLM9zn5aDhg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*Q4tCdhwrklvJLM9zn5aDhg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*Q4tCdhwrklvJLM9zn5aDhg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*Q4tCdhwrklvJLM9zn5aDhg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*Q4tCdhwrklvJLM9zn5aDhg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*Q4tCdhwrklvJLM9zn5aDhg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*Q4tCdhwrklvJLM9zn5aDhg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*Q4tCdhwrklvJLM9zn5aDhg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Wait! Don’t we need, like, a million images to train a deep learning model? Sometimes you do, but often you don’t. I’m relying on two main points to reduce my training requirements significantly:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">First, <em>transfer learning. </em>Which simply means that, instead of training a model from scratch, I start with a weights file that’s been trained on the COCO dataset (we provide that in the github repo). Although the COCO dataset does <strong>not</strong> contain a balloon class, it contains a lot of other images (~120K), so the trained weights have already learned a lot of the features common in natural images, which really helps. And, second, given the simple use case here, I’m not demanding high accuracy from this model, so the tiny dataset should suffice.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">There are a lot of tools to annotate images. I ended up using <a href="http://www.robots.ox.ac.uk/~vgg/software/via/" target="_self">VIA (VGG Image Annotator)</a> because of its simplicity. It’s a single HTML file that you download and open in a browser. Annotating the first few images was very slow, but once I got used to the user interface, I was annotating at around an object a minute.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*6SICkQA-YCLp88A7GFM4Ag.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*6SICkQA-YCLp88A7GFM4Ag.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*6SICkQA-YCLp88A7GFM4Ag.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*6SICkQA-YCLp88A7GFM4Ag.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*6SICkQA-YCLp88A7GFM4Ag.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*6SICkQA-YCLp88A7GFM4Ag.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6SICkQA-YCLp88A7GFM4Ag.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*6SICkQA-YCLp88A7GFM4Ag.png 640w, https://miro.medium.com/v2/resize:fit:720/1*6SICkQA-YCLp88A7GFM4Ag.png 720w, https://miro.medium.com/v2/resize:fit:750/1*6SICkQA-YCLp88A7GFM4Ag.png 750w, https://miro.medium.com/v2/resize:fit:786/1*6SICkQA-YCLp88A7GFM4Ag.png 786w, https://miro.medium.com/v2/resize:fit:828/1*6SICkQA-YCLp88A7GFM4Ag.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*6SICkQA-YCLp88A7GFM4Ag.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*6SICkQA-YCLp88A7GFM4Ag.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*6SICkQA-YCLp88A7GFM4Ag.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">UI of the VGG Image Annotator tool</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you don’t like the VIA tool, here is a list of the other tools I tested:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><a href="http://labelme2.csail.mit.edu/" target="_self">LabelMe</a>: One of the most known tools. The UI was a bit too slow, though, especially when zooming in on large images.</li><li class="ff3" style="font-size:22px;"><a href="https://rectlabel.com/" target="_self">RectLabel</a>: Simple and easy to work with. Mac only.</li><li class="ff3" style="font-size:22px;"><a href="https://www.labelbox.io/" target="_self">LabelBox</a>: Pretty good for larger labeling projects and has options for different types of labeling tasks.</li><li class="ff3" style="font-size:22px;"><a href="http://www.robots.ox.ac.uk/~vgg/software/via/" target="_self">VGG Image Annotator (VIA)</a>: Fast, light, and really well designed. This is the one I ended up using.</li><li class="ff3" style="font-size:22px;"><a href="https://github.com/tylin/coco-ui" target="_self">COCO UI</a>: The tool used to annotate the COCO dataset.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Loading the Dataset</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">There isn’t a universally accepted format to store segmentation masks. Some datasets save them as PNG images, others store them as polygon points, and so on. To handle all these cases, our implementation provides a Dataset class that you inherit from and then override a few functions to read your data in whichever format it happens to be.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The VIA tool saves the annotations in a JSON file, and each mask is a set of polygon points. I didn’t find documentation for the format, but it’s pretty easy to figure out by looking at the generated JSON. I included comments in the code to explain how the parsing is done.</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4"><strong>Code Tip:</strong><br></br>An easy way to write code for a new dataset is to copy <a href="https://github.com/matterport/Mask_RCNN/blob/master/samples/coco/coco.py" target="_self">coco.py</a> and modify it to your needs. Which is what I did. I saved the new file as <a href="https://github.com/matterport/Mask_RCNN/blob/v2.1/samples/balloon/balloon.py" target="_self">balloons.py</a></p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">My <code>BalloonDataset</code> class looks like this:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>class <strong>BalloonDataset</strong>(utils.Dataset):</span><span>    def <strong>load_balloons</strong>(self, dataset_dir, subset):<br/>        ...</span><span>    def <strong>load_mask</strong>(self, image_id):<br/>        ...</span><span>    def <strong>image_reference</strong>(self, image_id):<br/>        ...</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><code>load_balloons</code> reads the JSON file, extracts the annotations, and iteratively calls the internal <code>add_class</code> and <code>add_image</code> functions to build the dataset.</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4"><code>load_mask</code> generates bitmap masks for every object in the image by drawing the polygons.</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><code>image_reference</code> simply returns a string that identifies the image for debugging purposes. Here it simply returns the path of the image file.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">You might have noticed that my class doesn’t contain functions to load images or return bounding boxes. The default <code>load_image</code> function in the base <code>Dataset</code> class handles loading images. And, bounding boxes are generated dynamically from the masks.</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4"><strong>Code Tip:</strong><br></br>Your dataset might not be in JSON. My BalloonDataset class reads JSON because that’s what the VIA tool generates. Don’t convert your dataset to a format similar to COCO or the VIA format. Insetad, write your own Dataset class to load whichever format your dataset comes in. See the <a href="https://github.com/matterport/Mask_RCNN/tree/master/samples" target="_self">samples</a> and notice how each uses its own Dataset class.</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">Verify the Dataset</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To verify that my new code is implemented correctly I added this <a href="https://github.com/matterport/Mask_RCNN/blob/v2.1/samples/balloon/inspect_balloon_data.ipynb" target="_self">Jupyter notebook</a>. It loads the dataset, visualizes masks and bounding boxes, and visualizes the anchors to verify that my anchor sizes are a good fit for my object sizes. Here is an example of what you should expect to see:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*OKE6wyZFfh2f_aZ3rd9BRw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*OKE6wyZFfh2f_aZ3rd9BRw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*OKE6wyZFfh2f_aZ3rd9BRw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*OKE6wyZFfh2f_aZ3rd9BRw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*OKE6wyZFfh2f_aZ3rd9BRw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*OKE6wyZFfh2f_aZ3rd9BRw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OKE6wyZFfh2f_aZ3rd9BRw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*OKE6wyZFfh2f_aZ3rd9BRw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*OKE6wyZFfh2f_aZ3rd9BRw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*OKE6wyZFfh2f_aZ3rd9BRw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*OKE6wyZFfh2f_aZ3rd9BRw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*OKE6wyZFfh2f_aZ3rd9BRw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*OKE6wyZFfh2f_aZ3rd9BRw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*OKE6wyZFfh2f_aZ3rd9BRw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*OKE6wyZFfh2f_aZ3rd9BRw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Sample from inspect_balloon_data notebook</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4"><strong>Code Tip:</strong><br></br>To create this notebook I copied <a href="https://github.com/matterport/Mask_RCNN/blob/master/samples/coco/inspect_data.ipynb" target="_self">inspect_data.ipynb</a>, which we wrote for the COCO dataset, and modified one block of code at the top to load the Balloons dataset instead.</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Configurations</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The configurations for this project are similar to the base configuration used to train the COCO dataset, so I just needed to override 3 values. As I did with the <code>Dataset</code> class, I inherit from the base <code>Config</code> class and add my overrides:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>class BalloonConfig(Config):</span><span>    # Give the configuration a recognizable name<br/>    NAME = "balloons"</span><span>    # Number of classes (including background)<br/>    NUM_CLASSES = 1 + 1  # Background + balloon</span><span>    # Number of training steps per epoch<br/>    STEPS_PER_EPOCH = 100</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The base configuration uses input images of size 1024x1024 px for best accuracy. I kept it that way. My images are a bit smaller, but the model resizes them automatically.</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4"><strong>Code Tip:</strong><br></br>The base Config class is in <a href="https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/config.py" target="_self">config.py</a>. And BalloonConfig is in<a href="https://github.com/matterport/Mask_RCNN/blob/v2.1/samples/balloon/balloon.py#L61" target="_self"> balloons.py</a>.</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Training</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Mask R-CNN is a fairly large model. Especially that our implementation uses ResNet101 and FPN. So you need a modern GPU with 12GB of memory. It might work on less, but I haven’t tried. I used <a href="https://aws.amazon.com/ec2/instance-types/p2/" target="_self">Amazon’s P2 instances</a> to train this model, and given the small dataset, training takes less than an hour.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Start the training with this command, running from the <code>balloon</code> directory. Here, we’re specifying that training should start from the pre-trained COCO weights. The code will download the weights from our repository automatically:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>python3 balloon.py train --dataset=/path/to/dataset <strong>--model=coco</strong></span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">And to resume training if it stopped:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>python3 balloon.py train --dataset=/path/to/dataset <strong>--model=last</strong></span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4"><strong>Code Tip:</strong>In addition to balloons.py, the repository has three more examples: <a href="https://github.com/matterport/Mask_RCNN/blob/master/samples/shapes/train_shapes.ipynb" target="_self">train_shapes.ipynb</a> which trains a toy model to detect geometric shapes, <a href="https://github.com/matterport/Mask_RCNN/blob/master/samples/coco/coco.py" target="_self">coco.py</a> which trains on the COCO dataset, and <a href="https://github.com/matterport/Mask_RCNN/tree/master/samples/nucleus" target="_self">nucleus</a> which segments nuclei in microscopy images.</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Inspecting the Results</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The <a href="https://github.com/matterport/Mask_RCNN/blob/v2.1/samples/balloon/inspect_balloon_model.ipynb" target="_self">inspect_balloon_model</a> notebook shows the results generated by the trained model. Check the notebook for more visualizations and a step by step walk through the detection pipeline.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*BvqnziHW514YyO20UNtS3g.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*BvqnziHW514YyO20UNtS3g.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*BvqnziHW514YyO20UNtS3g.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*BvqnziHW514YyO20UNtS3g.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*BvqnziHW514YyO20UNtS3g.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*BvqnziHW514YyO20UNtS3g.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BvqnziHW514YyO20UNtS3g.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*BvqnziHW514YyO20UNtS3g.png 640w, https://miro.medium.com/v2/resize:fit:720/1*BvqnziHW514YyO20UNtS3g.png 720w, https://miro.medium.com/v2/resize:fit:750/1*BvqnziHW514YyO20UNtS3g.png 750w, https://miro.medium.com/v2/resize:fit:786/1*BvqnziHW514YyO20UNtS3g.png 786w, https://miro.medium.com/v2/resize:fit:828/1*BvqnziHW514YyO20UNtS3g.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*BvqnziHW514YyO20UNtS3g.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*BvqnziHW514YyO20UNtS3g.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*BvqnziHW514YyO20UNtS3g.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4"><strong>Code Tip:</strong><br></br>This notebook is a simplified version of <a href="https://github.com/matterport/Mask_RCNN/blob/master/samples/coco/inspect_model.ipynb" target="_self">inspect_mode.ipynb</a>, which includes visualizations and debugging code for the COCO dataset.</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Color Splash</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Finally, now that we have object masks, let’s use them to apply the color splash effect. The method is really simple: create a grayscale version of the image, and then, in areas marked by the object mask, copy back the color pixels from original image. Here is an example:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*iPAtWFnShPhX5atbY3V0pQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*iPAtWFnShPhX5atbY3V0pQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*iPAtWFnShPhX5atbY3V0pQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*iPAtWFnShPhX5atbY3V0pQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*iPAtWFnShPhX5atbY3V0pQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*iPAtWFnShPhX5atbY3V0pQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iPAtWFnShPhX5atbY3V0pQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*iPAtWFnShPhX5atbY3V0pQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*iPAtWFnShPhX5atbY3V0pQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*iPAtWFnShPhX5atbY3V0pQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*iPAtWFnShPhX5atbY3V0pQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*iPAtWFnShPhX5atbY3V0pQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*iPAtWFnShPhX5atbY3V0pQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*iPAtWFnShPhX5atbY3V0pQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*iPAtWFnShPhX5atbY3V0pQ.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4"><strong>Code Tip:</strong><br></br>The code that applies the effect is in the <a href="https://github.com/matterport/Mask_RCNN/blob/v2.1/samples/balloon/balloon.py#L201" target="_self">color_splash()</a> function. And <a href="https://github.com/matterport/Mask_RCNN/blob/v2.1/samples/balloon/balloon.py#L221" target="_self">detect_and_color_splash()</a> handles the whole process from loading the image, running instance segmentation, and applying the color splash filter.</p></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">FAQ</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><strong>Q:</strong> I want to dive deeper and understand the details, what should I read?<br></br><strong>A:</strong> Read these papers in this order: <a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=AF8817DD0F70B32AA08B2ECBBA8099FA?doi=10.1.1.715.2453&rep=rep1&type=pdf" target="_self">RCNN (pdf)</a>, <a href="https://arxiv.org/abs/1504.08083" target="_self">Fast RCNN</a>, <a href="https://arxiv.org/abs/1506.01497" target="_self">Faster RCNN</a>, <a href="https://arxiv.org/abs/1612.03144" target="_self">FPN</a>, <a href="https://arxiv.org/abs/1703.06870" target="_self">Mask RCNN</a>.</li><li class="ff3" style="font-size:22px;"><strong>Q:</strong> Where can I ask more questions?<br></br><strong>A:</strong> The <a href="https://github.com/matterport/Mask_RCNN/issues" target="_self">Issues page on GitHub</a> is active, you can use it for questions, as well as to report issues. Remember to search closed issues as well in case your question has been answered already.</li><li class="ff3" style="font-size:22px;"><strong>Q:</strong> Can I contribute to this project?<br></br><strong>A:</strong> That would be great. Pull Requests are always welcome.</li><li class="ff3" style="font-size:22px;"><strong>Q:</strong> Can I join your team and work on fun projects like this one?<br></br><strong>A:</strong> Yes, we’re hiring for deep learning and computer vision. <a href="https://matterport.com/careers/" target="_self">Apply here</a>.</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*w_ownWZZ38QhiVjVU757DA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*w_ownWZZ38QhiVjVU757DA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*w_ownWZZ38QhiVjVU757DA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*w_ownWZZ38QhiVjVU757DA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*w_ownWZZ38QhiVjVU757DA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*w_ownWZZ38QhiVjVU757DA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w_ownWZZ38QhiVjVU757DA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*w_ownWZZ38QhiVjVU757DA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*w_ownWZZ38QhiVjVU757DA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*w_ownWZZ38QhiVjVU757DA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*w_ownWZZ38QhiVjVU757DA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*w_ownWZZ38QhiVjVU757DA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*w_ownWZZ38QhiVjVU757DA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*w_ownWZZ38QhiVjVU757DA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/max/700/1*w_ownWZZ38QhiVjVU757DA.png"></picture></div><br></div></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>