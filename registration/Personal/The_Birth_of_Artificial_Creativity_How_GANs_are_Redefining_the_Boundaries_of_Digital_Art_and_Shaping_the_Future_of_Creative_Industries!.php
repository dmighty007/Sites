<!DOCTYPE html>
                <html>
                <head>
                    <title>“The Birth of Artificial Creativity: How GANs are Redefining the Boundaries of Digital Art, and Shaping the Future of Creative Industries!”</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://medium.com/@pugliafrancesco3/the-birth-of-artificial-creativity-how-gans-are-redefining-the-boundaries-of-digital-art-and-903468d16b9b"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://medium.com/@pugliafrancesco3?source=post_page-----903468d16b9b--------------------------------">Author : PugliaFrancesco</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>“The Birth of Artificial Creativity: How GANs are Redefining the Boundaries of Digital Art, and Shaping the Future of Creative Industries!”</h3></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*rWJzTja7SpbT6Nb4rPch1Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*rWJzTja7SpbT6Nb4rPch1Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*rWJzTja7SpbT6Nb4rPch1Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*rWJzTja7SpbT6Nb4rPch1Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*rWJzTja7SpbT6Nb4rPch1Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*rWJzTja7SpbT6Nb4rPch1Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rWJzTja7SpbT6Nb4rPch1Q.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*rWJzTja7SpbT6Nb4rPch1Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*rWJzTja7SpbT6Nb4rPch1Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*rWJzTja7SpbT6Nb4rPch1Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*rWJzTja7SpbT6Nb4rPch1Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*rWJzTja7SpbT6Nb4rPch1Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*rWJzTja7SpbT6Nb4rPch1Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*rWJzTja7SpbT6Nb4rPch1Q.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*rWJzTja7SpbT6Nb4rPch1Q.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Index</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><a href="#1fbf" target="_self">What could happen if the turing test had another artificial intelligence as judge instead of a human?</a></li><li class="ff3" style="font-size:22px;"><a href="#073c" target="_self">Architecture of GAN</a></li><li class="ff3" style="font-size:22px;"><a href="#f2ba" target="_self">The characters in the story</a></li><li class="ff3" style="font-size:22px;"><a href="#ce70" target="_self">The Story</a></li><li class="ff3" style="font-size:22px;"><a href="#7e1e" target="_self">The History of GANs</a></li><li class="ff3" style="font-size:22px;"><a href="#b90f" target="_self">How to implement a GAN with Pytorch</a></li><li class="ff3" style="font-size:22px;"><a href="#ab29" target="_self">Possible Future scenarios</a></li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">What could happen if the Turing test had another artificial intelligence as judge instead of a human?</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The Turing Test created by British mathematician Alan Turing in 1950 was designed to determine whether a machine is capable of thinking and behaving like a human being.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Basically, the Turing Test is a kind of imitation game in which a human being and a computer test each other through a series of written conversations. A human judge must interact with both, but without knowing which of the two is responding, and then decide which of the two entities is the human being.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">What makes this test so extraordinary is its ability to challenge our very understanding of what it means to be ‘intelligent’ and ‘human’. Turing’s challenge was one of the first global proposals to define the criteria for so-called ‘general artificial intelligence’, i.e. a form of intelligence that can fully replicate the human being’s ability to reason and adapt.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Even today, more than seventy years after its creation, the Turing Test is still one of the fundamental benchmarks for computer science and artificial intelligence. Its challenge continues to stimulate the critical thinking and creativity of scientists, programmers and thinkers around the world, pushing them ever further in their quests towards achieving true artificial intelligence.</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4"><strong>How are GANs introduced with respect to the Turing test?</strong></p></blockquote></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">GANs, or Generative Adversarial Networks, are a revolutionary technology that has forever changed the way machines generate artificial images. But their connection to the Turing Test goes far beyond the simple challenge between man and machine. Instead of answering questions from a human examiner, GANs use a interaction between a generator and a discriminator to generate increasingly realistic images, constantly challenging their own ability to distinguish between reality and fiction.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The generator creates ‘fake’ images while the discriminator tries to distinguish them from the real thing, in a continuous duel that pushes both networks to constantly improve the output of the generated images, ingenious isn’t it?</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4">I have been up against tough competition all my life. I wouldn’t know how to get along without it.Walt Disney OR Generator</p></blockquote></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Architecture of GAN</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*BRQKreEJc_j67ypNJthizQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*BRQKreEJc_j67ypNJthizQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*BRQKreEJc_j67ypNJthizQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*BRQKreEJc_j67ypNJthizQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*BRQKreEJc_j67ypNJthizQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*BRQKreEJc_j67ypNJthizQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BRQKreEJc_j67ypNJthizQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*BRQKreEJc_j67ypNJthizQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*BRQKreEJc_j67ypNJthizQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*BRQKreEJc_j67ypNJthizQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*BRQKreEJc_j67ypNJthizQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*BRQKreEJc_j67ypNJthizQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*BRQKreEJc_j67ypNJthizQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*BRQKreEJc_j67ypNJthizQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*BRQKreEJc_j67ypNJthizQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Fig: Architecture of GAN Img Ref: <a href="https://developers.google.com/machine-learning/gan/gan_structure" target="_self">Google Developers</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*NoTRxtyGIwacRCkHzXMV-w.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*NoTRxtyGIwacRCkHzXMV-w.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*NoTRxtyGIwacRCkHzXMV-w.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*NoTRxtyGIwacRCkHzXMV-w.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*NoTRxtyGIwacRCkHzXMV-w.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*NoTRxtyGIwacRCkHzXMV-w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NoTRxtyGIwacRCkHzXMV-w.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*NoTRxtyGIwacRCkHzXMV-w.png 640w, https://miro.medium.com/v2/resize:fit:720/1*NoTRxtyGIwacRCkHzXMV-w.png 720w, https://miro.medium.com/v2/resize:fit:750/1*NoTRxtyGIwacRCkHzXMV-w.png 750w, https://miro.medium.com/v2/resize:fit:786/1*NoTRxtyGIwacRCkHzXMV-w.png 786w, https://miro.medium.com/v2/resize:fit:828/1*NoTRxtyGIwacRCkHzXMV-w.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*NoTRxtyGIwacRCkHzXMV-w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*NoTRxtyGIwacRCkHzXMV-w.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*NoTRxtyGIwacRCkHzXMV-w.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">The characters in the story</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I decided to start by customising the protagonists a bit inspired by Daniel Kahneman’s masterpiece “Thinking, Fast and Slow” where he does the same for system 1 and system 2.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><strong>The generator : </strong>is a neural network that is trained to create data that is indistinguishable from the real data set that it is trying to mimic. It does this by attempting to fool a second neural network, known as the discriminator, which is trained to distinguish between real and generated data.I like to see him as a forger. In my story will have the name of Giovanni</li><li class="ff3" style="font-size:22px;"><strong>the discriminator</strong> is a neural network that acts as a classifier. Its main goal is to distinguish between real and fake images produced by the generator. The discriminator takes an input image and produces a single output value, which represents the probability that the input image is real.I like to see him as an art critic who has to work out whether there are forgeries In my story will have the name of Jacques</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:37%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 292px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*2Kj30RAUb0gM65AjhnLSEw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*2Kj30RAUb0gM65AjhnLSEw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*2Kj30RAUb0gM65AjhnLSEw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*2Kj30RAUb0gM65AjhnLSEw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*2Kj30RAUb0gM65AjhnLSEw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*2Kj30RAUb0gM65AjhnLSEw.png 1100w, https://miro.medium.com/v2/resize:fit:584/format:webp/1*2Kj30RAUb0gM65AjhnLSEw.png 584w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 292px" srcset="https://miro.medium.com/v2/resize:fit:640/1*2Kj30RAUb0gM65AjhnLSEw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*2Kj30RAUb0gM65AjhnLSEw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*2Kj30RAUb0gM65AjhnLSEw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*2Kj30RAUb0gM65AjhnLSEw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*2Kj30RAUb0gM65AjhnLSEw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*2Kj30RAUb0gM65AjhnLSEw.png 1100w, https://miro.medium.com/v2/resize:fit:584/1*2Kj30RAUb0gM65AjhnLSEw.png 584w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:292/1*2Kj30RAUb0gM65AjhnLSEw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;"><strong>Giovanni </strong>picture generated by dall-e 2<strong> </strong>note the paint brush</p><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:42%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 325px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Xo-mtsYYVRpDJM9vGlTZjQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Xo-mtsYYVRpDJM9vGlTZjQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Xo-mtsYYVRpDJM9vGlTZjQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Xo-mtsYYVRpDJM9vGlTZjQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Xo-mtsYYVRpDJM9vGlTZjQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Xo-mtsYYVRpDJM9vGlTZjQ.png 1100w, https://miro.medium.com/v2/resize:fit:650/format:webp/1*Xo-mtsYYVRpDJM9vGlTZjQ.png 650w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 325px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Xo-mtsYYVRpDJM9vGlTZjQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*Xo-mtsYYVRpDJM9vGlTZjQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*Xo-mtsYYVRpDJM9vGlTZjQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*Xo-mtsYYVRpDJM9vGlTZjQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*Xo-mtsYYVRpDJM9vGlTZjQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*Xo-mtsYYVRpDJM9vGlTZjQ.png 1100w, https://miro.medium.com/v2/resize:fit:650/1*Xo-mtsYYVRpDJM9vGlTZjQ.png 650w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:325/1*Xo-mtsYYVRpDJM9vGlTZjQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;"><strong>Jacques</strong></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">The Story</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This short story serves to clarify who the protagonists are and their roles before going into the design of a Gan and also because I am a romantic and like to novelise a bit.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In the bustling and vibrant city of Naples, a young forger named Giovanni has a seemingly impossible dream — to create works of art so perfect that even the most experienced experts in the field would be deceived. Despite his humble beginnings, Giovanni’s passion for art and tireless dedication to mastering the most advanced painting techniques eventually lead to the creation of a painting so flawless it could easily pass as the work of a Renaissance master.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In Paris, Jacques, a renowned French art expert, is tasked with examining a collection of old paintings at the prestigious Louvre museum. Giovanni seizes the opportunity to test his skills and presents Jacques with a magnificent painting, claiming it’s from the collection of a wealthy Neapolitan collector. Jacques is initially struck by the painting’s beauty but soon begins to suspect it’s a fake, leading to an intense game of cat-and-mouse between the two.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Giovanni continues to create more and more perfect paintings, challenging Jacques to discover the truth, and their rivalry becomes more and more heated with each new work of art.</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4">Sometimes it is the people who no one imagines anything of, who do the things that no one can imagine.</p></blockquote></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">The History of GANs</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*-GJY_2CbMHbSsOw4gpYUGg.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*-GJY_2CbMHbSsOw4gpYUGg.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*-GJY_2CbMHbSsOw4gpYUGg.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*-GJY_2CbMHbSsOw4gpYUGg.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*-GJY_2CbMHbSsOw4gpYUGg.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*-GJY_2CbMHbSsOw4gpYUGg.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-GJY_2CbMHbSsOw4gpYUGg.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*-GJY_2CbMHbSsOw4gpYUGg.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*-GJY_2CbMHbSsOw4gpYUGg.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*-GJY_2CbMHbSsOw4gpYUGg.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*-GJY_2CbMHbSsOw4gpYUGg.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*-GJY_2CbMHbSsOw4gpYUGg.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*-GJY_2CbMHbSsOw4gpYUGg.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*-GJY_2CbMHbSsOw4gpYUGg.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*-GJY_2CbMHbSsOw4gpYUGg.jpeg"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Ian Goodfellow<a href="https://en.wikipedia.org/wiki/File:Ian_Goodfellow.jpg" target="_self"> https://en.wikipedia.org/wiki/File:Ian_Goodfellow.jpg</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The history of GANs is a compelling journey through innovation and ingenuity, where each step led to new advances that revolutionised the world of artificial intelligence. The inspiration came from the noise contrast estimator, but it was Goodfellow’s genius that led to the development of GANs as we know them today and it was in June 2014.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Many had similar ideas, but only Goodfellow was able to take them forward and create a generative model using randomness in the generator. Since then, GANs have continued to make great strides and have found applications in fields such as generative modelling, image processing, image recognition, Music creation GANs can creatively generate music, which can be used to create new songs or soundtracks and many more.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">GANs have also made their way into the art world, with the creation of unique and captivating abstract paintings and the arrival of AI-enabled artworks. Edmond de Belamy, a painting created with the help of GANs, was even sold at auction for $432,500.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In May 2020, Nvidia researchers taught an AI system to recreate the game of Pac-Man simply by watching it being played.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">But the applications of GANs do not stop there. Samsung researchers have demonstrated the ability to generate videos of a person talking from a single photo, while musicians can generate neural melodies from lyrics using GANs technology. Furthermore, GANs have the potential to revolutionise many other areas of artificial intelligence and machine learning applications.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">GANs are an incredible innovation that has opened new doors to the world of artificial intelligence. The applications are countless and, with each step forward, the technology becomes more sophisticated and powerful.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">How to implement a GAN with Pytorch</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*yQFiLWPd8xSzBo_dsdFdXw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*yQFiLWPd8xSzBo_dsdFdXw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*yQFiLWPd8xSzBo_dsdFdXw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*yQFiLWPd8xSzBo_dsdFdXw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*yQFiLWPd8xSzBo_dsdFdXw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*yQFiLWPd8xSzBo_dsdFdXw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yQFiLWPd8xSzBo_dsdFdXw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*yQFiLWPd8xSzBo_dsdFdXw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*yQFiLWPd8xSzBo_dsdFdXw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*yQFiLWPd8xSzBo_dsdFdXw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*yQFiLWPd8xSzBo_dsdFdXw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*yQFiLWPd8xSzBo_dsdFdXw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*yQFiLWPd8xSzBo_dsdFdXw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*yQFiLWPd8xSzBo_dsdFdXw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*yQFiLWPd8xSzBo_dsdFdXw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">PICTURE GENERATED BY DALL-E 2</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Now, and only now that we have explained the logic behind GANs, their history, and what they are for, can we introduct a more practical topic, how to implement a GAN?</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">How to implement a GAN from a logical perspective in 3 step :</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;">Define the GAN architecture: Decide on the architecture of the generator and the discriminator. The generator takes a noise vector as input and produces an image, while the discriminator takes an image as input and produces a probability that the image is real (1) or generated (0).</li><li class="ff3" style="font-size:22px;">Train the GAN: Train the generator and the discriminator alternately. In each iteration, train the generator to generate images that fool the discriminator, and train the discriminator to distinguish between real images and generated images. It’s important to update the discriminator more times than the generator to avoid a collapse of the generator.</li><li class="ff3" style="font-size:22px;">Evaluate the GAN: Once the GAN has been trained, you can evaluate the images generated by the generator. This typically involves using the generator to generate a large number of images and evaluating their visual quality. Additionally, you can use the discriminator to evaluate the quality of the generated images by checking the probability assigned by the discriminator to the generated images compared to the real images.</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">While there are many other important steps in implementing a GAN, such as preparing the training data and optimizing the model, the 3-step approach I outlined provides a high-level overview of the key components of the GAN architecture and how they work together.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Preparing the training data is an important step in the implementation of a GAN, as it requires a large amount of high-quality data to train the model. This can involve selecting and cleaning the data, as well as preprocessing the images to prepare them for training.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Optimizing the model is also an important step in the implementation of a GAN, as it involves tuning the hyperparameters of the model to improve its performance. This can include adding regularization techniques like batch normalization, and adding noise to the input images.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Additionally, there are many other important considerations when implementing a GAN, such as the choice of loss function, the selection of an appropriate learning rate, and the use of techniques like early stopping to prevent overfitting.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Overall, while the 3-step approach provides a simple and straightforward way to understand the key components of a GAN, the actual implementation of a GAN can be complex and involve many additional steps and considerations.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">THE DATASET</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:78%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 579px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*fJwR0wXJQAxRXrTT1jP3Uw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*fJwR0wXJQAxRXrTT1jP3Uw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*fJwR0wXJQAxRXrTT1jP3Uw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*fJwR0wXJQAxRXrTT1jP3Uw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*fJwR0wXJQAxRXrTT1jP3Uw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*fJwR0wXJQAxRXrTT1jP3Uw.png 1100w, https://miro.medium.com/v2/resize:fit:1158/format:webp/1*fJwR0wXJQAxRXrTT1jP3Uw.png 1158w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 579px" srcset="https://miro.medium.com/v2/resize:fit:640/1*fJwR0wXJQAxRXrTT1jP3Uw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*fJwR0wXJQAxRXrTT1jP3Uw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*fJwR0wXJQAxRXrTT1jP3Uw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*fJwR0wXJQAxRXrTT1jP3Uw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*fJwR0wXJQAxRXrTT1jP3Uw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*fJwR0wXJQAxRXrTT1jP3Uw.png 1100w, https://miro.medium.com/v2/resize:fit:1158/1*fJwR0wXJQAxRXrTT1jP3Uw.png 1158w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:579/1*fJwR0wXJQAxRXrTT1jP3Uw.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The CIFAR-10 dataset is a popular image classification dataset that consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. The 10 classes are:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;">Airplane</li><li class="ff3" style="font-size:22px;">Automobile</li><li class="ff3" style="font-size:22px;">Bird</li><li class="ff3" style="font-size:22px;">Cat</li><li class="ff3" style="font-size:22px;">Deer</li><li class="ff3" style="font-size:22px;">Dog</li><li class="ff3" style="font-size:22px;">Frog</li><li class="ff3" style="font-size:22px;">Horse</li><li class="ff3" style="font-size:22px;">Ship</li><li class="ff3" style="font-size:22px;">Truck</li></ol></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The dataset is split into a training set of 50,000 images and a test set of 10,000 images. The training set is further divided into five batches, each containing 10,000 images. The test set is a single batch of 10,000 images.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The images in the CIFAR-10 dataset were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton at the University of Toronto. The dataset was released in 2009 and has since become a popular benchmark dataset for evaluating image classification algorithms.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">The goal of this GAN</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The goal of this GAN is to generate realistic images of the CIFAR-10 dataset. The Generator network takes random noise as input and generates images that look like they belong to the CIFAR-10 dataset. The Discriminator network is trained to distinguish between real and fake images, and the Generator network is trained to fool the Discriminator network into thinking that its generated images are real. The objective of the GAN is to find the Nash equilibrium between the Discriminator and Generator networks, where the Generator can generate images that are indistinguishable from real images, and the Discriminator cannot tell the difference between real and fake images.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The steps described above for creating a GAN can be seen in the provided code. The Generator and Discriminator networks are defined in the code, as well as the parameters of the GAN. The networks are initialized, and the loss function and optimizers are defined. The GAN is trained for a certain number of epochs, and generated images are saved at the end of each epoch. Finally, new images are generated with the trained Generator and saved for visualization.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">How to Create a GAN in PyTorch: A Step-by-Step Guide with CIFAR-10 Dataset</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;"><strong>Import the necessary libraries</strong>: — PyTorch — torchvision.transforms — torchvision.datasets — torch.nn — torch.optim — torchvision.utils — matplotlib.pyplot</li></ol></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span><span>import</span> torch<br/><span>import</span> torchvision.transforms <span>as</span> transforms<br/><span>import</span> torchvision.datasets <span>as</span> datasets<br/><span>import</span> torch.nn <span>as</span> nn<br/><span>import</span> torch.optim <span>as</span> optim<br/><span>import</span> torchvision.utils <span>as</span> vutils<br/><span>import</span> matplotlib.pyplot <span>as</span> plt</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>2. Download the dataset and prepare it for training.</strong> In this example, the CIFAR-10 dataset is used. The dataset is downloaded and transformed using PyTorch’s transforms module.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>transform = transforms.Compose([transforms.Resize(<span>64</span>),<br/>                                transforms.CenterCrop(<span>64</span>),<br/>                                transforms.ToTensor(),<br/>                                transforms.Normalize(mean=[<span>0.5</span>, <span>0.5</span>, <span>0.5</span>], std=[<span>0.5</span>, <span>0.5</span>, <span>0.5</span>])])<br/><br/>train_set = datasets.CIFAR10(root=<span>'./data'</span>, train=<span>True</span>, download=<span>True</span>, transform=transform)<br/><br/>device = torch.device(<span>"cuda:0"</span> <span>if</span> torch.cuda.is_available() <span>else</span> <span>"cpu"</span>)<br/><br/>batch_size = <span>128</span><br/>train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=<span>True</span>, num_workers=<span>2</span>)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Let us analyse the code clearly First of all Performing transformations on images is essential to improve the quality of the data and to prepare them for training a neural network. Transformations help to make the images homogeneous, remove any background noise and normalise the data, improving the neural network’s ability to generalise and make accurate predictions.Background noise’ refers to unwanted elements in the images that are not relevant to the object or scene you want to represent.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The transforms.Resize(64) transformation is performed before the transforms.CentreCrop(64) transformation because it ensures that all images have at least one dimension equal to 64 pixels. However, this transformation may leave one of the two image dimensions (height or width) with a dimension of less than 64 pixels.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To avoid this problem, the transforms.CenterCrop(64) transformation is subsequently performed to crop the image to an exact size of 64x64 pixels.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The third transformation performed is transforms.ToTensor(), which converts the image into a PyTorch tensor. <br></br>Finally, the fourth transformation performed in the code is <code>transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])</code>, which normalizes the pixel values of the images so that they have a mean of 0.5 and a standard deviation of 0.5 for each of the three RGB channels. This transformation is performed at the end of the preprocessing pipeline because if performed before the previous transformations, it would have changed the pixel values before other transformations were performed. The <code>normalize</code> variable in the code stores the <code>transforms.Normalize</code> function for later use in normalizing an image tensor.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>3. Define the Generator and Discriminator networks</strong>. The Generator network takes random noise as input and generates an image, while the Discriminator network takes an image as input and predicts whether it is real or fake.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span><span>class</span> <span>Generator</span>(nn.Module):<br/>    <span>def</span> <span>__init__</span>(<span>self, nz, ngf, nc</span>):<br/>        <span>super</span>(Generator, self).__init__()<br/>        self.main = nn.Sequential(<br/>            nn.ConvTranspose2d(nz, ngf * <span>8</span>, <span>4</span>, <span>1</span>, <span>0</span>, bias=<span>False</span>),<br/>            nn.BatchNorm2d(ngf * <span>8</span>),<br/>            nn.ReLU(<span>True</span>),<br/>            nn.ConvTranspose2d(ngf * <span>8</span>, ngf * <span>4</span>, <span>4</span>, <span>2</span>, <span>1</span>, bias=<span>False</span>),<br/>            nn.BatchNorm2d(ngf * <span>4</span>),<br/>            nn.ReLU(<span>True</span>),<br/>            nn.ConvTranspose2d(ngf * <span>4</span>, ngf * <span>2</span>, <span>4</span>, <span>2</span>, <span>1</span>, bias=<span>False</span>),<br/>            nn.BatchNorm2d(ngf * <span>2</span>),<br/>            nn.ReLU(<span>True</span>),<br/>            nn.ConvTranspose2d(ngf * <span>2</span>, ngf, <span>4</span>, <span>2</span>, <span>1</span>, bias=<span>False</span>),<br/>            nn.BatchNorm2d(ngf),<br/>            nn.ReLU(<span>True</span>),<br/>            nn.ConvTranspose2d(ngf, nc, <span>4</span>, <span>2</span>, <span>1</span>, bias=<span>False</span>),<br/>            nn.Tanh()<br/>        )<br/><br/>    <span>def</span> <span>forward</span>(<span>self, <span>input</span></span>):<br/>        <span>return</span> self.main(<span>input</span>)<br/><br/><span>class</span> <span>Discriminator</span>(nn.Module):<br/>    <span>def</span> <span>__init__</span>(<span>self, ndf, nc</span>):<br/>        <span>super</span>(Discriminator, self).__init__()<br/>        self.main = nn.Sequential(<br/>            nn.Conv2d(nc, ndf, <span>4</span>, <span>2</span>, <span>1</span>, bias=<span>False</span>),<br/>            nn.LeakyReLU(<span>0.2</span>, inplace=<span>True</span>),<br/>            nn.Conv2d(ndf, ndf * <span>2</span>, <span>4</span>, <span>2</span>, <span>1</span>, bias=<span>False</span>),<br/>            nn.BatchNorm2d(ndf * <span>2</span>),<br/>            nn.LeakyReLU(<span>0.2</span>, inplace=<span>True</span>),<br/>            nn.Conv2d(ndf * <span>2</span>, ndf * <span>4</span>, <span>4</span>, <span>2</span>, <span>1</span>, bias=<span>False</span>),<br/>            nn.BatchNorm2d(ndf * <span>4</span>),<br/>            nn.LeakyReLU(<span>0.2</span>, inplace=<span>True</span>),<br/>            nn.Conv2d(ndf * <span>4</span>, ndf * <span>8</span>, <span>4</span>, <span>2</span>, <span>1</span>, bias=<span>False</span>),<br/>            nn.BatchNorm2d(ndf * <span>8</span>),<br/>            nn.LeakyReLU(<span>0.2</span>, inplace=<span>True</span>),<br/>            nn.Conv2d(ndf * <span>8</span>, <span>1</span>, <span>4</span>, <span>1</span>, <span>0</span>, bias=<span>False</span>),<br/>            nn.Sigmoid()<br/>        )<br/><br/>    <span>def</span> <span>forward</span>(<span>self, <span>input</span></span>):<br/>        <span>return</span> self.main(<span>input</span>).view(-<span>1</span>, <span>1</span>).squeeze(<span>1</span>)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In the first part of the code, we define our Generator class, our Giovanni in the story, the forger who is to generate the images. It is important to explain that the ReLU activation function (nn.ReLU()) is used in all intermediate layers of the neural network because it is a simple and efficient non-linear activation function. The ReLU function is capable of representing complex non-linear functions and is computationally efficient.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The Tanh activation function (nn.Tanh()) is used in the last layer of the neural network because it maps the pixel values of the generated image in the range between -1 and 1. This is particularly useful for image generation because neural network inputs are usually normalised in the range between -1 and 1. In addition, the Tanh function is able to produce a synthetic image with softer pixel values than the ReLU activation function, which can improve the quality of the generated synthetic image.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>The Generator class</strong> is a transposed convolutional neural network, used to generate synthetic images from a random input. The structure of the network is defined inside the constructor <code>__init__()</code> of the class, which defines the layers of the network.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In particular, the neural network is composed of a sequence of transposed convolutional layers (nn.ConvTranspose2d()), which progressively increase the image resolution. Each transposed convolutional layer receives an input tensor of dimensions (batch_size, nz, height, width), where <code>nz</code> represents the size of the random input noise, while <code>height</code> and <code>width</code> represent the height and width of the output image. The first transposed convolutional layer takes the random input noise as input, while the following layers take the output tensor of the last transposed convolutional layer as input.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">A batch normalization (nn.BatchNorm2d()) is applied to each transposed convolutional layer, which normalizes the layer activations for each batch, making the network more stable and reducing the risk of overfitting.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>The Discriminator class</strong> our Jacques is a convolutional neural network used to classify images into two categories: “real” and “fake”. The structure of the network is defined inside the constructor <code>__init__()</code> of the class, which defines the layers of the network.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In particular, the neural network is composed of a sequence of convolutional layers (<code>nn.Conv2d()</code>) that progressively reduce the image resolution. Each convolutional layer receives an input tensor of dimensions <code>(batch_size, nc, height, width)</code>, where <code>nc</code> represents the number of image channels (e.g., 3 channels for an RGB image), while <code>height</code> and <code>width</code> represent the height and width of the image.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Finally, the last layer of the network uses the Sigmoid activation function (nn.Sigmoid()), which maps the class to which the image belongs into a value between 0 and 1, representing the probability that the image is ‘real’. Specifically, a value close to 1 indicates that the image is most likely ‘real’, while a value close to 0 indicates that the image is most likely ‘fake’.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>4 Define the parameters of the GAN</strong>. These include the dimension of the input noise to the generator, the size of the feature maps, the number of channels in the image, and the learning rate for the optimizers.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In this example, the following parameters are defined:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><code>nz</code>: The dimension of the input noise to the generator. It is set to 100.</li><li class="ff3" style="font-size:22px;"><code>ngf</code>: The size of the feature maps in the generator network. It is set to 64.</li><li class="ff3" style="font-size:22px;"><code>ndf</code>: The size of the feature maps in the discriminator network. It is set to 64.</li><li class="ff3" style="font-size:22px;"><code>nc</code>: The number of channels in the image. It is set to 3 for RGB images.</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>nz = <span>100</span> <br/>ngf = <span>64</span> <br/>ndf = <span>64</span> <br/>nc = <span>3</span> </span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">5 Initialize the generator and discriminator networks and define the loss function and optimizers.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The generator and discriminator networks are initialized using the parameters defined in step 4.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The binary cross-entropy loss function (<code>nn.BCELoss()</code>) is used to train both networks. Two optimizers (<code>optim.Adam()</code>) are defined for the generator and discriminator networks.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span><span># Initialize the generator and discriminator networks</span><br/>netG = Generator(nz, ngf, nc)<br/>netD = Discriminator(ndf, nc).to(device)<br/><br/><span># define the loss function</span><br/>criterion = nn.BCELoss() <span># Funzione di perdita utilizzata per addestrare la GAN</span><br/><br/><span># define the optimizers.</span><br/>optimizerD = optim.Adam(netD.parameters(), lr=<span>0.0002</span>, betas=(<span>0.5</span>, <span>0.999</span>))<br/>optimizerG = optim.Adam(netG.parameters(), lr=<span>0.0002</span>, betas=(<span>0.5</span>, <span>0.999</span>))</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">6 Train the GAN by iterating over the training dataset.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The GAN is trained by iterating over the training dataset for a specified number of epochs. In each iteration, the discriminator is trained first by minimizing the binary cross-entropy loss between its predictions and the true labels (1 for real images, 0 for fake images). Then, the generator is trained by minimizing the binary cross-entropy loss between the discriminator’s predictions and the true labels (1 for real images).</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span><span># Definition of GAN parameters</span><br/>img_list = []<br/>fixed_noise = torch.randn(<span>64</span>, nz, <span>1</span>, <span>1</span>, device=device)<br/><br/>num_epochs = <span>150</span><br/>batch_size = <span>256</span><br/>lr = <span>0.0002</span><br/>real_label = <span>1</span><br/>fake_label = <span>0</span><br/><br/><br/><span># Defining the dataloader for the image dataset</span><br/>dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=<span>True</span>)<br/><br/><br/><span># Defining the loss function and optimisers for the two networks</span><br/>criterion = nn.BCELoss()<br/>optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(<span>0.5</span>, <span>0.999</span>))<br/>optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(<span>0.5</span>, <span>0.999</span>))<br/><br/><span># Move the GAN network to the GPU, if available</span><br/>netG.to(device)<br/><br/><span># GAN training</span><br/><span>for</span> epoch <span>in</span> <span>range</span>(num_epochs):<br/>    <span>for</span> i, data <span>in</span> <span>enumerate</span>(dataloader, <span>0</span>):<br/>        <span># discriminator training </span><br/>        netD.zero_grad()<br/>        real_images = data[<span>0</span>].to(device)  <span># move the real picture to  GPU</span><br/>        b_size = real_images.size(<span>0</span>)<br/>        label = torch.full((b_size,), real_label, device=device).<span>float</span>()<br/><br/>        output = netD(real_images)<br/>        errD_real = criterion(output, label)<br/>        errD_real.backward()<br/>        D_x = output.mean().item()<br/><br/>        noise = torch.randn(b_size, nz, <span>1</span>, <span>1</span>, device=device, dtype=torch.float32)<br/><br/>        fake = netG(noise)<br/>        label.fill_(real_label)<br/><br/>        output = netD(fake.detach())<br/>        errD_fake = criterion(output, label)<br/>        errD_fake.backward()<br/>        D_G_z1 = output.mean().item()<br/>        errD = errD_real + errD_fake<br/>        optimizerD.step()<br/><br/>        <span># Generator training </span><br/>        netG.zero_grad()<br/>        label.fill_(real_label)<br/>        output = netD(fake)<br/>        errG = criterion(output, label)<br/>        errG.backward()<br/>        D_G_z2 = output.mean().item()<br/>        optimizerG.step()<br/><br/>        <span># print of the result </span><br/>        <span>if</span> i % <span>100</span> == <span>0</span>:<br/>            <span>print</span>(<span>'[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f / %.4f'</span><br/>                  % (epoch, num_epochs, i, <span>len</span>(dataloader),<br/>                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))<br/><br/>        <span># Adds generated images to the list</span><br/>        <span>if</span> (i % <span>500</span> == <span>0</span>) <span>or</span> ((epoch == num_epochs-<span>1</span>) <span>and</span> (i == <span>len</span>(dataloader)-<span>1</span>)):<br/>          <span>with</span> torch.no_grad():<br/>            fake_images = netG(fixed_noise).detach().cpu()<br/>          img_list.append(vutils.make_grid(fake_images, padding=<span>2</span>, normalize=<span>True</span>))<br/><br/> </span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">First, an empty list is defined to store the generated images. Then, a random noise tensor is created and several parameters related to the GAN network are defined. The training loop for the GAN network is executed, in which the discriminator and generator networks are trained for each batch. The intermediate results are printed at every 100th batch. At the end of each epoch, a generated image is saved in the “img_list” list.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">7 Save a generated image at the end of each epoch:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span><span>#Saving an image generated at the end of each epoch</span><br/>    <span>with</span> torch.no_grad():<br/>        fixed_noise = fixed_noise.to(device)  <span># Move the noise to the GPU</span><br/>        fake = netG(fixed_noise).detach().cpu()<br/>    img_list.append(vutils.make_grid(fake, padding=<span>2</span>, normalize=<span>True</span>))</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">8 Generate and save images with the trained generator:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span><span># Generate and save images with the trained generator</span><br/><br/>plt.imshow(img_list[-<span>1</span>].permute(<span>1</span>, <span>2</span>, <span>0</span>))<br/>plt.show()</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Possible Future scenarios</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Now I see two diametrically opposed possible scenarios.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:84%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 620px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*lnt87XQFkbUFMf_SR9HgIA.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*lnt87XQFkbUFMf_SR9HgIA.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*lnt87XQFkbUFMf_SR9HgIA.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*lnt87XQFkbUFMf_SR9HgIA.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*lnt87XQFkbUFMf_SR9HgIA.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*lnt87XQFkbUFMf_SR9HgIA.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1240/format:webp/1*lnt87XQFkbUFMf_SR9HgIA.jpeg 1240w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 620px" srcset="https://miro.medium.com/v2/resize:fit:640/1*lnt87XQFkbUFMf_SR9HgIA.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*lnt87XQFkbUFMf_SR9HgIA.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*lnt87XQFkbUFMf_SR9HgIA.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*lnt87XQFkbUFMf_SR9HgIA.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*lnt87XQFkbUFMf_SR9HgIA.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*lnt87XQFkbUFMf_SR9HgIA.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1240/1*lnt87XQFkbUFMf_SR9HgIA.jpeg 1240w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:620/1*lnt87XQFkbUFMf_SR9HgIA.jpeg"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Scenario A )</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Imagine a future where GANs were implemented in every aspect of our daily lives. With their ability to create images, sound, text and video, GANs have made our lives easier and more beautiful. It is now possible to generate entire virtual environments that are extremely realistic. For example, if we want to furnish our house, we can use GANs to generate different configurations of furniture and see how they fit into our space. Or, if we want to create a video or animation, GANs can automatically generate the missing frames to complete the sequence. GANs help designers, artists and musicians to create new and original content quickly and efficiently.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">But GANs are not only useful in the creative field, they can also help in the medical field. For example, they can also create customised prostheses that perfectly fit the patient’s individual size and needs.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">GANs have revolutionised the online shopping experience. Thanks to GANs, online shops can generate realistic images and videos of their products, making the shopping experience more engaging and satisfying. GANs can also help predict market trends and provide customers with personalised product recommendations.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In this optimistic future, GANs help humanity in every aspect of life, making it easier, more creative and more satisfying. With their extraordinary creative capabilities and wide range of applications, GANs are becoming an indispensable and valuable technology for improving the quality of our lives.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Scenario B)</strong></p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*pum1kXz-_Im_53FoSsbK4Q.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*pum1kXz-_Im_53FoSsbK4Q.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*pum1kXz-_Im_53FoSsbK4Q.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*pum1kXz-_Im_53FoSsbK4Q.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*pum1kXz-_Im_53FoSsbK4Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*pum1kXz-_Im_53FoSsbK4Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pum1kXz-_Im_53FoSsbK4Q.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*pum1kXz-_Im_53FoSsbK4Q.png 640w, https://miro.medium.com/v2/resize:fit:720/1*pum1kXz-_Im_53FoSsbK4Q.png 720w, https://miro.medium.com/v2/resize:fit:750/1*pum1kXz-_Im_53FoSsbK4Q.png 750w, https://miro.medium.com/v2/resize:fit:786/1*pum1kXz-_Im_53FoSsbK4Q.png 786w, https://miro.medium.com/v2/resize:fit:828/1*pum1kXz-_Im_53FoSsbK4Q.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*pum1kXz-_Im_53FoSsbK4Q.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*pum1kXz-_Im_53FoSsbK4Q.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*pum1kXz-_Im_53FoSsbK4Q.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">PICTURE GENERATED BY DALL-E 2</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In the future, GANs have evolved to the point where they can create visual and audio content indistinguishable from reality. This technology is used pervasively in society, from advertising communications to news and entertainment.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">However, with the increasing reliance on GANs, the concept of truth is becoming increasingly blurred. People begin to doubt the veracity of everything they see or hear, and conspiracy theories become increasingly popular. In addition, GANs are fraudulently used to create false documents and identities, which leads to an increase in crime.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Society begins to rely on nothing, as there is no longer any objective truth. Personal opinions become the norm, and there is no longer a common basis for cooperation and collective decision-making. Furthermore, GANs become increasingly complex and difficult to understand, leading to a gap between those who use them and those who do not understand them.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In this scenario, GANs will become an instrument of political control, used to maintain power and leadership over the people. The truth will no longer be relevant, but rather the opinion and perception that politicians want the public to have.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This manipulation of the truth will create a gap between those who have access to GAN and those who do not. Politicians who are better able to use this technology will be able to exert more influence on society, thus creating an even more divided and polarised society.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Society becomes increasingly polarised, with groups warring against each other based on personal opinions and beliefs. Eventually, society disintegrates, as there is no longer a common basis on which to build.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Thanks</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Thank you to all the readers who took the time to read my article. I hope you found the article informative and interesting.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">My goal was to offer a comprehensive and accurate view of the topic, and I hope I did justice to the complexity of the subject.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I appreciate your attention and for choosing to read my article. I hope I provided useful information and stimulated your curiosity on the topic.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you have any questions or comments, please do not hesitate to contact me. I would be happy to receive feedback from you and to continue the dialogue on this interesting topic.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Follow me on linkedin <a href="https://www.linkedin.com/in/francesco-puglia-5847a0250/" target="_self">https://www.linkedin.com/in/francesco-puglia-5847a0250/</a></p></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>