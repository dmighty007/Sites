<!DOCTYPE html>
                <html>
                <head>
                    <title>24 Really F***ing Interesting Deep Learning Papers</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://medium.com/dataseries/24-really-f-ing-interesting-deep-learning-papers-fa26afe3a070"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://medium.com/@andre-ye?source=post_page-----fa26afe3a070--------------------------------">Author : Andre Ye</a> </h5></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*vDv6imRMp8GNTTfM 640w, https://miro.medium.com/v2/resize:fit:720/0*vDv6imRMp8GNTTfM 720w, https://miro.medium.com/v2/resize:fit:750/0*vDv6imRMp8GNTTfM 750w, https://miro.medium.com/v2/resize:fit:786/0*vDv6imRMp8GNTTfM 786w, https://miro.medium.com/v2/resize:fit:828/0*vDv6imRMp8GNTTfM 828w, https://miro.medium.com/v2/resize:fit:1100/0*vDv6imRMp8GNTTfM 1100w, https://miro.medium.com/v2/resize:fit:1400/0*vDv6imRMp8GNTTfM 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*vDv6imRMp8GNTTfM 640w, https://miro.medium.com/v2/resize:fit:720/0*vDv6imRMp8GNTTfM 720w, https://miro.medium.com/v2/resize:fit:750/0*vDv6imRMp8GNTTfM 750w, https://miro.medium.com/v2/resize:fit:786/0*vDv6imRMp8GNTTfM 786w, https://miro.medium.com/v2/resize:fit:828/0*vDv6imRMp8GNTTfM 828w, https://miro.medium.com/v2/resize:fit:1100/0*vDv6imRMp8GNTTfM 1100w, https://miro.medium.com/v2/resize:fit:1400/0*vDv6imRMp8GNTTfM 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/0*vDv6imRMp8GNTTfM"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;"><a href="https://unsplash.com/photos/WmnsGyaFnCQ" target="_self">Unsplash</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>24 Really F***ing Interesting Deep Learning Papers</h3></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5 text-muted">With links and brief summaries</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I’ve read a lot of deep learning papers, but some stand out because of how unique and interesting their ideas are. They may not be the most cited, nor necessarily the most practical— but they definitely get the gears turning. Here they are, in no particular order.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><a href="https://www.nature.com/articles/s41598-019-47765-6" target="_self">“DeepInsight: A methodology to transform a non-image data to an image for convolution neural network architecture”</a>. Using t-SNE to transform tabular data into images so computer vision methods can be used to model tabular data.</li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/abs/2105.03404" target="_self">“ResMLP: Feedforward networks for image classification with data-efficient training”</a>. Building a functional image classification architecture without convolutions, attention, capsules — jut feedforward layers.</li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/abs/1911.11423" target="_self">“Single Headed Attention RNN: Stop Thinking With Your Head”</a>. The funniest deep learning paper I have had the pleasure of coming across. Author Stephen Merity offers a humorous and insightful critique of modern Transformer-centric recurrent-ditching NLP research and proposes reconsidering the usefulness of recurrent layers.</li><li class="ff3" style="font-size:22px;"><a href="https://aclanthology.org/2022.acl-long.131.pdf" target="_self">“GPT-D: Inducing Dementia-related Linguistic Anomalies by Deliberate Degradation of Artificial Neural Language Models”</a>. Title says all.</li><li class="ff3" style="font-size:22px;"><a href="https://facctconference.org/static/pdfs_2022/facct22-140.pdf" target="_self">“Predictability and Surprise in AI Models”</a>. Really interesting theoretical and empirical analysis of the extent to which large language model behavior in an ‘OOD’ deployment context can be predictable/stable, and implications for AI public policy. (Who can really be held responsible for OOD AI behavior if AI itself is provably unstable in such a context?)</li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/pdf/2108.07258.pdf?utm_source=morning_brew" target="_self">“On the Opportunities and Risks of Foundation Models”</a>. Landmark inter-disciplinary AI paper. Also, by far the most number of authors I have seen on an AI paper, ever.</li><li class="ff3" style="font-size:22px;"><a href="https://transformer-circuits.pub/2021/framework/index.html" target="_self">“A Mathematical Framework for Transformer Circuits”</a>. Ever wonder why Transformers — as conceptually simple as they are — work so well on such incredibly complex tasks? This great post by Anthropic’s research team begins building a mathematical framework for how Transformers work and why so well. (Turns out, Transformers have a mathematical analog for a <code>Ctrl+C Ctrl+V</code> ).</li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/abs/2105.04026" target="_self">“The Modern Mathematics of Deep Learning”</a>. Deep learning has always been what I call an ‘empiricist-forward’ field: developments are generally made because someone tried some implementation out and it worked well rather than because a rigorous theory suggests it will work out. This has worked out decently so far, but theory is powerful. It gives us rigor. It gives us predictability. It lets us both understand <em>that</em> things work and <em>why</em>. This paper is a difficult read but a great survey of such a theory of deep learning.</li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/abs/1803.03635" target="_self">“The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks”</a>. Really important theoretical work emerging from the surprising success of pruning — a technique by which a computer vision or NLP model can be shrunk by between 5 to 20 times with minimal loss in performance — explaining how neural networks are trained and arrive at solutions.</li><li class="ff3" style="font-size:22px;"><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Ramanujan_Whats_Hidden_in_a_Randomly_Weighted_Neural_Network_CVPR_2020_paper.pdf" target="_self">“What’s Hidden in a Randomly Weighted Neural Network?”</a>. Building upon the Lottery Ticket Hypothesis, this paper shows that you can discover a subnetwork within a large randomly initialized network which performs just as well as if that large neural network was trained. The subnetwork’s weights are not trained at all! A fascinating experiment which reveals a lot about the nature of large neural networks.</li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/pdf/1905.02175.pdf" target="_self">“Adversarial Examples Are Not Bugs, They Are Features”</a>. A unique take on a phenomena which has troubled AI researchers and thinkers since their discovery.</li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/pdf/2110.09485.pdf" target="_self">“Learning in High Dimension Always Amounts to Extrapolation”</a>. A series of experiments showing how the distinction between interpolation and extrapolation becomes incredibly blurry in the high-dimensional spaces neural networks operate on.</li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/abs/1703.04908" target="_self">“Emergence of Grounded Compositional Language in Multi-Agent Populations”</a>. Rather than training large language models to model human language, we allow agents to ‘create’ both the generation and interpretation of a synthetic language used to solve tasks which require inter-agent comunication.</li><li class="ff3" style="font-size:22px;"><a href="https://www.cell.com/patterns/fulltext/S2666-3899(21)00061-1" target="_self">“Moving beyond ‘algorithmic bias is a data problem’”</a>. A really interesting and much needed response to the prevalent idea that the GIGO (Garbage-In, Garbage-Out) paradigm is really all there is to AI Fairness.</li><li class="ff3" style="font-size:22px;"><a href="https://aclanthology.org/D19-1250.pdf" target="_self">“Language Models as Knowledge Bases?”</a>. Ask GPT-3 if it knows the year Lincoln was born in; ask it how many countries there are in Africa; ask it the conversion rate from Fahrenheit to Celsius; and it will tell you the answer to all of them. (It, however, cannot accurately tell you the answer to arithmetic problems like 4 + 8.) Can we interpret large language models as fluid, continuous knowledge bases which have an incredible quantity of human knowledge stored in fluid weights?</li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/pdf/2105.14103.pdf" target="_self">“An Attention Free Transformer”</a>. What happens when we remove attention — what many consider to be the most defining characteristic of the Transformer architecture — from a Transformer model? The result: a potentially more memory-efficient model which works both on image and NLP tasks.</li><li class="ff3" style="font-size:22px;"><a href="https://thegradient.pub/transformers-are-graph-neural-networks/" target="_self">“Transformers are Graph Neural Networks”</a>. A novel interpretation of Transformers.</li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/pdf/1606.04474.pdf" target="_self">“Learning to learn by gradient descent by gradient descent”</a>. We’re all used to optimizing neural networks with gradient descent. How about optimizing gradient descent with gradient descent?</li><li class="ff3" style="font-size:22px;"><a href="https://rajatvd.github.io/Generating-Words-From-Embeddings/" target="_self">“Generating Words from Embeddings”</a>. In most NLP models, words are associated with embeddings. What if we interpolate between embeddings to generate new nonsensical but realistic-sounding words?</li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/pdf/1904.11955.pdf" target="_self">“On Exact Computation with an Infinitely Wide Neural Net”</a>. A theoretically informed work which helps us understand the limits of network scaling.</li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/pdf/1911.01547.pdf" target="_self">“On the Measure of Intelligence”</a>. An essential novel work proposing an equation for the intelligence of a system, with plently of quantiative and philosophical theoretical discussion. Additionally, proposes a concrete benchmark task to develop and optimize models on.</li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/pdf/1506.02142.pdf" target="_self">“Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning”</a>. A novel interpretation of a time-honored mechanism.</li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/abs/2203.05482" target="_self">“Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time”</a>. An interesting approach to model training which suggests that simply averaging the weights of various fine-tuned versions of a model can lead to performance gains.</li><li class="ff3" style="font-size:22px;"><a href="https://www.nature.com/articles/s41586-021-04223-6" target="_self">“Deep physical neural networks trained with backpropagation”</a>. What more is there to say?</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I find my papers from <a href="https://arxiv.org/list/cs.LG/recent" target="_self">the daily arXiv ML submissions list</a>, the <a href="https://thesequence.substack.com/" target="_self">free version of The Sequence</a>, and <a href="https://search.zeta-alpha.com/" target="_self">especially from Zeta Alpha</a>, which is a great paper discovery platform.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Happy reading! Feel free to suggest interesting papers you’ve come across in the responses too.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">If you’re interested in staying updated on new articles, consider <a href="https://andre-ye.medium.com/subscribe" target="_self">subscribing</a>. If you’d like to support my writing, joining Medium via my <a href="https://andre-ye.medium.com/membership" target="_self">referral link</a> is a great way. Cheers!</p></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>