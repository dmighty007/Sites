<!DOCTYPE html>
                <html>
                <head>
                    <title>Trends in AI — August 2022</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://pub.towardsai.net/trends-in-ai-august-2022-2c7327bf1928"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://medium.com/@sergicastella?source=post_page-----2c7327bf1928--------------------------------">Author : Sergi Castella i Sapé</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>Trends in AI — August 2022</h3></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5 text-muted">3% of Google’s new code is written by a Language Model, DeepSpeed for making inferences with a Trillion-scale Mixture of Experts, Beating neural scaling laws, prompting in Information Retrieval, Language Model Cascades, and much more.</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*2FsTLBZoMzdrS153ivoA5w.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*2FsTLBZoMzdrS153ivoA5w.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*2FsTLBZoMzdrS153ivoA5w.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*2FsTLBZoMzdrS153ivoA5w.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*2FsTLBZoMzdrS153ivoA5w.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*2FsTLBZoMzdrS153ivoA5w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2FsTLBZoMzdrS153ivoA5w.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*2FsTLBZoMzdrS153ivoA5w.png 640w, https://miro.medium.com/v2/resize:fit:720/1*2FsTLBZoMzdrS153ivoA5w.png 720w, https://miro.medium.com/v2/resize:fit:750/1*2FsTLBZoMzdrS153ivoA5w.png 750w, https://miro.medium.com/v2/resize:fit:786/1*2FsTLBZoMzdrS153ivoA5w.png 786w, https://miro.medium.com/v2/resize:fit:828/1*2FsTLBZoMzdrS153ivoA5w.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*2FsTLBZoMzdrS153ivoA5w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*2FsTLBZoMzdrS153ivoA5w.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*2FsTLBZoMzdrS153ivoA5w.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Image by <a href="http://www.zeta-alpha.com" target="_self">Zeta Alpha</a>.</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">While blockbuster research has slowed down slightly in the past month, probably because of the summer season, conferences are back at full speed in person: <a href="https://2022.naacl.org/" target="_self">NAACL in Seattle</a>, <a href="https://sigir.org/sigir2022/" target="_self">SIGIR in Madrid</a>, and also <a href="https://www.zeta-alpha.com/post/can-ai-help-us-understand-icml-2022" target="_self">ICML, for which we created a special guide</a> with the help of GPT-3. Other news we’d like to highlight, to begin with are:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><a href="https://ai.googleblog.com/2022/07/ml-enhanced-code-completion-improves.html" target="_self">Google shares how code generation is being used within the company</a>. TL;DR almost 3% of all new code is generated by accepting suggestions from a Language Model, a suggestion acceptance rate of around 25%, a 6% reduction in coding iteration duration, and an average of 21 characters per accept.</li><li class="ff3" style="font-size:22px;"><a href="https://techcrunch.com/2022/07/12/openai-rival-ai21-labs-raises-64m-to-ramp-up-its-ai-powered-language-services/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAC9VRCH84zB6NLauX7XYt0Wr8M-ZR83b19Jd4VfWkffs_yODkFmCZGZDXoe_HZm-bscbI2G26VWRkweUytIm8iuK0arfbO4hVln6Qahxk9mLBT4U8HruDpXoWNWOBA6F1daf6wCiwnbL_Fe_orTu-v-wq2zlo-LlABqiMBwOhdvA" target="_self">AI21 Labs raises $64M with ambitions to rival OpenAI.</a> The company, founded and led by <a href="https://www.yoavshoham.net/" target="_self">Yoav Shoham</a>, <a href="https://twitter.com/origoshen?lang=en" target="_self">Ori Goshen</a>, and <a href="https://il.linkedin.com/in/amnon-shashua" target="_self">Amnon Shashua</a>, will develop more sophisticated Language Models.</li><li class="ff3" style="font-size:22px;"><a href="https://www.bloomberg.com/news/articles/2022-05-17/ian-goodfellow-former-apple-director-of-machine-learning-to-join-deepmind" target="_self">Ian Goodfellow leaves Apple to join DeepMind under Oriol Vinyals</a>, and <a href="https://twitter.com/karpathy/status/1547332300186066944?s=20&t=0otF0xjoN1Ge5gYXCythnA" target="_self">Andrej Karpathy leaves Tesla</a> after 5 years of leading its autonomous driving research division.</li><li class="ff3" style="font-size:22px;">Yolo-v7 was released… twice! There was a bit of a clash of names here: <a href="https://arxiv.org/abs/2207.02696" target="_self">here’s the new paper</a> and <a href="https://github.com/WongKinYiu/yolov7" target="_self">its implementation</a>, and <a href="https://github.com/jinfagang/yolov7" target="_self">here’s also the YOLO-v7 project</a> which has been under development for a few months longer and will now be renamed to a more generic YOLOvn.</li><li class="ff3" style="font-size:22px;"><a href="https://www.linkedin.com/posts/nicola-richmond-92973a3_becauseitmatters-activity-6951093112320528384-Cvg6?utm_source=linkedin_share&utm_medium=member_desktop_web" target="_self">Nicola Richmond is the new VP of AI in Benevolent AI</a>, a startup applying modern Deep Learning techniques for drug discovery.</li><li class="ff3" style="font-size:22px;"><a href="https://ai.facebook.com/blog/introducing-sphere-meta-ais-web-scale-corpus-for-better-knowledge-intensive-nlp/" target="_self">Meta AI releases Sphere, a web-scale corpus for better knowledge-intensive NLP</a>, and here’s its <a href="https://github.com/facebookresearch/sphere" target="_self">GitHub repo</a>. The dataset is curated as a subset of <a href="https://github.com/facebookresearch/cc_net" target="_self">CCNet</a> and aims to enhance research at the intersection between Natural Language Processing and Information Retrieval.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">🔬 Research</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Every month we analyze the most recent research literature and select a varied set of 10 papers you should know of. This month we’re covering topics such as Reinforcement Learning (RL), Scaling Laws, Information Retrieval, Language Models, and more.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><a href="https://arxiv.org/abs/2206.14486" target="_self">1. Beyond neural scaling laws: beating power law scaling via data pruning</a></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>By Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, Ari S. Morcos.</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>❓ Why → </strong>Scaling laws¹ is a pervasive empirical phenomenon in modern Neural Networks, where the error is observed to off as a power of the training set size, model size, or both. While some have embraced this fact to devise a research agenda focused on scaling up, many think there must be ways to build <em>better</em> models without the need for outrageous scale. This paper explores a technique — data pruning — that can improve the learning efficiency of NNs “beating” scaling laws.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>💡 Key insights → </strong>In the context of this work, <mark>pruning refers to dropping training data samples from the training dataset, not pruning weights of the Neural Network</mark>. The intuition behind the method proposed is simple: imagine you could rank samples in a training dataset from “easy to learn” to “hard to learn” for a model. A typical dataset will contain too many of the <em>easy-to-learn</em> samples — that is to say, fewer would suffice to reach good performance on those samples — and too few from the <em>hard-to-learn</em> side — meaning you’d need more examples to train the model in properly.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">One way of solving this problem is to scale up the whole training dataset because given enough scaling up — assuming the data distribution is uniform — you’ll get enough of the “hard to learn” samples eventually. But that’s very wasteful. What if we could use a priori to curate a training dataset that contains a better balance of easy-to-learn and hard-to-learn samples? This is what this paper investigates.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This problem can be formalized as trying to find a <strong>pruning metric</strong> to assign to each training sample (a proxy for its <em>hardness</em>), which is then used to prune the training dataset to the desired size. They propose a new metric in this paper that is comparable to existing work requiring labeled data.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*lQAe-LBozLm4T5zSItzJOA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*lQAe-LBozLm4T5zSItzJOA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*lQAe-LBozLm4T5zSItzJOA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*lQAe-LBozLm4T5zSItzJOA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*lQAe-LBozLm4T5zSItzJOA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*lQAe-LBozLm4T5zSItzJOA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lQAe-LBozLm4T5zSItzJOA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*lQAe-LBozLm4T5zSItzJOA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*lQAe-LBozLm4T5zSItzJOA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*lQAe-LBozLm4T5zSItzJOA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*lQAe-LBozLm4T5zSItzJOA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*lQAe-LBozLm4T5zSItzJOA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*lQAe-LBozLm4T5zSItzJOA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*lQAe-LBozLm4T5zSItzJOA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*lQAe-LBozLm4T5zSItzJOA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Source: <a href="https://arxiv.org/pdf/2206.14486.pdf" target="_self">https://arxiv.org/pdf/2206.14486.pd</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">However, the most interesting contribution, in my opinion, is their section on data pruning without labels. They perform k-means clustering on the embeddings of a pre-trained ImageNet model and define the hardness of each sample as its distance to the nearest centroid: the intuition is that prototypical samples that are easy to learn will be closest to a centroid, whereas hard-to-learn samples will fall further away from their cluster centroids. The results show how around 20% of the training samples from ImageNet can be pruned without sacrificing performance.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To be fair, the results in the paper are not jaw-dropping, but the key ideas behind it have the potential to be useful in other tasks such as image segmentation, language modeling, or any other multimodal dataset curation.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><a href="https://arxiv.org/abs/2206.15477" target="_self">2. Denoised MDPs: Learning World Models Better Than the World Itself</a> | <a href="https://ssnl.github.io/denoised_mdp/" target="_self">Project Page</a> | <a href="https://github.com/facebookresearch/denoised_mdp/" target="_self">Code</a></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>By Tongzhou Wang, Simon S. Du, Antonio Torralba, Phillip Isola, Amy Zhang, Yuandong Tian.</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>❓ Why → </strong>The <em>world </em>contains a lot of information that’s irrelevant for you to be able to navigate it. At the core of many Machine Learning techniques is the ability to discern relevant and useful <em>signals</em> — or patterns — from noise.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>💡 Key insights →</strong> This work formalizes the problem of separating “the good from the irrelevant information” in the context of reinforcement learning by identifying information that’s both <strong>controllable by the agent</strong> and <strong>relevant for the reward</strong>, as described in the figure below.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*UiSN5cUvCYgNZgiNSAdfmQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*UiSN5cUvCYgNZgiNSAdfmQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*UiSN5cUvCYgNZgiNSAdfmQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*UiSN5cUvCYgNZgiNSAdfmQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*UiSN5cUvCYgNZgiNSAdfmQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*UiSN5cUvCYgNZgiNSAdfmQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UiSN5cUvCYgNZgiNSAdfmQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*UiSN5cUvCYgNZgiNSAdfmQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*UiSN5cUvCYgNZgiNSAdfmQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*UiSN5cUvCYgNZgiNSAdfmQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*UiSN5cUvCYgNZgiNSAdfmQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*UiSN5cUvCYgNZgiNSAdfmQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*UiSN5cUvCYgNZgiNSAdfmQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*UiSN5cUvCYgNZgiNSAdfmQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*UiSN5cUvCYgNZgiNSAdfmQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Source: <a href="https://arxiv.org/pdf/2206.15477.pdf" target="_self">https://arxiv.org/pdf/2206.15477.pdf</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Based on this idea, the authors propose Denoised MDPs (Markov Decision Process), a method for learning a factorization of state representations that disentangles the controllable and reward-relevant bit of the state using information theoretic principles. The gist of it is that different factors of the state should be maximally or minimally predictive of other factors depending on their relationship, which enables the authors to set up a variational objective for the agent to optimize (you’ll need to check out the paper for the real mathy goodies).</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*lZ-RlRmRLYKhdUU2W-3MCw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*lZ-RlRmRLYKhdUU2W-3MCw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*lZ-RlRmRLYKhdUU2W-3MCw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*lZ-RlRmRLYKhdUU2W-3MCw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*lZ-RlRmRLYKhdUU2W-3MCw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*lZ-RlRmRLYKhdUU2W-3MCw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lZ-RlRmRLYKhdUU2W-3MCw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*lZ-RlRmRLYKhdUU2W-3MCw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*lZ-RlRmRLYKhdUU2W-3MCw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*lZ-RlRmRLYKhdUU2W-3MCw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*lZ-RlRmRLYKhdUU2W-3MCw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*lZ-RlRmRLYKhdUU2W-3MCw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*lZ-RlRmRLYKhdUU2W-3MCw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*lZ-RlRmRLYKhdUU2W-3MCw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*lZ-RlRmRLYKhdUU2W-3MCw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Source: <a href="https://arxiv.org/pdf/2206.15477.pdf" target="_self">https://arxiv.org/pdf/2206.15477.pdf</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The result is a world model that explicitly models what information should be discarded as noise and what information should be used for modeling the agent’s decisions. The authors prove how this approach is competitive in the DeepMind Control Suite, and fascinatingly, they showcase qualitatively how the Denoised MDP representations work by training a decoder on reconstructing the input so you can understand what the signal representation of the state learns to capture. You can watch those on their <a href="https://www.tongzhouwang.info/denoised_mdp/" target="_self">project page</a>.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*9pgm-RnrDEgLi-TfkOnBEQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*9pgm-RnrDEgLi-TfkOnBEQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*9pgm-RnrDEgLi-TfkOnBEQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*9pgm-RnrDEgLi-TfkOnBEQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*9pgm-RnrDEgLi-TfkOnBEQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*9pgm-RnrDEgLi-TfkOnBEQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9pgm-RnrDEgLi-TfkOnBEQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*9pgm-RnrDEgLi-TfkOnBEQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*9pgm-RnrDEgLi-TfkOnBEQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*9pgm-RnrDEgLi-TfkOnBEQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*9pgm-RnrDEgLi-TfkOnBEQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*9pgm-RnrDEgLi-TfkOnBEQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*9pgm-RnrDEgLi-TfkOnBEQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*9pgm-RnrDEgLi-TfkOnBEQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*9pgm-RnrDEgLi-TfkOnBEQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Source: <a href="https://www.tongzhouwang.info/denoised_mdp/" target="_self">https://www.tongzhouwang.info/denoised_mdp/</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><a href="https://arxiv.org/abs/2207.07087" target="_self">3. Parameter-Efficient Prompt Tuning Makes Generalized and Calibrated Neural Text Retrievers</a> | <a href="https://github.com/THUDM/P-tuning-v2/tree/main/PT-Retrieval" target="_self">Code</a></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>By Weng Lam Tam, Xiao Liu, Kaixuan Ji, Lilong Xue, Xingjian Zhang, Yuxiao Dong, Jiahua Liu, Maodi Hu, Jie Tang.</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>❓ Why → </strong>Prompting has been making strides in NLP for the past couple of years, and now it also seems promising for Information Retrieval.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>💡 Key insights → </strong>Prompt tuning is a technique for adapting a pre-trained frozen model to a given task by adding trainable prompt tokens to the input of the sequence model. One of the main advantages of this approach with respect to the more common full model finetuning is that it only requires retraining a small subset of parameters which is much more efficient and also enables higher reusability of the original pre-trained model.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Now they train both a Dense Passage Retriever (retrieval by nearest neighbor search of query and doc embeddings) and a ColBERT⁴ type model with late interaction (includes joint modeling of query and document). The difference here is that instead of finetuning the whole model, they only finetune a prompt while keeping the pre-trained LM frozen. They base their implementation on the P-Tuning v² method, which is a more advanced form of prompt tuning where the trainable prompt is not only appended to the input but also at each layer of the Transformer.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*GxGSZx1xGPeBsVp5h2I0uw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*GxGSZx1xGPeBsVp5h2I0uw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*GxGSZx1xGPeBsVp5h2I0uw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*GxGSZx1xGPeBsVp5h2I0uw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*GxGSZx1xGPeBsVp5h2I0uw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*GxGSZx1xGPeBsVp5h2I0uw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GxGSZx1xGPeBsVp5h2I0uw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*GxGSZx1xGPeBsVp5h2I0uw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*GxGSZx1xGPeBsVp5h2I0uw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*GxGSZx1xGPeBsVp5h2I0uw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*GxGSZx1xGPeBsVp5h2I0uw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*GxGSZx1xGPeBsVp5h2I0uw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*GxGSZx1xGPeBsVp5h2I0uw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*GxGSZx1xGPeBsVp5h2I0uw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*GxGSZx1xGPeBsVp5h2I0uw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Source: <a href="https://arxiv.org/pdf/2207.07087.pdf" target="_self">https://arxiv.org/pdf/2207.07087.pdf</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The most interesting part of the results is that of generalization. While prompt tuning performs comparably to full fine-tuning on in-domain benchmarks, it outperforms substantially in various cross-domain datasets from the BEIR³ benchmark.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*v_RFuJZQSHPkDn8nvzm5TA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*v_RFuJZQSHPkDn8nvzm5TA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*v_RFuJZQSHPkDn8nvzm5TA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*v_RFuJZQSHPkDn8nvzm5TA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*v_RFuJZQSHPkDn8nvzm5TA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*v_RFuJZQSHPkDn8nvzm5TA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v_RFuJZQSHPkDn8nvzm5TA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*v_RFuJZQSHPkDn8nvzm5TA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*v_RFuJZQSHPkDn8nvzm5TA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*v_RFuJZQSHPkDn8nvzm5TA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*v_RFuJZQSHPkDn8nvzm5TA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*v_RFuJZQSHPkDn8nvzm5TA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*v_RFuJZQSHPkDn8nvzm5TA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*v_RFuJZQSHPkDn8nvzm5TA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*v_RFuJZQSHPkDn8nvzm5TA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Source: <a href="https://arxiv.org/pdf/2207.07087.pdf" target="_self">https://arxiv.org/pdf/2207.07087.pdf</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Yet again, this work strengthens the hypothesis that prompting and all its advantages are a viable alternative to fine-tuning and will probably keep increasing in popularity.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><a href="https://arxiv.org/abs/2207.00032" target="_self">4. DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale</a> | <a href="https://github.com/microsoft/DeepSpeed" target="_self">Code</a></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>By Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, Yuxiong He.</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>❓ Why → </strong>DeepSpeed — the framework for large-scale distributed training of large NNs developed and used by Microsoft — now also for inference besides training.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>💡 Key insights → </strong>The landscape of large Transformer architectures has diversified in the past year since the success of a sparse Mixture of Expert models in scaling up models in the Trillion parameter regime. The key advantage of these architectures is that they have increased expressive power from their larger sizes, but at inference time, only an input-dependent subset of the weights is used, which makes them more efficient (if the implementation is also optimized!). The downside is that training and running these models efficiently is more involved because most existing Deep Learning libraries and hardware were not designed with this type of computation.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">While DeepSpeed was previously designed to enable training large Transformers, the latest update focuses on making inference faster both in latency and throughput on all kinds of Transmodels, including sparsely activated architectures.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We’re talking about a system that enables parallelism in heterogeneous hardware at a scale of hundreds of GPUs, CPU, and NVMe memory that enables high-speed inference even when large models can’t fit in aggregate GPU memory.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*fTMBQJogwt4AwoVvna0Cug.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*fTMBQJogwt4AwoVvna0Cug.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*fTMBQJogwt4AwoVvna0Cug.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*fTMBQJogwt4AwoVvna0Cug.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*fTMBQJogwt4AwoVvna0Cug.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*fTMBQJogwt4AwoVvna0Cug.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fTMBQJogwt4AwoVvna0Cug.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*fTMBQJogwt4AwoVvna0Cug.png 640w, https://miro.medium.com/v2/resize:fit:720/1*fTMBQJogwt4AwoVvna0Cug.png 720w, https://miro.medium.com/v2/resize:fit:750/1*fTMBQJogwt4AwoVvna0Cug.png 750w, https://miro.medium.com/v2/resize:fit:786/1*fTMBQJogwt4AwoVvna0Cug.png 786w, https://miro.medium.com/v2/resize:fit:828/1*fTMBQJogwt4AwoVvna0Cug.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*fTMBQJogwt4AwoVvna0Cug.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*fTMBQJogwt4AwoVvna0Cug.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*fTMBQJogwt4AwoVvna0Cug.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Source: <a href="https://arxiv.org/pdf/2207.00032.pdf" target="_self">https://arxiv.org/pdf/2207.00032.pdf</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Let’s be honest, though, most people reading this article won’t ever have the need to use such a framework for training Trillion-scale models themselves, but it’s still a fascinating read if you’re interested in the engineering that goes behind training and running massive Neural Networks.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><a href="https://arxiv.org/abs/2207.05221" target="_self">5. Language Models (Mostly) Know What They Know</a></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>By Saurav Kadavath et al.</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>❓ Why → </strong>Performance is far from the only desirable property of ML models. Accurately knowing how confident they’re in their outputs can be more important, especially in safety-focused applications.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>💡 Key insights → </strong>Calibration is the concept used in Machine Learning to indicate how well-tuned a model’s prediction confidence is (e.g. a perfectly calibrated model with an output with 90% certainty should be <em>right</em> 9/10 times, no less, no more).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This work first investigates the calibration of LMs answering questions by formulating the prompt in a multiple choice fashion where the output of the model is a single token with the answer. Given that a single token is an answer, the probability can be calculated directly from the model’s likelihood of that token, normalized over valid token answers (e.g. A, B or C).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">While LMs are highly sensitive to prompt design and formatting, given an appropriate question formulation, large LMs are very well calibrated. Interestingly, this capacity breaks down at smaller scales (see figure below), and also at the tail of very low or very high confidence.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*a3bT7EhJZjoDq3otWgJYEQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*a3bT7EhJZjoDq3otWgJYEQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*a3bT7EhJZjoDq3otWgJYEQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*a3bT7EhJZjoDq3otWgJYEQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*a3bT7EhJZjoDq3otWgJYEQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*a3bT7EhJZjoDq3otWgJYEQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a3bT7EhJZjoDq3otWgJYEQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*a3bT7EhJZjoDq3otWgJYEQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*a3bT7EhJZjoDq3otWgJYEQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*a3bT7EhJZjoDq3otWgJYEQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*a3bT7EhJZjoDq3otWgJYEQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*a3bT7EhJZjoDq3otWgJYEQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*a3bT7EhJZjoDq3otWgJYEQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*a3bT7EhJZjoDq3otWgJYEQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*a3bT7EhJZjoDq3otWgJYEQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Source: <a href="https://arxiv.org/pdf/2207.05221.pdf" target="_self">https://arxiv.org/pdf/2207.05221.pdf</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The paper dives into more modes of calibration comparisons and analysis than you’ll find in the paper, but the takeaway remains: LMs <em>know what they know </em>(are well calibrated), but results are still very vulnerable to prompt variations, and models need to be large enough.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*0bq_ifBqIe6g3acfK7_34A.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*0bq_ifBqIe6g3acfK7_34A.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*0bq_ifBqIe6g3acfK7_34A.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*0bq_ifBqIe6g3acfK7_34A.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*0bq_ifBqIe6g3acfK7_34A.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*0bq_ifBqIe6g3acfK7_34A.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0bq_ifBqIe6g3acfK7_34A.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*0bq_ifBqIe6g3acfK7_34A.png 640w, https://miro.medium.com/v2/resize:fit:720/1*0bq_ifBqIe6g3acfK7_34A.png 720w, https://miro.medium.com/v2/resize:fit:750/1*0bq_ifBqIe6g3acfK7_34A.png 750w, https://miro.medium.com/v2/resize:fit:786/1*0bq_ifBqIe6g3acfK7_34A.png 786w, https://miro.medium.com/v2/resize:fit:828/1*0bq_ifBqIe6g3acfK7_34A.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*0bq_ifBqIe6g3acfK7_34A.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*0bq_ifBqIe6g3acfK7_34A.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*0bq_ifBqIe6g3acfK7_34A.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Source: <a href="https://arxiv.org/pdf/2207.05221.pdf" target="_self">https://arxiv.org/pdf/2207.05221.pdf</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><a href="https://arxiv.org/abs/2207.07078" target="_self">6. Towards Grand Unification of Object Tracking</a> (Unicorn🦄) | <a href="https://github.com/MasterBin-IIAU/Unicorn" target="_self">Code</a></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>By Bin Yan, Yi Jiang, Peize Sun, Dong Wang, Zehuan Yuan, Ping Luo, Huchuan Lu.</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>❓ Why → </strong>Decluttering and unifying Machine Learning model architectures have proven to be fruitful in NLP in the past few years, now time for video Computer Vision tasks.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>💡 Key insights → </strong>When it comes to video-related tasks, existing top-performing models still tend to rely on task-specific designs and overspecialize for their specific application as a result.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The authors propose a single model architecture that performs competitively in 4 modes for Object Tracking: Single Object Tracking (SOT), Multiple Object Tracking (MOT), Video Object Segmentation (VOS), and Multi-Object Tracking and Segmentation (MOTS).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This architecture is quite complex and is best understood through the figure below. In broad strokes, it starts with a unified backbone for embedding images, then a unified embedding is computed for the reference and current frame. A Transformer is used for the feature interaction between the unified embedding and the current frame to output class, box, and masks corresponding to all object tracking flavors.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*jfT-GguCiYu-bJ8yEY4uOA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*jfT-GguCiYu-bJ8yEY4uOA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*jfT-GguCiYu-bJ8yEY4uOA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*jfT-GguCiYu-bJ8yEY4uOA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*jfT-GguCiYu-bJ8yEY4uOA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*jfT-GguCiYu-bJ8yEY4uOA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jfT-GguCiYu-bJ8yEY4uOA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*jfT-GguCiYu-bJ8yEY4uOA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*jfT-GguCiYu-bJ8yEY4uOA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*jfT-GguCiYu-bJ8yEY4uOA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*jfT-GguCiYu-bJ8yEY4uOA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*jfT-GguCiYu-bJ8yEY4uOA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*jfT-GguCiYu-bJ8yEY4uOA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*jfT-GguCiYu-bJ8yEY4uOA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*jfT-GguCiYu-bJ8yEY4uOA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Source: <a href="https://arxiv.org/pdf/2207.07078.pdf" target="_self">https://arxiv.org/pdf/2207.07078.pdf</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The system is benchmarked on several object tracking benchmarks such as LaSOT, TrackingNet, MOT17, BDD100K (and others) and sets new state-of-the-art performances in most of them.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*pRs-R2Dl_aS0663F.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*pRs-R2Dl_aS0663F.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*pRs-R2Dl_aS0663F.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*pRs-R2Dl_aS0663F.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*pRs-R2Dl_aS0663F.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*pRs-R2Dl_aS0663F.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pRs-R2Dl_aS0663F.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*pRs-R2Dl_aS0663F.png 640w, https://miro.medium.com/v2/resize:fit:720/0*pRs-R2Dl_aS0663F.png 720w, https://miro.medium.com/v2/resize:fit:750/0*pRs-R2Dl_aS0663F.png 750w, https://miro.medium.com/v2/resize:fit:786/0*pRs-R2Dl_aS0663F.png 786w, https://miro.medium.com/v2/resize:fit:828/0*pRs-R2Dl_aS0663F.png 828w, https://miro.medium.com/v2/resize:fit:1100/0*pRs-R2Dl_aS0663F.png 1100w, https://miro.medium.com/v2/resize:fit:1400/0*pRs-R2Dl_aS0663F.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/0*pRs-R2Dl_aS0663F.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Source: <a href="https://arxiv.org/pdf/2207.07078.pdf" target="_self">https://arxiv.org/pdf/2207.07078.pdf</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><a href="https://arxiv.org/abs/2207.10551" target="_self">7. Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?</a></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>By Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q. Tran, Dani Yogatama, Donald Metzler.</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>❓ Why → </strong>Are we missing phenomena by benchmarking models at a particular compute/data scale? Yes.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>💡 Key insights → </strong>The authors perform hundreds of experiments across scales for a wide range of architectures: vanilla, efficient and advanced Transformers, MLP mixer, and Convolution-based models. The experiments consist of pretraining with autoregressive language modeling, which results in an upstream performance (language perplexity), followed by supervised fine-tuning on GLUE, SuperGLUE, and SQuAD (downstream performance).</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The TL;DR is straightforward. While not the best option in all scale regimes, the vanilla Transformer shows the most robust and consistent performance scaling results across scales. Other useful tidbits of wisdom are:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Convolution and MLP-based architectures do well on pretraining (upstream performance) but fail to transfer properly when finetuned. This points toward the importance of architectural inductive biases when it comes to transferring learning.</li><li class="ff3" style="font-size:22px;">Efficient Transformers only perform competitively with their full-attention counterparts in a certain scale regime and lag behind if scaled up sufficiently.</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*D6lO2NfGjVcvjJ1IQ4k7rA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*D6lO2NfGjVcvjJ1IQ4k7rA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*D6lO2NfGjVcvjJ1IQ4k7rA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*D6lO2NfGjVcvjJ1IQ4k7rA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*D6lO2NfGjVcvjJ1IQ4k7rA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*D6lO2NfGjVcvjJ1IQ4k7rA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D6lO2NfGjVcvjJ1IQ4k7rA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*D6lO2NfGjVcvjJ1IQ4k7rA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*D6lO2NfGjVcvjJ1IQ4k7rA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*D6lO2NfGjVcvjJ1IQ4k7rA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*D6lO2NfGjVcvjJ1IQ4k7rA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*D6lO2NfGjVcvjJ1IQ4k7rA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*D6lO2NfGjVcvjJ1IQ4k7rA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*D6lO2NfGjVcvjJ1IQ4k7rA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*D6lO2NfGjVcvjJ1IQ4k7rA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Source: <a href="https://arxiv.org/pdf/2207.10551.pdf" target="_self">https://arxiv.org/pdf/2207.10551.pdf</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><a href="https://arxiv.org/abs/2207.11240" target="_self">8. Discrete Key-Value Bottleneck</a></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>By Frederik Träuble, Anirudh Goyal, Nasim Rahaman, Michael Mozer, Kenji Kawaguchi, Yoshua Bengio, Bernhard Schölkopf.</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>❓ Why → </strong>As the benchmarking culture in ML slowly but surely shifts focus to out-of-domain generalization, inductive biases tailored for it will become more relevant.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>💡 Key insights → </strong>This method is part of a longer research agenda that we've highlighted before from Bengio's group (e.g. <a href="https://arxiv.org/abs/2103.01197" target="_self">Coordination Among Neural Modules Through a Shared Global Workspace</a>) focused on how introducing bottlenecks of information flow in architectures leads to representations that are robust and generalize better in an out of distribution sets. In this case, the proposed method consists of 3 steps:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">Encoding a high dimensional input (e.g. image) into an embedding with an encoder pre-trained on a large dataset.</li><li class="ff3" style="font-size:22px;">Splitting the embedding into C lower dimensional heads and finding the nearest neighbor from a set of predefined vectors which are frozen during training. Then use that nearest neighbor’s representation across heads to reconstruct the embedding. The discretized value codes are frozen during the training of the decoder.</li><li class="ff3" style="font-size:22px;">The decoder gets the reconstructed embedding as input and produces the task-specific output.</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*uZGa7Y_w6aDAsU9H83lydQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*uZGa7Y_w6aDAsU9H83lydQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*uZGa7Y_w6aDAsU9H83lydQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*uZGa7Y_w6aDAsU9H83lydQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*uZGa7Y_w6aDAsU9H83lydQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*uZGa7Y_w6aDAsU9H83lydQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uZGa7Y_w6aDAsU9H83lydQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*uZGa7Y_w6aDAsU9H83lydQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*uZGa7Y_w6aDAsU9H83lydQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*uZGa7Y_w6aDAsU9H83lydQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*uZGa7Y_w6aDAsU9H83lydQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*uZGa7Y_w6aDAsU9H83lydQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*uZGa7Y_w6aDAsU9H83lydQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*uZGa7Y_w6aDAsU9H83lydQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*uZGa7Y_w6aDAsU9H83lydQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Source: <a href="https://arxiv.org/pdf/2207.11240.pdf" target="_self">https://arxiv.org/pdf/2207.11240.pdf</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The experiments focus on the setting where a model is trained for one distribution and then adapted to a new one, as you can see in the figure below. The model is initialized by training on i.i.d. data, updating the decoder weights, and keeping the codebooks. When adapting the model to a distribution shift, the decoder is frozen, and only the codebook values are updated.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*rytDKrQCpVCDXjvI1zZhxQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*rytDKrQCpVCDXjvI1zZhxQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*rytDKrQCpVCDXjvI1zZhxQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*rytDKrQCpVCDXjvI1zZhxQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*rytDKrQCpVCDXjvI1zZhxQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*rytDKrQCpVCDXjvI1zZhxQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rytDKrQCpVCDXjvI1zZhxQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*rytDKrQCpVCDXjvI1zZhxQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*rytDKrQCpVCDXjvI1zZhxQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*rytDKrQCpVCDXjvI1zZhxQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*rytDKrQCpVCDXjvI1zZhxQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*rytDKrQCpVCDXjvI1zZhxQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*rytDKrQCpVCDXjvI1zZhxQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*rytDKrQCpVCDXjvI1zZhxQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*rytDKrQCpVCDXjvI1zZhxQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Source: <a href="https://arxiv.org/pdf/2207.11240.pdf" target="_self">https://arxiv.org/pdf/2207.11240.pdf</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Their experiments prove how this approach reduces catastrophic forgetting and results in more robust predictions. This work will not have a big short-term impact — results are not groundbreaking — but some of these ideas could be key catalyst for the next leap in <em>true </em>generalization.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><a href="https://arxiv.org/abs/2207.10342" target="_self">9. Language Model Cascades</a></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>By David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-dickstein, Kevin Murphy, Charles Sutton.</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>❓ Why → </strong>Large Language Models have become so powerful that they’re increasingly used as a black-box building block for other applications such as Reinforcement Learning or data augmentation.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>💡 Key insights → </strong>This work formalizes the interaction of Language Models from the lens of probabilistic programming: directed graphical models of random variables, which map to natural language strings. This turns out to be a powerful language to analyze existing advanced prompting techniques such as Scratchpad⁶ or chain-of-thought⁷. Moreover, it can be used to design new forms of interaction between LMs or advanced promoting techniques.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The practical value of this method is still a bit of a question, but it has the potential to be a powerful conceptual tool for building complex ML systems by abstracting away the classic Deep Learning toolbox (weights, embeddings, gradient descent, objective functions), creating the space for new higher-level mechanisms for reasoning.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*ISX2zskxq5y-rHgtcIgDNw.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*ISX2zskxq5y-rHgtcIgDNw.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*ISX2zskxq5y-rHgtcIgDNw.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*ISX2zskxq5y-rHgtcIgDNw.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*ISX2zskxq5y-rHgtcIgDNw.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ISX2zskxq5y-rHgtcIgDNw.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ISX2zskxq5y-rHgtcIgDNw.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*ISX2zskxq5y-rHgtcIgDNw.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*ISX2zskxq5y-rHgtcIgDNw.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*ISX2zskxq5y-rHgtcIgDNw.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*ISX2zskxq5y-rHgtcIgDNw.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*ISX2zskxq5y-rHgtcIgDNw.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*ISX2zskxq5y-rHgtcIgDNw.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*ISX2zskxq5y-rHgtcIgDNw.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*ISX2zskxq5y-rHgtcIgDNw.jpeg"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Source: <a href="https://arxiv.org/pdf/2207.10342.pdf" target="_self">https://arxiv.org/pdf/2207.10342.pdf</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7"><a href="https://arxiv.org/abs/2206.15049" target="_self">ZeroC: A Neuro-Symbolic Model for Zero-shot Concept Recognition and Acquisition at Inference Time</a></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>By Tailin Wu, Megan Tjandrasuwita, Zhengxuan Wu, Xuelin Yang, Kevin Liu, Rok Sosič, Jure Leskovec.</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>❓ Why → </strong>Neurosymbolics delivering on its promise..?</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>💡 Key insights → </strong>ZeroC is a method to represent concepts as graphs of constituent concept models (i.e. primary shapes like lines). The main objective of this paper is to build a system that can recognize unseen concepts at inference time. For instance, in the figure below, the letter F was not seen by the model, but it’s able to disentangle its components (lines) and their relationships (angle and positions), representing them as an explicit graph with 3 nodes and 3 edges.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*IFfE-FkEZ_2oRGkjttdA-A.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*IFfE-FkEZ_2oRGkjttdA-A.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*IFfE-FkEZ_2oRGkjttdA-A.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*IFfE-FkEZ_2oRGkjttdA-A.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*IFfE-FkEZ_2oRGkjttdA-A.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*IFfE-FkEZ_2oRGkjttdA-A.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IFfE-FkEZ_2oRGkjttdA-A.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*IFfE-FkEZ_2oRGkjttdA-A.png 640w, https://miro.medium.com/v2/resize:fit:720/1*IFfE-FkEZ_2oRGkjttdA-A.png 720w, https://miro.medium.com/v2/resize:fit:750/1*IFfE-FkEZ_2oRGkjttdA-A.png 750w, https://miro.medium.com/v2/resize:fit:786/1*IFfE-FkEZ_2oRGkjttdA-A.png 786w, https://miro.medium.com/v2/resize:fit:828/1*IFfE-FkEZ_2oRGkjttdA-A.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*IFfE-FkEZ_2oRGkjttdA-A.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*IFfE-FkEZ_2oRGkjttdA-A.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*IFfE-FkEZ_2oRGkjttdA-A.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Source: <a href="https://arxiv.org/pdf/2206.15049.pdf" target="_self">https://arxiv.org/pdf/2206.15049.pdf</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The recipe for training such a system relies on Energy-Based Models (EBMs): feed positive and negative image/graph representation pairs and minimize the energy of positive pairs while accounting for the graph invariances of the representation. The experiments show success in modest gridworld environments where the primary shapes and relationships are quite simple, but this represents a first step toward learning structure-rich representations that could become useful in the context of few-shot learning and generalization.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>References:</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>[1] “Scaling Laws for Neural Language Models” by Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei; 2020.</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>[2] “P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks” by Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, Jie Tang, 2022.</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>[3] “BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models” by</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, Iryna Gurevych; 2021.</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>[4] “ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT” by Omar Khattab, Matei Zaharia; 2020.</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>[6] “Show Your Work: Scratchpads for Intermediate Computation with Language Models” by Maxwell Nye et al. 2021.</em></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><em>[7] “Chain of Thought Prompting Elicits Reasoning in Large Language Models” by Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou; 2022.</em></p></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>