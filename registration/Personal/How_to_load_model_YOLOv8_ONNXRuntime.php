<!DOCTYPE html>
                <html>
                <head>
                    <title>How to load model YOLOv8 ONNXRuntime</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://alimustoofaa.medium.com/how-to-load-model-yolov8-onnx-runtime-b632fad33cec"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://medium.com/?source=post_page-----b632fad33cec--------------------------------">Author : Ali Mustofa</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>How to load model YOLOv8 ONNXRuntime</h3></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*UuJlywIL1eGZyUYMM-udJg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*UuJlywIL1eGZyUYMM-udJg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*UuJlywIL1eGZyUYMM-udJg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*UuJlywIL1eGZyUYMM-udJg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*UuJlywIL1eGZyUYMM-udJg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*UuJlywIL1eGZyUYMM-udJg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UuJlywIL1eGZyUYMM-udJg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*UuJlywIL1eGZyUYMM-udJg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*UuJlywIL1eGZyUYMM-udJg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*UuJlywIL1eGZyUYMM-udJg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*UuJlywIL1eGZyUYMM-udJg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*UuJlywIL1eGZyUYMM-udJg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*UuJlywIL1eGZyUYMM-udJg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*UuJlywIL1eGZyUYMM-udJg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*UuJlywIL1eGZyUYMM-udJg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">YOLOv8 is the latest version (v8) of the YOLO (You Only Look Once) object detection system. YOLO is a real-time, one-shot object detection system that aims to perform object detection in a single forward pass of the network, making it fast and efficient. YOLOv8 is an improved version of the previous YOLO models with improved accuracy and faster inference speed.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">ONNX (Open Neural Network Exchange) is an open format to represent deep learning models. To convert a YOLOv8 model to ONNX format, you need to use a tool such as ONNX Runtime, which provides an API to convert models from different frameworks to ONNX format. The exact steps would depend on the programming framework and tools you are using to develop and run your YOLOv8 model.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5"><strong>How to convert?</strong></h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>model = YOLO(<span>"yolov8n.pt"</span>)  <br/>results = model.predict(source=<span>"https://ultralytics.com/images/bus.jpg"</span>)[<span>0</span>]<br/><br/>model.export(<span>format</span>=<span>"onnx"</span>,imgsz=[<span>640</span>,<span>640</span>], opset=<span>12</span>)  <span># export the model to ONNX format</span></span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5"><strong>How to load ONNX Runtime?</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><code>onnxruntime</code> is an open-source runtime engine for executing machine learning models that are represented in the Open Neural Network Exchange (ONNX) format. ONNX is an open standard for representing deep learning models that allows for interoperability between different frameworks and tools. ONNX models can be exported from a variety of deep learning frameworks, such as PyTorch, TensorFlow, and Keras, and can be executed on different hardware platforms, such as CPUs, GPUs, and FPGAs.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><code>onnxruntime</code> provides a unified API for executing ONNX models across different hardware platforms and operating systems. It supports a wide range of execution providers, including CPU, GPU, and specialized accelerators, such as NVIDIA TensorRT and Intel OpenVINO. <code>onnxruntime</code> also provides support for model optimization and quantization to improve model performance and reduce memory and storage requirements.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><code>onnxruntime</code> can be used in a variety of applications, such as computer vision, natural language processing, and speech recognition. It provides a high-performance and flexible runtime engine that allows for efficient execution of deep learning models in production environments.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Load model</strong></p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span><span># Install onnx runtime</span><br/>pip3 install onnxruntime</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span><span>import</span> onnxruntime<br/><br/>opt_session = onnxruntime.SessionOptions()<br/>opt_session.enable_mem_pattern = <span>False</span><br/>opt_session.enable_cpu_mem_arena = <span>False</span><br/>opt_session.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_DISABLE_ALL</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The code you have provided sets up an onnxruntime.SessionOptions object with a few options. Let’s go through them one by one:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">enable_mem_pattern: This option controls whether memory pattern optimization is enabled or not. When enabled, onnxruntime can analyze the memory usage pattern of a model and allocate memory more efficiently, which can reduce memory usage and improve performance. Setting this option to False disables memory pattern optimization.</li><li class="ff3" style="font-size:22px;">enable_cpu_mem_arena: This option controls whether CPU memory arena is enabled or not. Memory arena is a technique used by onnxruntime to manage memory more efficiently. When enabled, onnxruntime allocates memory in chunks and reuses them, which can improve performance. Setting this option to False disables CPU memory arena.</li><li class="ff3" style="font-size:22px;">graph_optimization_level: This option controls the level of optimization performed on the ONNX graph during inference. onnxruntime provides several levels of optimization, ranging from ORT_DISABLE_ALL (which disables all optimization) to ORT_ENABLE_EXTENDED (which enables all available optimization). In the code you provided, ORT_DISABLE_ALL is used, which disables all optimization.</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>model_path = <span>'yolov8n.onnx'</span><br/>EP_list = [<span>'CUDAExecutionProvider'</span>, <span>'CPUExecutionProvider'</span>]<br/><br/>ort_session = onnxruntime.InferenceSession(model_path, providers=EP_list)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The code you provided sets up an onnxruntime.InferenceSession object, which is used to load an ONNX model and run inference on it. Let’s go through the parameters used:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">model_path: This parameter specifies the path to the ONNX model file that you want to load. In the example you provided, the path is set to ‘model_name.onnx’.</li><li class="ff3" style="font-size:22px;">providers: This parameter specifies the list of execution providers that should be used for inference. Execution providers are responsible for executing the operations in the ONNX model, and onnxruntime provides several built-in execution providers, including CPUExecutionProvider and CUDAExecutionProvider (which use CPU and GPU for inference, respectively). In the example you provided, a list of two execution providers is used: [‘CUDAExecutionProvider’, ‘CPUExecutionProvider’]. This means that onnxruntime will try to use the CUDAExecutionProvider first, and if it is not available (e.g., because there is no GPU), it will fall back to the CPUExecutionProvider.</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>model_inputs = ort_session.get_inputs()<br/>input_names = [model_inputs[i].name <span>for</span> i <span>in</span> <span>range</span>(<span>len</span>(model_inputs))]<br/>input_shape = model_inputs[<span>0</span>].shape<br/>input_shape</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The code you provided retrieves the input information for the loaded ONNX model using the get_inputs method of the onnxruntime.InferenceSession object. Let’s go through the variables that are created:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">model_inputs: This variable is a list of input nodes for the loaded ONNX model. Each input node is represented as an onnxruntime.NodeArg object, which contains information about the name, shape, and data type of the input.</li><li class="ff3" style="font-size:22px;">input_names: This variable is a list of the names of the input nodes for the model. The names can be used to identify the inputs when running inference on the model.</li><li class="ff3" style="font-size:22px;">input_shape: This variable contains the shape of the first input node in the model. Since the ONNX model can have multiple inputs, the shape of each input may be different. In the example you provided, the shape of the first input node is retrieved by accessing model_inputs[0].shape.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">By retrieving the input information for the model, you can prepare the input data that needs to be passed to the model when running inference. You can use the input names and shapes to create the input data in the correct format. Once you have prepared the input data, you can pass it to the run method of the onnxruntime.InferenceSession object to run inference on the model.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>model_output = ort_session.get_outputs()<br/>output_names = [model_output[i].name <span>for</span> i <span>in</span> <span>range</span>(<span>len</span>(model_output))]</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The code you provided retrieves the output information for the loaded ONNX model using the get_outputs method of the onnxruntime.InferenceSession object. Let’s go through the variables that are created:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">model_output: This variable is a list of output nodes for the loaded ONNX model. Each output node is represented as an onnxruntime.NodeArg object, which contains information about the name, shape, and data type of the output.</li><li class="ff3" style="font-size:22px;">output_names: This variable is a list of the names of the output nodes for the model. The names can be used to identify the outputs when running inference on the model.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">By retrieving the output information for the model, you can access the output data produced by the model during inference. You can use the output names and shapes to retrieve the output data in the correct format. Once you have retrieved the output data, you can use it to perform further processing or analysis.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Detection</strong></p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span><span>import</span> cv2<br/><span>import</span> numpy <span>as</span> np<br/><span>from</span> <span>PIL</span> <span>import</span> <span>Image</span><br/><br/>image = cv2.<span>imread</span>(<span>'bus.jpg'</span>)<br/>image_height, image_width = image.<span>shape</span>[:<span>2</span>]<br/><span>Image</span>.<span>fromarray</span>(cv2.<span>cvtColor</span>(image, cv2.<span>COLOR_BGR2RGB</span>))</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>input_height, input_width = input_shape[2:]<br/>image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)<br/>resized = cv2.resize(image_rgb, (input_width, input_height))<br/><br/><span># Scale input pixel value to 0 to 1</span><br/>input_image = resized / 255.0<br/>input_image = input_image.transpose(2,0,1)<br/>input_tensor = input_image[np.newaxis, :, :, :].astype(np.float32)<br/>input_tensor.shape</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Let’s go through the code you provided:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">input_height and input_width: These variables contain the height and width of the input tensor required by the ONNX model. The height and width were obtained from the input_shape variable, which was retrieved from the ONNX model earlier.</li><li class="ff3" style="font-size:22px;">image_rgb: This variable contains the input image data in RGB color format. The original image data was read using OpenCV’s imread function, which reads images in the BGR color format by default. The cvtColor function is used to convert the image data from BGR to RGB color format.</li><li class="ff3" style="font-size:22px;">resized: This variable contains the resized image data, which is resized to match the dimensions of the input tensor required by the ONNX model. The resize function is used to resize the image data.</li><li class="ff3" style="font-size:22px;">input_image: This variable contains the input tensor data after normalization and rearrangement of the image data. The pixel values of the resized image data are first scaled to the range of 0 to 1. Then, the dimensions of the image data are rearranged to match the input tensor format required by the ONNX model, which is (batch_size, channel, height, width).</li><li class="ff3" style="font-size:22px;">input_tensor: This variable contains the final input tensor data, which is a NumPy array with shape (batch_size, channel, height, width). The batch size is set to 1 since only a single image is being processed. The data type of the array is float32.</li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Once you have prepared the input tensor data, you can pass it to the run method of the onnxruntime.InferenceSession object to run inference on the model.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>outputs = ort_session.run(output_names, {input_names[<span>0</span>]: input_tensor})[<span>0</span>]</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">outputs: This variable contains the output data produced by the ONNX model during inference. The run method returns a tuple of outputs for the model, with each output represented as a NumPy array. In this case, we are only interested in the first output, which is why we use [0] to access it.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">To run inference on the model, we pass the input tensor data as a dictionary to the run method, where the keys are the names of the input nodes for the model (in this case, input_names[0]), and the values are the input tensor data. We also provide a list of output node names (output_names) so that the run method knows which outputs to return. Once the run method is called, it performs inference on the model and returns the output data in the form of NumPy arrays.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>predictions = np.squeeze(outputs).T<br/>conf_thresold = <span>0.8</span><br/><span># Filter out object confidence scores below threshold</span><br/>scores = np.<span>max</span>(predictions[:, <span>4</span>:], axis=<span>1</span>)<br/>predictions = predictions[scores &gt; conf_thresold, :]<br/>scores = scores[scores &gt; conf_thresold]  </span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The code you provided is used to filter out predictions with object confidence scores below a certain threshold. Let’s go through the variables that are created:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">conf_threshold: This variable specifies the threshold value for object confidence scores. Any predictions with object confidence scores below this threshold will be filtered out.</li><li class="ff3" style="font-size:22px;">scores: This variable contains the maximum object confidence score for each predicted bounding box. The object confidence score is the fifth element of the predicted values for each bounding box, i.e., predictions[:, 4:]. The max function is used to compute the maximum confidence score across all object classes for each predicted bounding box. The resulting scores array has shape (num_boxes,), where num_boxes is the number of predicted bounding boxes.</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span><span># Get the class with the highest confidence</span><br/>class_ids = np.argmax(predictions[:, <span>4</span>:], axis=<span>1</span>)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">class_ids: This variable contains the class IDs (i.e., the object classes) for each predicted bounding box. The class probabilities for each bounding box are given by the last num_classes elements of the predicted values for that bounding box, i.e., predictions[:, 4:]. The argmax function is used to find the index of the maximum probability (i.e., the predicted class) for each bounding box. The resulting class_ids array has shape (num_filtered_boxes,), where num_filtered_boxes is the number of filtered bounding boxes (i.e., those with confidence scores above the threshold). Each element of the class_ids array corresponds to the predicted object class for the corresponding bounding box</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span><span># Get bounding boxes for each object</span><br/>boxes = predictions[:, :<span>4</span>]<br/><br/><span>#rescale box</span><br/>input_shape = np.array([input_width, input_height, input_width, input_height])<br/>boxes = np.divide(boxes, input_shape, dtype=np.float32)<br/>boxes *= np.array([image_width, image_height, image_width, image_height])<br/>boxes = boxes.astype(np.int32)<br/>boxes</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The code you provided is used to compute the bounding box coordinates for each predicted object, and rescale them back to the original image size. Let’s go through the variables that are created:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">boxes: This variable contains the predicted bounding box coordinates for each object, in the format (x1, y1, x2, y2). The x1 and y1 coordinates are the top-left corner of the bounding box, and the x2 and y2 coordinates are the bottom-right corner of the bounding box. The boxes variable has shape (num_filtered_boxes, 4), where num_filtered_boxes is the number of filtered bounding boxes (i.e., those with confidence scores above the threshold). Each row of the boxes array corresponds to the predicted bounding box coordinates for the corresponding object.</li><li class="ff3" style="font-size:22px;">input_shape: This variable contains the shape of the input image, as (input_width, input_height, input_width, input_height). This is used to rescale the bounding box coordinates from the normalized input size (i.e., the size of the image that was passed to the model) back to the original image size.</li><li class="ff3" style="font-size:22px;">The first line of code simply gets the bounding box coordinates for each object from the predictions array. Since the bounding box coordinates are the first four elements of each row of the predictions array, we simply take the first four columns of the predictions array to get the boxes variable.</li><li class="ff3" style="font-size:22px;">The second line of code rescales the bounding box coordinates from the normalized input size to the original image size. This is done by dividing the boxes array by the input_shape array, element-wise, using numpy’s divide function. We cast the resulting array to np.float32 to avoid integer division.</li><li class="ff3" style="font-size:22px;">The third line of code multiplies the rescaled boxes array by the original image size, again element-wise. This rescales the bounding box coordinates from the normalized input size to the original image size. We cast the resulting array to np.int32 to ensure that the bounding box coordinates are integers (since we can’t have fractional pixel coordinates).</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span><span>def</span> <span>nms</span>(<span>boxes, scores, iou_threshold</span>):<br/>    <span># Sort by score</span><br/>    sorted_indices = np.argsort(scores)[<span>:</span><span>:-</span><span>1</span>]<br/><br/>    keep_boxes = []<br/>    <span>while</span> sorted_indices.size &gt; <span>0</span>:<br/>        <span># Pick the last box</span><br/>        box_id = sorted_indices[<span>0</span>]<br/>        keep_boxes.append(box_id)<br/><br/>        <span># Compute IoU of the picked box with the rest</span><br/>        ious = compute_iou(boxes[box_id, <span>:</span>], boxes[sorted_indices[<span>1</span><span>:</span>], <span>:</span>])<br/><br/>        <span># Remove boxes with IoU over the threshold</span><br/>        keep_indices = np.where(ious &lt; iou_threshold)[<span>0</span>]<br/><br/>        <span># print(keep_indices.shape, sorted_indices.shape)</span><br/>        sorted_indices = sorted_indices[keep_indices + <span>1</span>]<br/><br/>    <span>return</span> keep_boxes<br/><br/><span>def</span> <span>compute_iou</span>(<span>box, boxes</span>):<br/>    <span># Compute xmin, ymin, xmax, ymax for both boxes</span><br/>    xmin = np.maximum(box[<span>0</span>], boxes[<span>:</span>, <span>0</span>])<br/>    ymin = np.maximum(box[<span>1</span>], boxes[<span>:</span>, <span>1</span>])<br/>    xmax = np.minimum(box[<span>2</span>], boxes[<span>:</span>, <span>2</span>])<br/>    ymax = np.minimum(box[<span>3</span>], boxes[<span>:</span>, <span>3</span>])<br/><br/>    <span># Compute intersection area</span><br/>    intersection_area = np.maximum(<span>0</span>, xmax - xmin) * np.maximum(<span>0</span>, ymax - ymin)<br/><br/>    <span># Compute union area</span><br/>    box_area = (box[<span>2</span>] - box[<span>0</span>]) * (box[<span>3</span>] - box[<span>1</span>])<br/>    boxes_area = (boxes[<span>:</span>, <span>2</span>] - boxes[<span>:</span>, <span>0</span>]) * (boxes[<span>:</span>, <span>3</span>] - boxes[<span>:</span>, <span>1</span>])<br/>    union_area = box_area + boxes_area - intersection_area<br/><br/>    <span># Compute IoU</span><br/>    iou = intersection_area / union_area<br/><br/>    <span>return</span> iou<br/><br/><span># Apply non-maxima suppression to suppress weak, overlapping bounding boxes</span><br/>indices = nms(boxes, scores, <span>0.3</span>)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The code you provided is used to perform non-maximum suppression (NMS) on the filtered bounding boxes. NMS is a technique used in object detection to suppress overlapping bounding boxes and retain only the ones with the highest confidence scores</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Visualize</strong></p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span><span># Define classes </span><br/>CLASSES = [<br/> <span>'person'</span>, <span>'bicycle'</span>, <span>'car'</span>, <span>'motorcycle'</span>, <span>'airplane'</span>, <span>'bus'</span>, <span>'train'</span>, <span>'truck'</span>, <span>'boat'</span>, <br/> <span>'traffic light'</span>, <span>'fire hydrant'</span>, <span>'street sign'</span>, <span>'stop sign'</span>, <span>'parking meter'</span>, <span>'bench'</span>, <span>'bird'</span>, <span>'cat'</span>, <br/> <span>'dog'</span>, <span>'horse'</span>, <span>'sheep'</span>, <span>'cow'</span>, <span>'elephant'</span>, <span>'bear'</span>, <span>'zebra'</span>, <span>'giraffe'</span>, <span>'hat'</span>, <span>'backpack'</span>, <span>'umbrella'</span>, <br/> <span>'shoe'</span>, <span>'eye glasses'</span>, <span>'handbag'</span>, <span>'tie'</span>, <span>'suitcase'</span>, <span>'frisbee'</span>, <span>'skis'</span>, <span>'snowboard'</span>, <span>'sports ball'</span>, <br/> <span>'kite'</span>, <span>'baseball bat'</span>, <span>'baseball glove'</span>, <span>'skateboard'</span>, <span>'surfboard'</span>, <span>'tennis racket'</span>, <span>'bottle'</span>, <span>'plate'</span>, <br/> <span>'wine glass'</span>, <span>'cup'</span>, <span>'fork'</span>, <span>'knife'</span>, <span>'spoon'</span>, <span>'bowl'</span>, <span>'banana'</span>, <span>'apple'</span>, <span>'sandwich'</span>, <span>'orange'</span>, <span>'broccoli'</span>, <br/> <span>'carrot'</span>, <span>'hot dog'</span>, <span>'pizza'</span>, <span>'donut'</span>, <span>'cake'</span>, <span>'chair'</span>, <span>'couch'</span>, <span>'potted plant'</span>, <span>'bed'</span>, <span>'mirror'</span>, <br/> <span>'dining table'</span>, <span>'window'</span>, <span>'desk'</span>, <span>'toilet'</span>, <span>'door'</span>, <span>'tv'</span>, <span>'laptop'</span>, <span>'mouse'</span>, <span>'remote'</span>, <span>'keyboard'</span>, <br/> <span>'cell phone'</span>, <span>'microwave'</span>, <span>'oven'</span>, <span>'toaster'</span>, <span>'sink'</span>, <span>'refrigerator'</span>, <span>'blender'</span>, <span>'book'</span>, <span>'clock'</span>, <span>'vase'</span>,<br/> <span>'scissors'</span>, <span>'teddy bear'</span>, <span>'hair drier'</span>, <span>'toothbrush'</span><br/>]</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span><span>def</span> <span>xywh2xyxy</span>(<span>x</span>):<br/>    <span># Convert bounding box (x, y, w, h) to bounding box (x1, y1, x2, y2)</span><br/>    y = np.copy(x)<br/>    y[..., <span>0</span>] = x[..., <span>0</span>] - x[..., <span>2</span>] / <span>2</span><br/>    y[..., <span>1</span>] = x[..., <span>1</span>] - x[..., <span>3</span>] / <span>2</span><br/>    y[..., <span>2</span>] = x[..., <span>0</span>] + x[..., <span>2</span>] / <span>2</span><br/>    y[..., <span>3</span>] = x[..., <span>1</span>] + x[..., <span>3</span>] / <span>2</span><br/>    <span>return</span> y</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="iframe"><pre><span>image_draw = image.copy()<br/><span>for</span> (bbox, score, label) <span>in</span> <span>zip</span>(xywh2xyxy(boxes[indices]), scores[indices], class_ids[indices]):<br/>    bbox = bbox.<span>round</span>().astype(np.int32).tolist()<br/>    cls_id = <span>int</span>(label)<br/>    cls = CLASSES[cls_id]<br/>    color = (<span>0</span>,<span>255</span>,<span>0</span>)<br/>    cv2.rectangle(image_draw, <span>tuple</span>(bbox[:<span>2</span>]), <span>tuple</span>(bbox[<span>2</span>:]), color, <span>2</span>)<br/>    cv2.putText(image_draw,<br/>                <span>f'<span>{cls}</span>:<span>{<span>int</span>(score*<span>100</span>)}</span>'</span>, (bbox[<span>0</span>], bbox[<span>1</span>] - <span>2</span>),<br/>                cv2.FONT_HERSHEY_SIMPLEX,<br/>                <span>0.60</span>, [<span>225</span>, <span>255</span>, <span>255</span>],<br/>                thickness=<span>1</span>)</span></pre></div><br></div></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*RNOlmqavsgatEABPXzGMTw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*RNOlmqavsgatEABPXzGMTw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*RNOlmqavsgatEABPXzGMTw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*RNOlmqavsgatEABPXzGMTw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*RNOlmqavsgatEABPXzGMTw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*RNOlmqavsgatEABPXzGMTw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RNOlmqavsgatEABPXzGMTw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*RNOlmqavsgatEABPXzGMTw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*RNOlmqavsgatEABPXzGMTw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*RNOlmqavsgatEABPXzGMTw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*RNOlmqavsgatEABPXzGMTw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*RNOlmqavsgatEABPXzGMTw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*RNOlmqavsgatEABPXzGMTw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*RNOlmqavsgatEABPXzGMTw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*RNOlmqavsgatEABPXzGMTw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Results Detection</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5"><strong>Conclusion</strong></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">be used to perform object detection using a pre-trained YOLOv8n model in ONNX format. <code>onnxruntime</code> provides a flexible and high-performance runtime engine for executing deep learning models in production environments, and supports a wide range of hardware platforms and execution providers. The code also shows how the output of the model can be processed to extract the bounding boxes and class labels of the detected objects, which can be used for a variety of downstream applications, such as tracking, counting, or recognition.</p></div></div></div></section><section data-bs-version="5.1" class="content7 cid-ttbhFZC4Ql" id="content7-8" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
        <div class="container"><div class="row justify-content-center"><div class="col-12 col-md-9"><blockquote><p class="ff4">If you found this article helpful, please consider following my medium account for more informative and insightful content. Thanks</p></blockquote></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Link</strong> <strong>Notebook </strong>: <a href="https://colab.research.google.com/drive/1lIaP7r3I4kSgi3AVYf6fhgRZ56JMpXEL?usp=sharing" target="_self">https://colab.research.google.com/drive/1lIaP7r3I4kSgi3AVYf6fhgRZ56JMpXEL?usp=sharing</a></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">next :</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><a href="https://medium.com/@alimustoofaa/deploy-yolov8-onnx-1cbc02395a85" target="_self">deploy</a> <a href="https://medium.com/@alimustoofaa/deploy-yolov8-onnx-1cbc02395a85" target="_self">model</a></li><li class="ff3" style="font-size:22px;"><a href="https://medium.com/how-to-load-model-yolov8-onnx-runtime-b632fad33cec" target="_self">using</a> <a href="https://medium.com/how-to-load-model-yolov8-onnx-runtime-b632fad33cec" target="_self">onnxruntime</a></li></ul></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Reach me at</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;">LinkedIn : <a href="http://www.linkedin.com/in/alimustoofaa" target="_self">www.linkedin.com/in/alimustoofaa</a></li><li class="ff3" style="font-size:22px;">Email : hai.alimustofa@gmail.com</li><li class="ff3" style="font-size:22px;">Github : <a href="https://github.com/Alimustoofaa" target="_self">https://github.com/Alimustoofaa</a></li><li class="ff3" style="font-size:22px;">Twitter : <a href="https://twitter.com/Alimustoofaa" target="_self">https://twitter.com/Alimustoofaa</a></li></ul></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>