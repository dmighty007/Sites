<!DOCTYPE html>
                <html>
                <head>
                    <title>6 Papers Every Modern Data Scientist Must Read</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://towardsdatascience.com/6-papers-every-modern-data-scientist-must-read-1d0e708becd"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://shakedzy.medium.com/?source=post_page-----1d0e708becd--------------------------------">Author : Shaked Zychlinski</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>6 Papers Every Modern Data Scientist Must Read</h3></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5 text-muted">A list of some of the most important modern fundamentals of Deep Learning everyone in the field show be familiar with</h4></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*BJEZj3MB7iZO4qE95gFk2w.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*BJEZj3MB7iZO4qE95gFk2w.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*BJEZj3MB7iZO4qE95gFk2w.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*BJEZj3MB7iZO4qE95gFk2w.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*BJEZj3MB7iZO4qE95gFk2w.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*BJEZj3MB7iZO4qE95gFk2w.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BJEZj3MB7iZO4qE95gFk2w.jpeg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*BJEZj3MB7iZO4qE95gFk2w.jpeg 640w, https://miro.medium.com/v2/resize:fit:720/1*BJEZj3MB7iZO4qE95gFk2w.jpeg 720w, https://miro.medium.com/v2/resize:fit:750/1*BJEZj3MB7iZO4qE95gFk2w.jpeg 750w, https://miro.medium.com/v2/resize:fit:786/1*BJEZj3MB7iZO4qE95gFk2w.jpeg 786w, https://miro.medium.com/v2/resize:fit:828/1*BJEZj3MB7iZO4qE95gFk2w.jpeg 828w, https://miro.medium.com/v2/resize:fit:1100/1*BJEZj3MB7iZO4qE95gFk2w.jpeg 1100w, https://miro.medium.com/v2/resize:fit:1400/1*BJEZj3MB7iZO4qE95gFk2w.jpeg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*BJEZj3MB7iZO4qE95gFk2w.jpeg"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Photo by <a href="https://unsplash.com/@itfeelslikefilm?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText" target="_self">üá∏üáÆ Janko Ferliƒç</a> on <a href="https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText" target="_self">Unsplash</a></p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Listen to me discussing these papers on </strong><a href="https://open.spotify.com/episode/18hytWnvHhdIwphew2PnaS?si=9795e432804b42f6" target="_self">ExplAInable Podcast</a><strong> [Hebrew]</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Data Scientist, Machine Learning Expert, Algorithm Engineer, Deep Learning Researcher ‚Äî whatever your title might be, if using advanced concepts of Machine Learning is part of your career, then keeping up to date with the latest innovations is also a part of your everyday tasks. But in order to be on-top of all the latest ingenuities and truly understand how they work, we must also be familiar with the building blocks and foundations they rely on.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The field of Deep Learning is moving fast, breaking and setting new records in each and every possible metric exists. And as it evolves, it creates new fundamental concepts, allowing new architectures and concepts never seen before.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">While I tend to assume all modern ML-practitioners are familiar with the basics fundamentals, such as CNN, RNN, LSTM and GAN, some of the newer ones are occasionally missed or left out. And so, this blogpost will discuss the <em>new</em> fundamentals ‚Äî six papers I believe everyone in this field today should be familiar with.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:80%;"><iframe  scrolling="auto" width="435.0" height="318.0" frameborder="0" src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fgiphy.com%2Fembed%2FWoWm8YzFQJg5i%2Ftwitter%2Fiframe&display_name=Giphy&url=https%3A%2F%2Fmedia.giphy.com%2Fmedia%2FWoWm8YzFQJg5i%2Fgiphy.gif&image=https%3A%2F%2Fi.giphy.com%2Fmedia%2FWoWm8YzFQJg5i%2Fgiphy.gif&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=giphy" allowfullscreen=""></iframe></div><br><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">(1) Attention Is All You Need</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">[<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_self">Paper on arXiv</a>]</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Released in 2017 by a team from Google, this paper has revealed to the world a new neural-network block called a <em>Transformer ‚Äî </em>and can easily be marked as one of the most significant milestones in the development of modern Deep Learning models.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Transformers allow processing of sequences in a parallel method, unlike the preceding state-of-the-art which relied heavily on types of RNNs. The latter tend to have a few major drawbacks ‚Äî RNN layers have to rely on their own output values of the previous input. That causes slow training time, and also the well-known issue of vanishing/exploding gradients, causing RNNs to be inadequate to find relations between words which are too far apart in a sequence.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Transformers were able to tackle these issues by using two novel approaches ‚Äî the first, they used <em>positional-embeddings</em> to mark the location of each element in the sequence, and the second was the use of <em>attention</em>, and specifically self-attention, to allow the model to learn the relations between different elements of the sequence.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Other than the transformers themselves, the paper is filled with little gems, optimizing the model. Here are my personal favorite two:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ol><li class="ff3" style="font-size:22px;">The authors strictly mention they use Layer Normalization instead of Batch Normalization. Summarizing the original <a href="https://arxiv.org/pdf/1607.06450.pdf" target="_self">Layer Normalization paper</a>, it basically means normalization isn‚Äôt per feature over a batch, but over the entire sample (or layer), each sample by itself. The paper proves LN is better than BN in many cases, and as Geoffrey Hinton is one of the three authors, I‚Äôll take his word for that.</li><li class="ff3" style="font-size:22px;">Each sub-layer of the transformers in the Attention paper is wrapped by a skip-layer, or a residual-block (see image below). The idea behind this block is that given an input <strong>x</strong>, a network tries to learn some function <strong>H(x)</strong>, which can be loosely written as <strong>H(x) = F(x) + x</strong>. Using the skip-layer mechanism, we force the middle layer to learn <strong>F(x)</strong>, and according to the original <a href="https://arxiv.org/pdf/1512.03385.pdf" target="_self">Residual Learning layer</a>, it converges better this way.</li></ol></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*6HDuqhUzP92iXhHoS0Wl3w.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*6HDuqhUzP92iXhHoS0Wl3w.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*6HDuqhUzP92iXhHoS0Wl3w.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*6HDuqhUzP92iXhHoS0Wl3w.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*6HDuqhUzP92iXhHoS0Wl3w.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*6HDuqhUzP92iXhHoS0Wl3w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6HDuqhUzP92iXhHoS0Wl3w.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*6HDuqhUzP92iXhHoS0Wl3w.png 640w, https://miro.medium.com/v2/resize:fit:720/1*6HDuqhUzP92iXhHoS0Wl3w.png 720w, https://miro.medium.com/v2/resize:fit:750/1*6HDuqhUzP92iXhHoS0Wl3w.png 750w, https://miro.medium.com/v2/resize:fit:786/1*6HDuqhUzP92iXhHoS0Wl3w.png 786w, https://miro.medium.com/v2/resize:fit:828/1*6HDuqhUzP92iXhHoS0Wl3w.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*6HDuqhUzP92iXhHoS0Wl3w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*6HDuqhUzP92iXhHoS0Wl3w.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*6HDuqhUzP92iXhHoS0Wl3w.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">A residual block built of skip-layers. Taken from ‚ÄúAttention is All You Need‚Äù</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">(2) <mark>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</mark></h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">[<a href="https://arxiv.org/pdf/1810.04805.pdf" target="_self">Paper on arXiv</a>]</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Released in 2019 by researchers from Google, BERT is an NLP model non-one could ignore, and took the NLP field one huge leap forward. BERT (which stands for Bidirectional Encoder Representations from Transformers) was developed using transformers, and became a live proof of the power these type of models can hold. Personally, I tend to consider ‚Äú<em>Attention is All You Need</em>‚Äù and this paper as somewhat complementary; the first describing a more general and ‚Äútheoretical‚Äù approach, and the latter uses it for a specific and well-defined task.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">BERT was revolutionary for two main reasons: first, it was able to beat the state of the art results for <em>eleven</em> NLP tasks, and second ‚Äî it was trained and designed to be used for fine-tuning, so it can be easily matched and tailor-made for any specific NLP tasks.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">One of the key-factors which allowed BERT to be trained so efficiently was the use of <a href="https://arxiv.org/pdf/1609.08144.pdf" target="_self">WordPiece embeddings</a>, which allowed the model to break down words to fragments, making the vocabulary to be learned much smaller.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">(3) A Style-Based Generator Architecture for Generative Adversarial Networks</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">[<a href="https://arxiv.org/pdf/1812.04948.pdf" target="_self">Paper on arXiv</a>]</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This paper, published by Nvidia Labs in the end of 2018, has introduced the world to the StyleGAN model. Ever since Ian Goodfellow introduced the <a href="https://arxiv.org/pdf/1406.2661.pdf" target="_self">Generative Adversarial Networks</a> (GANs) in 2014, many researchers have done tremendous work in improving GAN capabilities, but mostly by focusing on the discriminator, as better a better discriminator will eventually lead to a better generator. Yet, this lack of focus on the generator itself caused ‚Äú<em>‚Ä¶ the generators continue to operate as black boxes, and despite recent efforts, the understanding of various aspects of the image synthesis process, e.g., the origin of stochastic features, is still lacking</em>‚Äù<em>. </em>The idea behind StyleGAN was to allow tuning some of the features of the output image using another input (a second image) by directly affecting the generator itself.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Mixing in the style of one input with the generated second input was done in two main steps ‚Äî first, the style-input was inserted to a dense network, outputting another learned vector. Second, a novel ‚Äú<em>Adaptive Instance Normalization</em>‚Äù (AdaIN) is applied ‚Äî the original-image input is normalized, and then the new style-embedding is mixed in:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:50%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 381px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*W11Toro4c8zSTDOPY77wkg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*W11Toro4c8zSTDOPY77wkg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*W11Toro4c8zSTDOPY77wkg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*W11Toro4c8zSTDOPY77wkg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*W11Toro4c8zSTDOPY77wkg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*W11Toro4c8zSTDOPY77wkg.png 1100w, https://miro.medium.com/v2/resize:fit:762/format:webp/1*W11Toro4c8zSTDOPY77wkg.png 762w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 381px" srcset="https://miro.medium.com/v2/resize:fit:640/1*W11Toro4c8zSTDOPY77wkg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*W11Toro4c8zSTDOPY77wkg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*W11Toro4c8zSTDOPY77wkg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*W11Toro4c8zSTDOPY77wkg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*W11Toro4c8zSTDOPY77wkg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*W11Toro4c8zSTDOPY77wkg.png 1100w, https://miro.medium.com/v2/resize:fit:762/1*W11Toro4c8zSTDOPY77wkg.png 762w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:381/1*W11Toro4c8zSTDOPY77wkg.png"></picture></div><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">But what I personally believe is the most interesting part of this paper, is how this model was trained and evaluated. Think of it for a second ‚Äî there is no dataset the network can learn from. And so, what the researchers did is nothing but extraordinary data science ‚Äî they understood and leveraged the data they have.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">When it comes to the loss, the authors explicitly mention they used the standard GAN loss ‚Äî it makes sense, as the images produced by StyleGAN should look realistic, just like any regular GAN produces. But when it comes to evaluation ‚Äî meaning, the output‚Äôs style is indeed altered ‚Äî they noticed what they call ‚Äúfeature entanglement‚Äù. They claim that since the photos the network was trained on are real, some facial features are correlated with one another ‚Äî for example, long hair will usually be associated with women, beards with men, blue eyes with caucasians, etc. So they trained a classifier for about 40 such attributes, and have shown that StyleGAN generated photos with higher attribute-separability than a regular GAN.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">(4) Learning Transferable Visual Models From Natural Language Supervision</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">[<a href="https://arxiv.org/pdf/2103.00020.pdf" target="_self">Paper on arXiv</a>]</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This paper, released in early 2021 by OpenAI, is probably one of the greatest revolutions in zero-shot classification algorithms, presenting a novel model known as <em>Contrastive Language-Image Pre-Training</em>, or CLIP for short.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">CLIP was trained over a massive dataset of 400 million pairs of images and their corresponding captions, and has learnt to embed both images and free-text to the same embedding-space, so that the image and its free-text description will share the same latent vector. CLIP works with any image and any English text, and has demonstrated outstanding capabilities in zero-shot classification tasks.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">More over, has been shown by several following works, latent vectors in CLIP space follow the arithmetic we are familiar with from NLP. Meaning, as we‚Äôve all learnt that in Word2Vec latent space, we get <em>Man + Queen - Woman = King</em>, then in CLIP space we can have the same equation, but while using the words <em>Man </em>and <em>Woman</em>, and images of <em>King </em>and <em>Queen</em>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">(5) Mastering the Game of Go with Deep Neural Networks and Tree Search</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">[<a href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf" target="_self">Paper on DeepMind‚Äôs site</a>]</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Released by DeepMind in early 2016, this paper has made a breakthrough in Reinforcement Learning when it presented <em>AlphaGo</em> ‚Äî a model which defeated the European Go champion by 5 games to 0. This was the first AI ever that has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">AlphaGo uses <a href="https://hal.inria.fr/inria-00116992/document" target="_self">Monte-Carlo Tree Search</a> (MCTS) in order to compute its next move ‚Äî meaning, it doesn‚Äôt really computes each and every node in the tree, but rather runs many simulations of the possible outcomes.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">AlphaGo authors indicate they trained a Policy Gradient model which was optimized over a Supervised Learning model, which in turn learned from recorded human experts games. But this heavy network was too slow to be used in real games for running simulations over MCTS, so they had to find a replacement. They instead trained two networks ‚Äî one lightweight Policy Gradient, which was far weaker, but also &gt;1000 times faster, and a second Value Network, which predicted the value (meaning, the chances of winning) for each state in the game. That Value Network learned over self-played games played by the heavy Policy Gradient, thus learning directly from it. At each time step of a MCTS simulation, the lightweight network was used to generate a gameplay from the current state till the game ended, and the reward it received was combined with the Value Network assessment of current state in order to provide a Q-Value assessment of that state. Once all simulations terminated, AlphaGo selected the state it visited the most during these exploration simulations.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*dgCmHk_iaArrTQRyNO8BtQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*dgCmHk_iaArrTQRyNO8BtQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*dgCmHk_iaArrTQRyNO8BtQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*dgCmHk_iaArrTQRyNO8BtQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*dgCmHk_iaArrTQRyNO8BtQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*dgCmHk_iaArrTQRyNO8BtQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dgCmHk_iaArrTQRyNO8BtQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*dgCmHk_iaArrTQRyNO8BtQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*dgCmHk_iaArrTQRyNO8BtQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*dgCmHk_iaArrTQRyNO8BtQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*dgCmHk_iaArrTQRyNO8BtQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*dgCmHk_iaArrTQRyNO8BtQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*dgCmHk_iaArrTQRyNO8BtQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*dgCmHk_iaArrTQRyNO8BtQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*dgCmHk_iaArrTQRyNO8BtQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">A MCTS phase of AlphaGo, taken from ‚ÄúMastering the Game of Go with Deep Neural Networks and Tree Search‚Äù</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In 2017, a year after this paper was published, DeepMind published a second Go-related paper, ‚Äú<a href="https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ" target="_self">Mastering the Game of Go without Human Knowledge</a>‚Äù, which presented <em>AlphaGo Zero</em>, which was trained only using self-play, contained a single network instead of two ‚Äî and beat AlphaGo 100‚Äì0.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-7">(6) Deep Neural Networks for YouTube Recommendations</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">[<a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf" target="_self">Paper on Google Research</a>]</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Released in 2016, this paper sheds light on the architecture of Deep Learning models used for recommendations in YouTube. In many ways, the high-level architecture and methods described in this paper are used widely in the industry till today.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This paper is a classic go-to for every Recommendations-using-Deep-Learning question you can probably think of, and covers a wide range of topics such as high-level architecture, handling massive scale (using candidate generation and ranking models), classification as extreme multi-class classification, optimizing training- and test-data for accurate predictions, the impact of features, the impact of increasing the network, using and choosing embedding and much much more. Bottom line, if you‚Äôre interested in the field of recommendation systems, this one is a must.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This concludes my top 6 papers every modern data scientist must read ‚Äî let me know if you think I missed a paper!</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:80%;"><iframe  scrolling="auto" width="435.0" height="339.0" frameborder="0" src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fgiphy.com%2Fembed%2FRTYYVD3pNBDXi%2Ftwitter%2Fiframe&display_name=Giphy&url=https%3A%2F%2Fmedia.giphy.com%2Fmedia%2FRTYYVD3pNBDXi%2Fgiphy.gif&image=https%3A%2F%2Fi.giphy.com%2Fmedia%2FRTYYVD3pNBDXi%2Fgiphy.gif&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=giphy" allowfullscreen=""></iframe></div><br><br></div></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>