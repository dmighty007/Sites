<!DOCTYPE html>
                <html>
                <head>
                    <title>Top Deep Learning Papers of 2022</title>
                <?php include_once 'Elemental/header.php'; ?><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><br><br><h5> This article is reformatted from originally published at <a href="https://medium.com/@diegobonila/top-deep-learning-papers-of-2022-a4826e0aac4"><strong>TDS(Towards Data Science)</strong></a></h5></br><h5> <a href="https://medium.com/@diegobonila?source=post_page-----a4826e0aac4--------------------------------">Author : Diego Bonilla</a> </h5></div></div></div></section><section data-bs-version="5.1" class="content4 cid-tt5SM2WLsM" id="content4-2" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
            <div class="container">
                <div class="row justify-content-center">
                    <div class="title col-md-12 col-lg-9">
                        <h3 class="mbr-section-title mbr-fonts-style mb-4 display-2">
                            <strong>Top Deep Learning Papers of 2022</h3></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I know we are all busy generating basic Python code in ChatGPT, but if you lend me 10 minutes of your time we can review together the <strong>Top Deep Learning Papers of 2022</strong>.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/0*wTDyu_OL3qf0o5mS.jpg 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/0*wTDyu_OL3qf0o5mS.jpg 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/0*wTDyu_OL3qf0o5mS.jpg 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/0*wTDyu_OL3qf0o5mS.jpg 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/0*wTDyu_OL3qf0o5mS.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/0*wTDyu_OL3qf0o5mS.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wTDyu_OL3qf0o5mS.jpg 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/0*wTDyu_OL3qf0o5mS.jpg 640w, https://miro.medium.com/v2/resize:fit:720/0*wTDyu_OL3qf0o5mS.jpg 720w, https://miro.medium.com/v2/resize:fit:750/0*wTDyu_OL3qf0o5mS.jpg 750w, https://miro.medium.com/v2/resize:fit:786/0*wTDyu_OL3qf0o5mS.jpg 786w, https://miro.medium.com/v2/resize:fit:828/0*wTDyu_OL3qf0o5mS.jpg 828w, https://miro.medium.com/v2/resize:fit:1100/0*wTDyu_OL3qf0o5mS.jpg 1100w, https://miro.medium.com/v2/resize:fit:1400/0*wTDyu_OL3qf0o5mS.jpg 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/0*wTDyu_OL3qf0o5mS.jpg"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">A stolen image depicting you finding out about this article</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Second year making this kind of article after the popularity of the <a href="https://medium.com/@diegobonila/top-deep-learning-papers-of-2021-529a6f2e17cb" target="_self">first one</a> (shameless plug). This year was <strong>amazing</strong> for Deep Learning. Of course generative models made huge breakthroughs and models keep getting bigger, smarter, and… well, bigger (Artificial General Intelligence? More like having a budget as big as some countries’ GDPs). Needless to say that my experience with Deep Learning might be very different from yours: I’m very into Computer Vision and Non-Supervised Learning and you might be more interested in Supervised NLP. Just letting you know, specially you, that this contains a miniscule amount of <strong>bias for Computer Vision</strong>. Is that a dealbreaker for you? If it is I’m sorry but I’m like this and you’ll have to accept me for who I am. If it isn’t, enough with this nonsense introduction and <strong>let’s go!</strong></p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Disclaimer! ☢️</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Altough some technical knowledge is required for a deeper understanding, I’ve tried to explain everything in plain english (focus on <em>tried</em>). Also, I always try not to get too serious with anything so expect that I guess. Lastly, the papers’ choice is very personal as mentioned, so there might be better ones out there. If you think so, let me know in the comments so everybody can get to know them too!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">VicReg (January 2022)</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Self-Supervised Learning, the path of intelligence according to Yann LeCun. Networks that are capable of learning from labeless data. The worst nightmare for anyone who loves spending hundreds of hours and thousands of dollars labeling. That’s how the enemy of Supervision looks like. However, it has some flaws.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Self-Supervised Learning is generally harder to train, needs special losses or architectures and reaches less accuracy than Supervised Learning. It also has an affinity to collapse. The latter was prevented using large batch sizes of positive and negative examples or by only forcing the negative examples’ embeddings to separate. Other works like <a href="https://arxiv.org/pdf/2011.10566.pdf" target="_self">SimSiam</a> introduced a stop-gradient operation as the hip new trick.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">But what if you were looking for something that could prevent collapse, does not require weight sharing, nor batch normalization, nor stop-gradient, nor contrastive samples? Well, you picky eater are looking for <a href="https://arxiv.org/pdf/2105.04906.pdf" target="_self">VicReg</a>. The basic idea is to introduce a loss that preserves the information content of the embeddings. How? With three terms: <strong>Invariance, Variance and Covariance. </strong>These terms ensure that the embeddings are different whilst preventing collapse. Very neat!</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*UbN0ytup97Y37-If5s3AZQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*UbN0ytup97Y37-If5s3AZQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*UbN0ytup97Y37-If5s3AZQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*UbN0ytup97Y37-If5s3AZQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*UbN0ytup97Y37-If5s3AZQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*UbN0ytup97Y37-If5s3AZQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UbN0ytup97Y37-If5s3AZQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*UbN0ytup97Y37-If5s3AZQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*UbN0ytup97Y37-If5s3AZQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*UbN0ytup97Y37-If5s3AZQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*UbN0ytup97Y37-If5s3AZQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*UbN0ytup97Y37-If5s3AZQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*UbN0ytup97Y37-If5s3AZQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*UbN0ytup97Y37-If5s3AZQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*UbN0ytup97Y37-If5s3AZQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">VICReg: joint embedding architecture with variance, invariance and covariance regularization</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The result is a very simple approach with little restrictions that archives results as good as the State of the Art. Also, and most importantly, they’ve linked an amazing GitHub repo with everything needed to implement it. More of this, please! ❤️</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This method was so cool that even Yann and company went back to try to understand how that black box thingy works. They published a more technical <a href="https://arxiv.org/pdf/2207.10081.pdf" target="_self">paper</a> explaining with greek letters <strong>why Self-Supervised learning, especially VicReg, learns</strong> and why they were right six months ago.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">But that’s not all! They came back with <a href="https://arxiv.org/pdf/2210.01571.pdf" target="_self">VICRegL</a>, the Digivolution of VicReg. This new method <strong>learns local and global features from images simultaneously </strong>which means is good for classification and segmentation tasks. It works by applying VicReg globally, with a global expander; and locally, via a local projector. Can’t wait for the technical explanation of this one six months from now!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">ConvNeXt (March 2022)</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This one is for those Computer Vision lovers who hate NLP and the mere presence of Transformers in their architectures is vomit inducing. Those people who know that Convolutions can still give State of the Art results in 2022. Those who finally understand how CNNs work and find out that no one is using them anymore… Well, it is with great pleasure to inform you that they are back. Stronger than ever.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In the paper <a href="https://arxiv.org/pdf/2201.03545.pdf" target="_self">A ConvNet for the 2020s</a>, the limits of what’s possible with CNNs is explored by taking ideas from Transformers. This might seem unsportsmanlike but, Mom… they did it first! They introduced CNNs priors in Swin Transformers! So it’s only fair IMO. Also, <strong>they archived better accuracy than Transformers</strong>.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Question: But, how?; Answer: With care, love and step by step. First, we take the well known and loved ResNet-50 model and train it like we train a ViT. Uh, already 78.8% accuracy! Let’s keep going. Then, we change the number of blocks to match Swin-T. Nice, 79.4% accuracy! Let’s now patchify the image like Transformers. Great, 79.5%! How about… using Depthwise Convolutions, and invert the bottlenecks, and… large kernel sizes! Where are we now… 80.5%?! Okay… ehmm… let’s see… who uses ReLU nowadays, right? We shall use GELU! If GPT and BERT use it, can’t be that bad. What else… too many activations and BatchNorm, let’s get rid of some… BatchNorm?! No, no, no, LayerNorm is the trend, let’s use that and also separate the downsampling layers, why? Idk, Swin-T does it, ask them… Where are we… <strong>87.8% top-1 accuracy on ImageNet 1k</strong>??!</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*jMa4vH5HuW_OYCWLzgIlHg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*jMa4vH5HuW_OYCWLzgIlHg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*jMa4vH5HuW_OYCWLzgIlHg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*jMa4vH5HuW_OYCWLzgIlHg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*jMa4vH5HuW_OYCWLzgIlHg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*jMa4vH5HuW_OYCWLzgIlHg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jMa4vH5HuW_OYCWLzgIlHg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*jMa4vH5HuW_OYCWLzgIlHg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*jMa4vH5HuW_OYCWLzgIlHg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*jMa4vH5HuW_OYCWLzgIlHg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*jMa4vH5HuW_OYCWLzgIlHg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*jMa4vH5HuW_OYCWLzgIlHg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*jMa4vH5HuW_OYCWLzgIlHg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*jMa4vH5HuW_OYCWLzgIlHg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*jMa4vH5HuW_OYCWLzgIlHg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">ImageNet-1K classification results</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Yep, we did it. CNNs did a comeback in 2022 and even dethroned our mortal enemies: the Transformers. This didn’t only put CNNs again in the scope of researchers, but also taught the NLP proud-of-oneselfs… you shoud rethink the importance of CNNs! They are still here. For now.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">They are so here in fact that, in November, Microsoft released the <a href="https://arxiv.org/pdf/2203.11926.pdf" target="_self">FocalNets</a> (thanks for not calling the paper <em>Focal Modulation is all we need for Vision</em> or something like that, much appreciated), a <strong>new State of the Art on ImageNet</strong> that replaces the Self-Attention mechanism using only Convolutions.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*k8bdZfqOmlX2PNfkrKmlIA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*k8bdZfqOmlX2PNfkrKmlIA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*k8bdZfqOmlX2PNfkrKmlIA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*k8bdZfqOmlX2PNfkrKmlIA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*k8bdZfqOmlX2PNfkrKmlIA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*k8bdZfqOmlX2PNfkrKmlIA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k8bdZfqOmlX2PNfkrKmlIA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*k8bdZfqOmlX2PNfkrKmlIA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*k8bdZfqOmlX2PNfkrKmlIA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*k8bdZfqOmlX2PNfkrKmlIA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*k8bdZfqOmlX2PNfkrKmlIA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*k8bdZfqOmlX2PNfkrKmlIA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*k8bdZfqOmlX2PNfkrKmlIA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*k8bdZfqOmlX2PNfkrKmlIA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*k8bdZfqOmlX2PNfkrKmlIA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Left: Comparing SA (a) and Focal Modulation (b) side by side. Right: Detailed illustration of context aggregation in Focal Modulation (c)</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Another win for CNNs!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Cold Diffusion (August 2022)</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Of course I had to mention Diffusion somehow in 2022. But I hope is not in the way that everyone thinks about it like <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.pdf" target="_self">Stable Diffusion</a> or <a href="https://arxiv.org/pdf/2111.05826.pdf" target="_self">Palette</a> and so on. The paper I’m going to show is about <strong>the generalization of the Diffusion process</strong>. We all know Diffusion proces, right? You first take an image and add Gaussian Noise slowly until is pretty much just noise. Then remove it slowly using the magic of AI until the original image is reconstructed. If you learn to do this for a long time you can sample just noise and make the network slowly “imagine” what is in there. So seems like the key word is <strong>noise</strong>.<strong> </strong>Or is it…</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*A08RIvY9MURdkwBEJao5zA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*A08RIvY9MURdkwBEJao5zA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*A08RIvY9MURdkwBEJao5zA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*A08RIvY9MURdkwBEJao5zA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*A08RIvY9MURdkwBEJao5zA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*A08RIvY9MURdkwBEJao5zA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A08RIvY9MURdkwBEJao5zA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*A08RIvY9MURdkwBEJao5zA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*A08RIvY9MURdkwBEJao5zA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*A08RIvY9MURdkwBEJao5zA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*A08RIvY9MURdkwBEJao5zA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*A08RIvY9MURdkwBEJao5zA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*A08RIvY9MURdkwBEJao5zA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*A08RIvY9MURdkwBEJao5zA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*A08RIvY9MURdkwBEJao5zA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Demonstration of the forward and backward processes for both hot and cold diffusions</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">You have been played, my friend. Thinking is all about noise and forgetting <strong>there are a lot of</strong> <strong>other image degradation types</strong>. Even deterministic ones! This gives a new way of understanding the Diffusion Process and makes it easily generalizable to other arbitrary processes. <a href="https://arxiv.org/pdf/2208.09392.pdf" target="_self">Cold Diffusion</a> (nice name btw) creates a Diffusion Process that <strong>does not require any randomness (like Gaussian Noise)</strong> that the common architectures rely on. The use of noise is tought to have some nice properties like as data augmentation that makes low density regions perform better; but, as it turns out, noise is not a necessity after all.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">The authors propose a different sampling method that takes in account smooth and differentiable degradations. Then, tried to Diffuse them using Blur, Inpainting, Super-Resolution and Snowification and generated nice reconstructions proving that the kingdom of noise is no more. But hold on, this is not all. Winter is comming… ❄️.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Cold Generation uses deterministic noise</strong> (what an <a href="https://www.merriam-webster.com/dictionary/oxymoron" target="_self">oxymoron</a>!) that is sampled and frozen at the start of the process and treated as a constant.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Ddoz68l094UNzy8qw6KTrg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Ddoz68l094UNzy8qw6KTrg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Ddoz68l094UNzy8qw6KTrg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Ddoz68l094UNzy8qw6KTrg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Ddoz68l094UNzy8qw6KTrg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Ddoz68l094UNzy8qw6KTrg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ddoz68l094UNzy8qw6KTrg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Ddoz68l094UNzy8qw6KTrg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*Ddoz68l094UNzy8qw6KTrg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*Ddoz68l094UNzy8qw6KTrg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*Ddoz68l094UNzy8qw6KTrg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*Ddoz68l094UNzy8qw6KTrg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*Ddoz68l094UNzy8qw6KTrg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*Ddoz68l094UNzy8qw6KTrg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*Ddoz68l094UNzy8qw6KTrg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Examples of generated samples from 128 × 128 CelebA and AFHQ datasets using cold diffusion with blur transformation</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">In brief: Diffusion Processes just got generalized, noise is no longer necessary, and welcome deterministic degradations and its diversity and properties!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Data2Vec (October 2022)</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Yay! Another paper about Self-Supervised Learning! Is today my birthday? A F̶a̶c̶e̶b̶o̶o̶k̶ META paper about learning about Speech, NLP or Computer Vision with a single architecture? 😱</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><strong>Data2Vec learns to represent data by masking part of itself</strong> and trying to reconstruct the rest in a self-distillation setup. The teacher network learns representations of the full data which then the student network has to learn to predict from masked inputs. This, like VicReg, does not need fancy tricks but instead it uses mulitple layer representations for the regression (instead of only the top layer).</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Qw7WH5v3qCwQqSnCJIxhBg.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Qw7WH5v3qCwQqSnCJIxhBg.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Qw7WH5v3qCwQqSnCJIxhBg.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Qw7WH5v3qCwQqSnCJIxhBg.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Qw7WH5v3qCwQqSnCJIxhBg.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Qw7WH5v3qCwQqSnCJIxhBg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qw7WH5v3qCwQqSnCJIxhBg.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Qw7WH5v3qCwQqSnCJIxhBg.png 640w, https://miro.medium.com/v2/resize:fit:720/1*Qw7WH5v3qCwQqSnCJIxhBg.png 720w, https://miro.medium.com/v2/resize:fit:750/1*Qw7WH5v3qCwQqSnCJIxhBg.png 750w, https://miro.medium.com/v2/resize:fit:786/1*Qw7WH5v3qCwQqSnCJIxhBg.png 786w, https://miro.medium.com/v2/resize:fit:828/1*Qw7WH5v3qCwQqSnCJIxhBg.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*Qw7WH5v3qCwQqSnCJIxhBg.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*Qw7WH5v3qCwQqSnCJIxhBg.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*Qw7WH5v3qCwQqSnCJIxhBg.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Illustration of how data2vec follows the same learning process for different modalities</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Facebook AI Research (or FAIR … how humble) is no strange to this kind of learning. In fact, in December 2021 they released the famous <a href="https://arxiv.org/pdf/2111.06377.pdf" target="_self">Masked Autoencoders</a> that uses a simpler but similar method to learn representations of images in a Self-Supervised way.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*VcA0eIRjIpsAbeXMGbrH0w.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*VcA0eIRjIpsAbeXMGbrH0w.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*VcA0eIRjIpsAbeXMGbrH0w.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*VcA0eIRjIpsAbeXMGbrH0w.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*VcA0eIRjIpsAbeXMGbrH0w.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*VcA0eIRjIpsAbeXMGbrH0w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VcA0eIRjIpsAbeXMGbrH0w.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*VcA0eIRjIpsAbeXMGbrH0w.png 640w, https://miro.medium.com/v2/resize:fit:720/1*VcA0eIRjIpsAbeXMGbrH0w.png 720w, https://miro.medium.com/v2/resize:fit:750/1*VcA0eIRjIpsAbeXMGbrH0w.png 750w, https://miro.medium.com/v2/resize:fit:786/1*VcA0eIRjIpsAbeXMGbrH0w.png 786w, https://miro.medium.com/v2/resize:fit:828/1*VcA0eIRjIpsAbeXMGbrH0w.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*VcA0eIRjIpsAbeXMGbrH0w.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*VcA0eIRjIpsAbeXMGbrH0w.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*VcA0eIRjIpsAbeXMGbrH0w.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">MAE architecture</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Of course, both methods use the powerful and loved <strong>Transformer architecture</strong> (<a href="https://arxiv.org/pdf/2205.03892.pdf" target="_self">ConvMAE</a> also exists) but Data2Vec is generalizes to all kinds of data. Each type of data is embedded accordingly to its origin and masked using learned mask tokens. The targets are constructed from the outputs of the top K blocks of the teacher network and masked in student mode. The objective is a <em>smooth </em>Smooth-L1 loss that regresses the targets. Thanks to this all-in-one method is easier to train multimodal tasks like, for example, audio-visual speech recognition.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">But, we are not done yet! Could you imagine if the people Facebook AI (or Meta AI or whatever) did a better iteration of the Data2Vec model, now faster and more accurate? Well, you better believe it because it’s true. You surely must be in shock right now, maybe with a single tear running down your cheek, and the only thing you stutter to say is: <em>but… how… how… is it called…? </em>Because surely, the inventors of “FAIR” must had a perfect occurrence for this, like <em>OmniNet </em>or more humble, the <em>GodFormer</em>! Right? … Well, this is awkard…</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;"><a href="https://scontent-mad1-2.xx.fbcdn.net/v/t39.2365-6/318667518_671132957886206_241152552400726647_n.pdf?_nc_cat=101&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=KDgp-qRukO0AX8xw2th&_nc_ht=scontent-mad1-2.xx&oh=00_AfD-C-x0jo6MDS0VYv1x2ObRFKJ7HiFXa7mr6g0qaeMWag&oe=63A84F17" target="_self">Data2Vec 2.0</a> <strong>increases the training efficiency of its predecessor maintaining its cross modality</strong> (don’t even want to talk about it). Basically, the mask tokens are no longer encoded and our friends the Convolutions are now in the Decoder, so it’s faster now.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*cEOxEuG1sx4u_597AErIGQ.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*cEOxEuG1sx4u_597AErIGQ.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*cEOxEuG1sx4u_597AErIGQ.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*cEOxEuG1sx4u_597AErIGQ.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*cEOxEuG1sx4u_597AErIGQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*cEOxEuG1sx4u_597AErIGQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cEOxEuG1sx4u_597AErIGQ.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*cEOxEuG1sx4u_597AErIGQ.png 640w, https://miro.medium.com/v2/resize:fit:720/1*cEOxEuG1sx4u_597AErIGQ.png 720w, https://miro.medium.com/v2/resize:fit:750/1*cEOxEuG1sx4u_597AErIGQ.png 750w, https://miro.medium.com/v2/resize:fit:786/1*cEOxEuG1sx4u_597AErIGQ.png 786w, https://miro.medium.com/v2/resize:fit:828/1*cEOxEuG1sx4u_597AErIGQ.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*cEOxEuG1sx4u_597AErIGQ.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*cEOxEuG1sx4u_597AErIGQ.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*cEOxEuG1sx4u_597AErIGQ.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">data2vec 2.0 uses the same learning objective for different modalities (but trains different models for each)</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Now the same target representation is predicted by the student but for different masked versions of the training example. New method, faster results:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*e4EdB-4AnyTPnAe-QhBcdw.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*e4EdB-4AnyTPnAe-QhBcdw.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*e4EdB-4AnyTPnAe-QhBcdw.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*e4EdB-4AnyTPnAe-QhBcdw.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*e4EdB-4AnyTPnAe-QhBcdw.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*e4EdB-4AnyTPnAe-QhBcdw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e4EdB-4AnyTPnAe-QhBcdw.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*e4EdB-4AnyTPnAe-QhBcdw.png 640w, https://miro.medium.com/v2/resize:fit:720/1*e4EdB-4AnyTPnAe-QhBcdw.png 720w, https://miro.medium.com/v2/resize:fit:750/1*e4EdB-4AnyTPnAe-QhBcdw.png 750w, https://miro.medium.com/v2/resize:fit:786/1*e4EdB-4AnyTPnAe-QhBcdw.png 786w, https://miro.medium.com/v2/resize:fit:828/1*e4EdB-4AnyTPnAe-QhBcdw.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*e4EdB-4AnyTPnAe-QhBcdw.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*e4EdB-4AnyTPnAe-QhBcdw.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*e4EdB-4AnyTPnAe-QhBcdw.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Efficiency of data2vec 2.0 for computer vision and speech processing in terms of wall clock time for pre-training Base models</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Well done, FAIR play.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">The Forward-Forward Algorithm (December 2022)</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">I have to admit this is a very personal choice. We couldn’t have a selection of papers without Geoffrey Hinton. It’s like December without snow or like Joe Rogan without DMT… it’s just empty…</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Hinton had a revelation in which he saw the Boltzmann Machines (also his work) and tought to himself why not <strong>get rid of the famous forward and backward passes of backpropagation and instead uses two forward passes, one with positive data and the other with negative data</strong>. This makes learning way different! No activities have to be stored for sequential data, nor any derivates have to be propagated. It always seem counterintuitive that the brain used a backward pass but rather it has been observed some loops forming between neurons. This allows to <strong>learn without stopping for backpropagation</strong>. Exactly that’s the idea behind the Forward-Forward Algorithm.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">We can take the sum of the squares of the output of the ReLU activation of a layer and call that <em>goodness</em>. If that goodness is below a threshold, we can call that <em>negative</em>, and if it’s well above: <em>positive</em>. So that’s pretty much the idea, learn to classify data into <em>positive </em>(an image with its correct label) and <em>negative </em>(an image with its incorrect label, for example). But there is a problem, in backpropagation the information between layers flows from one side to the other; meanwhile, in FF what is learned in later layers cannot affect the earlier ones. The solution? <strong>Treat single images as a sequence and the activity vector of each layer is now the normalized activity vector at both the layer above and the layer below at the previous time-step</strong>.</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><div class="image_style" style="width:96%;"><picture><source sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Ob4LVS-awaVS6mcxcZpEcA.png 640w, https://miro.medium.com/v2/resize:fit:720/format:webp/1*Ob4LVS-awaVS6mcxcZpEcA.png 720w, https://miro.medium.com/v2/resize:fit:750/format:webp/1*Ob4LVS-awaVS6mcxcZpEcA.png 750w, https://miro.medium.com/v2/resize:fit:786/format:webp/1*Ob4LVS-awaVS6mcxcZpEcA.png 786w, https://miro.medium.com/v2/resize:fit:828/format:webp/1*Ob4LVS-awaVS6mcxcZpEcA.png 828w, https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Ob4LVS-awaVS6mcxcZpEcA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ob4LVS-awaVS6mcxcZpEcA.png 1400w" type="image/webp"/><source data-testid="og" sizes="(min-resolution: 4dppx) and (max-width: 700px) 50vw, (-webkit-min-device-pixel-ratio: 4) and (max-width: 700px) 50vw, (min-resolution: 3dppx) and (max-width: 700px) 67vw, (-webkit-min-device-pixel-ratio: 3) and (max-width: 700px) 65vw, (min-resolution: 2.5dppx) and (max-width: 700px) 80vw, (-webkit-min-device-pixel-ratio: 2.5) and (max-width: 700px) 80vw, (min-resolution: 2dppx) and (max-width: 700px) 100vw, (-webkit-min-device-pixel-ratio: 2) and (max-width: 700px) 100vw, 700px" srcset="https://miro.medium.com/v2/resize:fit:640/1*Ob4LVS-awaVS6mcxcZpEcA.png 640w, https://miro.medium.com/v2/resize:fit:720/1*Ob4LVS-awaVS6mcxcZpEcA.png 720w, https://miro.medium.com/v2/resize:fit:750/1*Ob4LVS-awaVS6mcxcZpEcA.png 750w, https://miro.medium.com/v2/resize:fit:786/1*Ob4LVS-awaVS6mcxcZpEcA.png 786w, https://miro.medium.com/v2/resize:fit:828/1*Ob4LVS-awaVS6mcxcZpEcA.png 828w, https://miro.medium.com/v2/resize:fit:1100/1*Ob4LVS-awaVS6mcxcZpEcA.png 1100w, https://miro.medium.com/v2/resize:fit:1400/1*Ob4LVS-awaVS6mcxcZpEcA.png 1400w"/><img alt="" loading="lazy" role="presentation" src="https://miro.medium.com/v2/resize:fit:700/1*Ob4LVS-awaVS6mcxcZpEcA.png"></picture></div><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">The recurrent network used to process video</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">No one is measuring accuracy here since the paper has other purpose, but it was trained on MNIST, CIFAR-10 (also by Hinton) and Aesop’s fables with surprinsingly low errors. Of course, backpropagation is not going anywhere and there are many winters left in his kingdom, but slowly and surely new training methods that solve the big problems with backpropagation and have more natural similarities are rising from the ground. It’s going to be amazing to see how the AI landscape will change in some years from now.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">Honorable Mentions</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">What?! Already over? But… it happend so fast… Okay, how about some quick honorable mentions that for some arbitrary reason didn’t make the cut. Maybe I’m just trying to delay the goodbye as much as possible, but I think they are worth a mention:</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><ul><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/pdf/2207.09238.pdf" target="_self">Formal Algortithms for Transformers</a><strong> (July 2022):</strong> So you like Transformers, uh? Alright, okay… There you go, the <strong>Holy Bible of Transformers</strong>, thank me later. This paper contains a bunch of transformer architectures and algoritms with the mathematics included. They also explain what Transformers are and what do they do in a very nice and understandable way. This should be the way-to-go guide for every average Transfomers user.</li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/pdf/2110.12661.pdf" target="_self">ZerO Initialization</a><strong> (November 2022):</strong> This paper introduces an <strong>inizialization method using only deterministic 0 and 1 instead of random weights</strong>. This simple thing allows for training “ultra deep” networks without Batch Normalization and resulting in low-rank learning trajectories and sparse solutions. Also, as the network inizialization is not random, it improves reproducibility!</li><li class="ff3" style="font-size:22px;"><a href="https://arxiv.org/pdf/2211.06956.pdf" target="_self">Seeing Beyond the Brain</a><strong> (November 2022):</strong> This is a personal favourite from 2022. It’s quite surreal to think that using mask modeling and diffusion one can <strong>decode the images that a person is thinking of via MRI! </strong>Wow. Brain reading State of the Art 2022 I guess.</li><li class="ff3" style="font-size:22px;"><a href="https://phenaki.video/#interactive" target="_self">Phenaki</a><strong> (November 2022):</strong> Text-to-Image models are trending and there has been <a href="https://medium.com/@diegobonila/text-to-image-papers-to-read-in-summer-2022-c57c7c9f1044" target="_self">a lot</a> of papers about it (shameless plug). But if there is something that could get us closer to the first movie made by an AI is… <strong>Text-to-Video! </strong>This paper explains how to make videos from static and <strong>dynamic text</strong>. What is dynamic text? A script for example, or a set of instructions that you want the video to follow in time. They even generated short movies of <strong>2:30 minutes</strong>! Wow.</li></ul></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><p class="mbr-description mbr-fonts-style mt-2 align-center display-12 text-muted" style="font-size:12px;">Phenaky 2:30 minute generated video with dynamic text</p><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><h4 class="mbr-section-subtitle mbr-fonts-style mb-4 display-5">FAREWELL.md</h4></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">This is it. The inevitable end of our journey. I don’t know about you, but I had a blast one. I’m not too good with goodbyes so… I just hope you’ve learned something, that if you have any comments or notes let us know in the comments and… <strong>here is a video of a donkey laughing at a dog getting (slightly) shocked in an electric fence</strong>:</p></div></div></div></section><section data-bs-version="5.1" class="image3 cid-tt612oXwA9" id="image3-5" style="padding-top:0px; padding-bottom:0px; background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-12 col-lg-9">
                <div class="image-wrapper"><br></div></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">Probably the funniest video I’ve ever seen in 2022.</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-10">
            <br>
                <hr class="hr5">
            <br>
            </div>
        </div>
    </div>
</section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">😁 Did you like the story? 💬 Let me know in the comments and give it a 👏!! Share it with friends 👯!! This things take a lot of time and effort to be done so the feedback is very appreciated! ❤️</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">👉 Follow my <a href="https://medium.com/@diegobonila" target="_self">Medium</a> for more!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">👉 Follow my <a href="https://www.linkedin.com/in/diego-bonilla-salvador/" target="_self">LinkedIn</a> for more papers or interesting personal projects!</p></div></div></div></section><section data-bs-version="5.1" class="content5 cid-tt5UseJ9wk" id="content5-4" style="padding-top:0px; padding-bottom:0px;background-color: rgb(255, 255, 255);">
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-12 col-lg-9"><p class="ff3" style="font-size:22px;">(or don’t I don’t care)</p></div></div></div></section><?php include_once 'Elemental/footer.php'; ?>