{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import undetected_chromedriver as uc\n",
    "from time import sleep\n",
    "from urllib.parse import urljoin\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import json\n",
    "import numpy as np\n",
    "import tqdm.auto as tqdm\n",
    "import contextlib\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "\n",
    "#from feedpage import LinkScrapperPersonal\n",
    "#l = LinkScrapperPersonal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GiveR(url):\n",
    "    r = requests.get(url, allow_redirects=False)\n",
    "    if r.status_code != 301:\n",
    "        driver.get(url)\n",
    "        screen_height = driver.execute_script(\"return window.screen.height;\")   # get the screen height of the web\n",
    "        timeout = time.time() + 60*0.5\n",
    "        scroll_pause_time = 1\n",
    "        i = 1\n",
    "        for _ in range(10):\n",
    "            while True:\n",
    "                # scroll one screen height each time\n",
    "                driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "                i += 1\n",
    "                time.sleep(scroll_pause_time)\n",
    "\n",
    "                # update scroll height each time after scrolled, as the scroll height can change after we scrolled the page\n",
    "                scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")  \n",
    "                # Break the loop when the height we need to scroll to is larger than the total scroll height\n",
    "                if (screen_height) * i > scroll_height:\n",
    "                    break\n",
    "            r = driver.page_source\n",
    "            #bts = BeautifulSoup(r, 'html.parser').find_all(\"button\")\n",
    "            #for i in range(len(bts)):\n",
    "            #    if bts[i].getText() == \"Show more\":\n",
    "            #        selector = bts[i]['class'][0]\n",
    "            #btn = driver.find_elements(By.CLASS_NAME, selector)[0]\n",
    "            #btn.click()\n",
    "        r = driver.page_source\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_argument('--headless')\n",
    "#options.add_argument('--disable-gpu')\n",
    "#options.add_argument(\"--no-sandbox\")\n",
    "#options.add_argument(\"--window-size=1920x1080\")\n",
    "\n",
    "#driver = webdriver.Chrome(exec_file = \"/home/dibya/Dibyendu/Test/HTML/selenium/chromedriver/linux64/109.0.5414.74/chromedriver\", options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00010'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ld = f'0000{10}'[-5:]\n",
    "ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://hbcp.chemnetbase.com/faces/documents/06_25/06_25_0001.xhtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "big_data_table = []\n",
    "for i in range(1, 48):\n",
    "    driver = webdriver.Chrome(options= options)\n",
    "    mid =  f'000{i}'\n",
    "    mid = mid[-4:]\n",
    "    url = f\"https://hbcp.chemnetbase.com/faces/documents/06_25/06_25_{mid}.xhtml\"\n",
    "    r = GiveR(url)\n",
    "    soup = BeautifulSoup(r, 'html.parser')\n",
    "    trs = soup.find_all('tr', class_=\"ui-widget-content\")\n",
    "    data_table = []\n",
    "    for row in trs:\n",
    "        if len(row) == 10:\n",
    "            elem_list = []\n",
    "            for elem in row:\n",
    "                elem_list.append(elem.text)\n",
    "        data_table.append(elem_list)  \n",
    "    driver.close()\n",
    "    print(i)\n",
    "    big_data_table.append(data_table)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m-Xylene',\n",
       " '1,3-Dimethylbenzene',\n",
       " 'C8H10',\n",
       " 'C6H4(CH3)2',\n",
       " '108-38-3',\n",
       " '106.165',\n",
       " '139.1',\n",
       " '35.66',\n",
       " '42.65',\n",
       " '1']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_data_table[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for data_table in big_data_table:\n",
    "    for data in data_table:\n",
    "        all_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1775"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(np.array(all_data), columns = [\"Name\",\"Synonym\",\"Mol.Form\",\"Formula\",\"CAS\",\"Mol.Wt\",\"t_bp\",\"Delta_G(t)\",\"Delta_G(25)\",\"Ref\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Formula</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acetaldehyde</td>\n",
       "      <td>CH3CHO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acetic acid</td>\n",
       "      <td>CH3COOH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Acetic anhydride</td>\n",
       "      <td>C4H6O3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acetone</td>\n",
       "      <td>(CH3)2CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acetonitrile</td>\n",
       "      <td>CH3CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>3,4-Xylenol</td>\n",
       "      <td>C8H10O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>3,5-Xylenol</td>\n",
       "      <td>C8H10O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754</th>\n",
       "      <td>Zinc bromide</td>\n",
       "      <td>ZnBr2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>Zinc chloride</td>\n",
       "      <td>ZnCl2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>Zinc fluoride</td>\n",
       "      <td>ZnF2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>929 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name   Formula\n",
       "0         Acetaldehyde    CH3CHO\n",
       "1          Acetic acid   CH3COOH\n",
       "2     Acetic anhydride    C4H6O3\n",
       "3              Acetone  (CH3)2CO\n",
       "4         Acetonitrile     CH3CN\n",
       "...                ...       ...\n",
       "1752       3,4-Xylenol    C8H10O\n",
       "1753       3,5-Xylenol    C8H10O\n",
       "1754      Zinc bromide     ZnBr2\n",
       "1755     Zinc chloride     ZnCl2\n",
       "1756     Zinc fluoride      ZnF2\n",
       "\n",
       "[929 rows x 2 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Name', 'Formula']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(s, 'html.parser')\n",
    "trs = soup.find_all('tr', class_=\"ui-widget-content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pubchempy as pcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ZnCl2'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[1755].Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Zinc bromide'\n",
    "formula = 'ZnBr2'\n",
    "def giveSMILES(name, formula):\n",
    "    try:\n",
    "        results = pcp.get_compounds(name, 'name')\n",
    "        smiles = results[0].to_dict(properties=['canonical_smiles'])['canonical_smiles']\n",
    "    except Exception:\n",
    "        try:\n",
    "            results = pcp.get_compounds(formula, 'formula')\n",
    "            names = results[0].synonyms\n",
    "            names = list(map(lambda x : x.strip().lower().replace(\" \", \"\"), names))\n",
    "            if name.strip().lower().replace(\" \", \"\") in names:\n",
    "                smiles = results[0].to_dict(properties=['canonical_smiles'])['canonical_smiles']\n",
    "            else:\n",
    "                print(\"Err\")\n",
    "        except Exception:\n",
    "            smiles = \"Not None\"\n",
    "    return smiles\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_list = []\n",
    "for sm, fm in zip(df['Name'], df['Formula']):\n",
    "    smiles_list.append(giveSMILES(name, formula))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'canonical_smiles': '[Zn+2].[Br-].[Br-]'}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].to_dict(properties=['canonical_smiles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = []\n",
    "for row in trs:\n",
    "    if len(row) == 10:\n",
    "        elem_list = []\n",
    "        for elem in row:\n",
    "            elem_list.append(elem.text)\n",
    "    data_table.append(elem_list)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#38*46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_39651/1443987599.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlinks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersonal_links\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'l' is not defined"
     ]
    }
   ],
   "source": [
    "links = l.personal_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediumscrapper import ScrapeWebsite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"plinks.txt\" , \"w\") as file:\n",
    "#    file.write(\"\\n\".join(l.personal_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = open(\"plinks.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1cae8b84c8408085108884ff5ffef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Scrappable = ScrapeWebsite()\n",
    "for i in tqdm.trange(5,150):\n",
    "    curl = links[i]\n",
    "    #curl = \"https://albertoromgar.medium.com/5-practical-applications-where-chatgpt-shines-above-everything-else-9e21571b5ca1\"\n",
    "    Scrappable.DoScrap(curl, save_dir=\"./Personal/\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('Personal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Graph_Theory_A_fun_use_case_of_graph_algorithms_simply_explained.php'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Title Error!!\n",
      "No Title Error!!\n",
      "No Title Error!!\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    try:\n",
    "        data = open(f'Personal/{file}').read()\n",
    "        soup = BeautifulSoup(data)\n",
    "        mydivs = soup.find_all(\"div\", {\"class\": \"title\"})\n",
    "        mydivs[0].attrs['class'] = \"title col-md-12 col-lg-9\"\n",
    "        mydivs[0].h3.attrs['class'] = \"mbr-section-title mbr-fonts-style mb-4 display-2\"\n",
    "        para = soup.find_all(\"p\", {\"class\":\"ff3\"})\n",
    "        for p in para:\n",
    "            p['style'] = 'font-size:20px;'\n",
    "\n",
    "        with open(f'Personal/{file}', \"w\") as f:\n",
    "            f.write(soup.prettify())\n",
    "    except Exception:\n",
    "        print(\"No Title Error!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('vae.html').read()\n",
    "soup = BeautifulSoup(data)\n",
    "\n",
    "para = soup.find_all(\"p\", {\"class\":\"ff3\"})\n",
    "for p in para:\n",
    "    p['style'] = 'font-size:20px;'\n",
    "\n",
    "with open('vae.html', \"w\") as f:\n",
    "    f.write(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = soup.find_all(\"p\", {\"class\":\"ff3\"})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['style'] = 'font-size:22px;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<p class=\"ff3\" style=\"font-size:22px;\">This article has been taken from <a href=\"https://www.educative.io/collection/5679346740101120/5707702298738688?authorName=Arnav%20Aggarwal\" target=\"_self\">Step Up Your JS: A Comprehensive Guide to Intermediate JavaScript</a>, my online course. Feel free to <a href=\"https://www.educative.io/collection/page/5679346740101120/5707702298738688/5685265389584384\" target=\"_self\">view it there</a> for interactive code playgrounds and an online quiz.</p>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import undetected_chromedriver as uc\n",
    "from time import sleep\n",
    "from urllib.parse import urljoin\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import contextlib\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def GiveR(url):\n",
    "    r = requests.get(url, allow_redirects=False)\n",
    "    if r.status_code != 301:\n",
    "        driver.get(url)\n",
    "        screen_height = driver.execute_script(\"return window.screen.height;\")   # get the screen height of the web\n",
    "        timeout = time.time() + 60*0.5\n",
    "        scroll_pause_time = 1\n",
    "        i = 1\n",
    "        for _ in range(10):\n",
    "            while True:\n",
    "                # scroll one screen height each time\n",
    "                driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "                i += 1\n",
    "                time.sleep(scroll_pause_time)\n",
    "\n",
    "                # update scroll height each time after scrolled, as the scroll height can change after we scrolled the page\n",
    "                scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")  \n",
    "                # Break the loop when the height we need to scroll to is larger than the total scroll height\n",
    "                if (screen_height) * i > scroll_height:\n",
    "                    break\n",
    "            r = driver.page_source\n",
    "            bts = BeautifulSoup(r, 'html.parser').find_all(\"button\")\n",
    "            for i in range(len(bts)):\n",
    "                if bts[i].getText() == \"Show more\":\n",
    "                    selector = bts[i]['class'][0]\n",
    "            btn = driver.find_elements(By.CLASS_NAME, selector)[0]\n",
    "            btn.click()\n",
    "        r = driver.page_source\n",
    "        return r\n",
    "\n",
    "def QueryMaker(query):\n",
    "    r = GiveR(f\"https://medium.com/search?q={query}\")\n",
    "    soup = BeautifulSoup(r, 'html.parser')\n",
    "    articles = soup.find_all('article')\n",
    "    title_list, content_list, rt_list, link_list, img_list = [], [], [], [], []\n",
    "    for article in articles:\n",
    "        title, content, rt , link, img_link = getDetails(article)\n",
    "        title_list.append(title)\n",
    "        content_list.append(content)\n",
    "        rt_list.append(rt)\n",
    "        link_list.append(link)\n",
    "        img_list.append(img_link)\n",
    "    return title_list, content_list, rt_list, link_list, img_list\n",
    "\n",
    "def getDetails(article):\n",
    "    all_a = article.find_all(\"a\")\n",
    "    title, content, rt, link, img_link = None, None, None, None, None  \n",
    "    for a in all_a:\n",
    "        with contextlib.suppress(Exception):\n",
    "            #print(a.text)\n",
    "            label = a.extract()['aria-label']\n",
    "            #print(label)\n",
    "            if label == \"Post Preview Title\":\n",
    "                title = a.find(\"h2\").getText()\n",
    "                content = a.find(\"p\").getText()\n",
    "            elif label == \"Post Preview Reading Time\":\n",
    "                rt = a.getText()\n",
    "            elif label == \"Post Preview Image\":\n",
    "                link = a.extract()['href']\n",
    "            image = a.find('img')\n",
    "            img_link = image.extract()['src']\n",
    "            \n",
    "    return title, content, rt, link, img_link\n",
    "\n",
    "title, content, rt, link, img_link = [], [], [], [], []\n",
    "for query in [\"Machine+Learning\", \"Deep+Learning\", \"Python\", \"Programming\", \"Neural+Network\", \"CNN\", \"RNN\", \"GNN\", \"drug+discovery\", \"AI\"]:\n",
    "    t, c , r, l, i = QueryMaker(query = query)\n",
    "    title.append(t)\n",
    "    content.append(c)\n",
    "    rt.append(r)\n",
    "    link.append(l)\n",
    "    img_link.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_argument('--disable-gpu')\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--window-size=1920x1080\")\n",
    "options.add_argument('--headless')\n",
    "driver = uc.Chrome(exec_file = \"/home/dibya/Dibyendu/Test/HTML/selenium/chromedriver/linux64/109.0.5414.74/chromedriver\", options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfilter = lambda x : x[:25]\n",
    "all_title = list(chain.from_iterable(map(myfilter, title)))\n",
    "all_content = list(chain.from_iterable(map(myfilter, content)))\n",
    "all_rt = list(chain.from_iterable(map(myfilter, rt)))\n",
    "all_links = list(chain.from_iterable(map(myfilter, link)))\n",
    "all_images = list(chain.from_iterable(map(myfilter, img_link)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498ee994189b444aa4acd33f1a7df07c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_163104/142011659.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcurl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"https://medium.com/{links[i]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mScrappable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoScrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./Feed/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Soft/Fresh/Scraper/mediumscrapper.py\u001b[0m in \u001b[0;36mDoScrap\u001b[0;34m(self, url, save_dir)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marticle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'article'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'section'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "\n",
    "def formatDate(date):\n",
    "    return datetime.datetime.strptime(date,  '%b %d, %Y').strftime('%Y-%m-%d')\n",
    "\n",
    "def TagExtracter(Scrappable):\n",
    "    All_As = Scrappable.Soup.find_all('a', {'rel':\"noopener follow\"})\n",
    "    Checker = lambda x : 'tag' in x['href']\n",
    "    tag_links = np.array(All_As, dtype = object)[list(map(Checker, All_As))]\n",
    "    Tagger = lambda x : x.getText()\n",
    "    return np.unique(list(map(Tagger, tag_links)))\n",
    "\n",
    "def formatTitle(title):\n",
    "    remove_chars = ['[', ']', '/', '`', '\"', \"'\", '…', '.', ',', '’', '‘', '\\n', '“', '”', ':', '：', '，', \"?\", \"!\"]\n",
    "    replace_with_space = [':']\n",
    "\n",
    "    for char in remove_chars:\n",
    "        title = title.replace(char, '')\n",
    "\n",
    "    for char in replace_with_space:\n",
    "        title = title.replace(char, ' ')\n",
    "\n",
    "    title = '_'.join(title.split())\n",
    "\n",
    "    return title\n",
    "def transform_data(author, title, content, date, tag, image, dir = \"files\"):\n",
    "\n",
    "    # Replace single and double quotes\n",
    "    author = author.replace(\"'\", \"\").replace('\"', '').replace('’', '').replace('‘', '')\n",
    "    title = title.replace(\"'\", \"\").replace('\"', '').replace('’', '').replace('‘', '')\n",
    "    content = content.replace(\"'\", \"\").replace('\"', '').replace('’', '').replace('‘', '').replace('…', '...')[:200]\n",
    "    \n",
    "    # Replace punctuation\n",
    "    author = author.replace('\\n','').replace('”', '').replace('“', '').replace(':', '').replace('：', '').replace('，', '')\n",
    "    title = title.replace('\\n','').replace('”', '').replace('“', '').replace(':', '').replace('：', '').replace('，', '')\n",
    "    \n",
    "    # Format Title & Date\n",
    "    formated_title = formatTitle(title)\n",
    "    formatted_date = formatDate(date)\n",
    "    \n",
    "    # Join tag list into string\n",
    "    joined_tags = '_'.join(tag)\n",
    "    \n",
    "    # Return dictionary with transformed data\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"content\": content,\n",
    "        \"author\": author,\n",
    "        \"image\": image.replace(\"112\", \"640\"),\n",
    "        \"tag\": joined_tags,\n",
    "        \"dt\": formatted_date,\n",
    "        \"file\": f\"{dir}/{formated_title}.php\"\n",
    "    }\n",
    "\n",
    "\n",
    "def getOne(image, title, content, date, file, author, tag):\n",
    "       \n",
    "    return f\"\"\"<div class=\"item features-image сol-12 col-md-6 col-lg-4\" data-type=\"{tag}\">\n",
    "                    <div class=\"item-wrapper\">\n",
    "                        <div class=\"item-img\">\n",
    "                            <img src={image} alt=\"Image\" loading=\"lazy\" \n",
    "                                title=\"\">\n",
    "                        </div>\n",
    "                        <div class=\"item-content\">\n",
    "                            <h5 class=\"text-muted\" style=\"font-size:15px; float:left;\">\n",
    "                                <strong>{author}</strong>\n",
    "                                <em>&nbsp;{date}</em>\n",
    "                            </h5>\n",
    "                            <br>\n",
    "                            <h6 class=\"item-subtitle mbr-fonts-style mt-1 display-7\"><strong>{title}</strong>\n",
    "                            </h6>\n",
    "                            <p class=\"ff3\">{content}...</p>\n",
    "                        </div>\n",
    "                        <div class=\"mbr-section-btn item-footer mt-2 text-center\">\n",
    "                            <a class=\"slide btn text-center\" id=\"button\" href=\"{file}\" >&nbsp;</a></div>\n",
    "                    </div>\n",
    "                </div>\"\"\"\n",
    "\n",
    "def GetDiv( detail_list ):\n",
    "    forYou_text = \"\"\"<section data-bs-version=\"5.1\" class=\"content2 cid-sFAOw5Fdod\" id=\"content2-e\" style=\"padding-top: 0px; padding-bottom: 0px;\">\n",
    "            <div class=\"container\">\n",
    "            <div class=\"mbr-section-head\">\n",
    "            <br><br>\n",
    "                <h4 class=\"mbr-section-title mbr-fonts-style align-center mb-0 display-2\"><strong>Free Articles from\n",
    "                        Medium!</strong></h4>\n",
    "                <h5 class=\"mbr-section-subtitle mbr-fonts-style align-center mb-0 mt-2 display-5\">Read the recent blog\n",
    "                    about ML, DL and Data Science from Towards Data Science.</h5>\n",
    "            </div><div class=\"row mt-4\">\"\"\"\n",
    "\n",
    "    for i in range(len(detail_list)):\n",
    "        #print(i)\n",
    "        #data = transform_data(all_authors[i], all_titles[i], all_contents[i], all_dates[i], all_tags[i], all_images[i], dir = \"Feed\")\n",
    "        with contextlib.suppress(Exception):\n",
    "            data = transform_data(all_authors[i], all_titles[i], all_contents[i], all_dates[i], all_tags[i], all_images[i], dir = \"Feed\")\n",
    "            title = data['title']\n",
    "            date = data['dt']\n",
    "            image = data['image']\n",
    "            author = data['author']\n",
    "            content = data['content']\n",
    "            file = data['file']\n",
    "            tag = data['tag']\n",
    "            #print(data)\n",
    "            forYou_text += getOne(title=title, date=date, image=image, author=author, content=content, tag = tag, file = file)\n",
    "    forYou_text += \"\"\"</div></div></section>\"\"\"\n",
    "    return forYou_text\n",
    "\n",
    "Scrappable = ScrapeWebsite()\n",
    "author_list2 = []\n",
    "Address_list2 = []\n",
    "tag_list2 = []\n",
    "date_list2 = []\n",
    "for i in tqdm.trange(len(links)):\n",
    "    if links[i] != None:\n",
    "        curl = f'https://medium.com{links[i]}'\n",
    "        Scrappable.DoScrap(curl, save_dir=\"./Feed/\") \n",
    "        Author, Address = Scrappable.GiveAuthor(Scrappable.Soup)\n",
    "        Date = Scrappable.Soup.find('p', {'class':'pw-published-date'}).getText()\n",
    "        tags = TagExtracter(Scrappable)\n",
    "        tag_list2.append(tags)\n",
    "        date_list2.append(Date)\n",
    "        Address_list2.append(Address)\n",
    "        author_list2.append(Author)\n",
    "    else:\n",
    "        tag_list2.append(None)\n",
    "        date_list2.append(None)\n",
    "        Address_list2.append(None)\n",
    "        author_list2.append(None)\n",
    "\n",
    "all_authors = author_list2\n",
    "all_dates = date_list2\n",
    "all_address = Address_list2\n",
    "all_tags = tag_list2\n",
    "all_titles = np.load('title.npy', allow_pickle = True)\n",
    "all_images = np.load('images.npy', allow_pickle = True)\n",
    "all_contents = np.load('contents.npy', allow_pickle = True)\n",
    "\n",
    "head = open(\"Elemental/header.html\").read()\n",
    "foot = open(\"Elemental/footer.html\").read()\n",
    "with open(\"Feed.html\", \"w\") as f:\n",
    "    f.write(head)\n",
    "    f.write(GetDiv(all_address))\n",
    "    f.write(foot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"links.npy\", filtered_links)\n",
    "np.save(\"contents.npy\", filtered_content)\n",
    "np.save(\"images.npy\", filtered_img_links)\n",
    "np.save(\"rts.npy\", filtered_rt)\n",
    "np.save(\"title.npy\", filtered_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = np.load('links.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d13fa35701b3ccf10ec30731592d3f10e2e0dbd94b9263cf3db1223b8cdbfc1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
